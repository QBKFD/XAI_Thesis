{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "Connected to Python 3.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-0ea7f5060e54>:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# from fim import fpgrowth,fim --> this package is difficult to install. So the rule miner can be replaced by a random forest\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import chain, combinations\n",
    "import itertools\n",
    "from numpy.random import random\n",
    "from random import sample\n",
    "import time\n",
    "import operator\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csc_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "class BOA(object):\n",
    "    def __init__(self, binary_data,Y):\n",
    "        self.df = binary_data  \n",
    "        self.Y = Y\n",
    "        self.attributeLevelNum = defaultdict(int) \n",
    "        self.attributeNames = []\n",
    "        for i,name in enumerate(binary_data.columns):\n",
    "          attribute = name.split('_')[0]\n",
    "          self.attributeLevelNum[attribute] += 1\n",
    "          self.attributeNames.append(attribute)\n",
    "        self.attributeNames = list(set(self.attributeNames))\n",
    "        \n",
    "\n",
    "    def getPatternSpace(self):\n",
    "        print('Computing sizes for pattern space ...')\n",
    "        start_time = time.time()\n",
    "        \"\"\" compute the rule space from the levels in each attribute \"\"\"\n",
    "        for item in self.attributeNames:\n",
    "            self.attributeLevelNum[item+'_neg'] = self.attributeLevelNum[item]\n",
    "        self.patternSpace = np.zeros(self.maxlen+1)\n",
    "        tmp = [ item + '_neg' for item in self.attributeNames]\n",
    "        self.attributeNames.extend(tmp)\n",
    "        for k in range(1,self.maxlen+1,1):\n",
    "            for subset in combinations(self.attributeNames,k):\n",
    "                tmp = 1\n",
    "                for i in subset:\n",
    "                    tmp = tmp * self.attributeLevelNum[i]\n",
    "                self.patternSpace[k] = self.patternSpace[k] + tmp\n",
    "        print('\\tTook %0.3fs to compute patternspace' % (time.time() - start_time))               \n",
    "\n",
    "# This function generates rules that satisfy supp and maxlen using fpgrowth, then it selects the top N rules that make data have the biggest decrease in entropy\n",
    "# there are two ways to generate rules. fpgrowth can handle cases where the maxlen is small. If maxlen<=3, fpgrowth can generates rules much faster than randomforest. \n",
    "# If maxlen is big, fpgrowh tends to generate too many rules that overflow the memories. \n",
    "    def generate_rules(self,supp,maxlen,N, method = 'randomforest'):\n",
    "        self.maxlen = maxlen\n",
    "        self.supp = supp\n",
    "        df = 1-self.df #df has negative associations\n",
    "        df.columns = [name.strip() + '_neg' for name in self.df.columns]\n",
    "        df = pd.concat([self.df,df],axis = 1)\n",
    "#         if method =='fpgrowth' and maxlen<=3:\n",
    "#             itemMatrix = [[item for item in df.columns if row[item] ==1] for i,row in df.iterrows() ]  \n",
    "#             pindex = np.where(self.Y==1)[0]\n",
    "#             nindex = np.where(self.Y!=1)[0]\n",
    "#             print('Generating rules using fpgrowth')\n",
    "#             start_time = time.time()\n",
    "#             rules= fpgrowth([itemMatrix[i] for i in pindex],supp = supp,zmin = 1,zmax = maxlen)\n",
    "#             rules = [tuple(np.sort(rule[0])) for rule in rules]\n",
    "#             rules = list(set(rules))\n",
    "#             start_time = time.time()\n",
    "#             print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "#         else:\n",
    "        rules = []\n",
    "        start_time = time.time()\n",
    "        for length in range(1,maxlen+1,1):\n",
    "            n_estimators = min(pow(df.shape[1],length),4000)\n",
    "            clf = RandomForestClassifier(n_estimators = n_estimators,max_depth = length)\n",
    "            clf.fit(self.df,self.Y)\n",
    "            for n in range(n_estimators):\n",
    "                rules.extend(extract_rules(clf.estimators_[n],df.columns))\n",
    "        rules = [list(x) for x in set(tuple(x) for x in rules)]\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "        self.screen_rules(rules,df,N) # select the top N rules using secondary criteria, information gain\n",
    "        self.getPatternSpace()\n",
    "\n",
    "    def screen_rules(self,rules,df,N):\n",
    "        print('Screening rules using information gain')\n",
    "        start_time = time.time()\n",
    "        itemInd = {}\n",
    "        for i,name in enumerate(df.columns):\n",
    "            itemInd[name] = i\n",
    "        indices = np.array(list(itertools.chain.from_iterable([[itemInd[x] for x in rule] for rule in rules])))\n",
    "        len_rules = [len(rule) for rule in rules]\n",
    "        indptr =list(accumulate(len_rules))\n",
    "        indptr.insert(0,0)\n",
    "        indptr = np.array(indptr)\n",
    "        data = np.ones(len(indices))\n",
    "        ruleMatrix = csc_matrix((data,indices,indptr),shape = (len(df.columns),len(rules)))\n",
    "        mat = np.matrix(df) * ruleMatrix\n",
    "        lenMatrix = np.matrix([len_rules for i in range(df.shape[0])])\n",
    "        Z =  (mat ==lenMatrix).astype(int)\n",
    "        Zpos = [Z[i] for i in np.where(self.Y>0)][0]\n",
    "        TP = np.array(np.sum(Zpos,axis=0).tolist()[0])\n",
    "        supp_select = np.where(TP>=self.supp*sum(self.Y)/100)[0]\n",
    "        FP = np.array(np.sum(Z,axis = 0))[0] - TP\n",
    "        TN = len(self.Y) - np.sum(self.Y) - FP\n",
    "        FN = np.sum(self.Y) - TP\n",
    "        p1 = TP.astype(float)/(TP+FP)\n",
    "        p2 = FN.astype(float)/(FN+TN)\n",
    "        pp = (TP+FP).astype(float)/(TP+FP+TN+FN)\n",
    "        tpr = TP.astype(float)/(TP+FN)\n",
    "        fpr = FP.astype(float)/(FP+TN)\n",
    "        cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
    "        cond_entropy[p1*(1-p1)==0] = -((1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2)))[p1*(1-p1)==0]\n",
    "        cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
    "        cond_entropy[p1*(1-p1)*p2*(1-p2)==0] = 0\n",
    "        select = np.argsort(cond_entropy[supp_select])[::-1][-N:]\n",
    "        self.rules = [rules[i] for i in supp_select[select]]\n",
    "        self.RMatrix = np.array(Z[:,supp_select[select]])\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(self.rules)))\n",
    "\n",
    "    def set_parameters(self, a1=100,b1=10,a2=100,b2=10,al=None,bl=None):\n",
    "        # input al and bl are lists\n",
    "        # a1/(a1 + b1) and a2/(a2 + b2) should be a  bigger than 0.5. They correspond to alpha_+, beta_+ and alpha_- and beta_- from the paper. In fact, these four values play an important role and it needs to be tuned while following the a1/(a1 + b1) >0.5 and a2/(a2 + b2) >0.5 principle\n",
    "        self.alpha_1 = a1\n",
    "        self.beta_1 = b1\n",
    "        self.alpha_2 = a2\n",
    "        self.beta_2 = b2\n",
    "        if al ==None or bl==None or len(al)!=self.maxlen or len(bl)!=self.maxlen:\n",
    "            print('No or wrong input for alpha_l and beta_l. The model will use default parameters!')\n",
    "            self.C = [1.0/self.maxlen for i in range(self.maxlen)]\n",
    "            self.C.insert(0,-1)\n",
    "            self.alpha_l = [10 for i in range(self.maxlen+1)]\n",
    "            self.beta_l= [10*self.patternSpace[i]/self.C[i] for i in range(self.maxlen+1)]\n",
    "        else:\n",
    "            self.alpha_l=[1] + list(al)\n",
    "            self.beta_l = [1] + list(bl)\n",
    "\n",
    "    def fit(self, Niteration = 300, Nchain = 1, q = 0.1, init = [], keep_most_accurate_model = True,print_message=True):\n",
    "        # print('Searching for an optimal solution...')\n",
    "        start_time = time.time()\n",
    "        nRules = len(self.rules)\n",
    "        self.rules_len = [len(rule) for rule in self.rules]\n",
    "        maps = defaultdict(list)\n",
    "        T0 = 1000\n",
    "        split = 0.7*Niteration\n",
    "        acc = {}\n",
    "        most_accurate_model = defaultdict(list)\n",
    "        for chain in range(Nchain):\n",
    "            # initialize with a random pattern set\n",
    "            if init !=[]:\n",
    "                rules_curr = init.copy()\n",
    "            else:\n",
    "                N = sample(range(1,min(8,nRules),1),1)[0]\n",
    "                rules_curr = sample(range(nRules),N)\n",
    "            rules_curr_norm = self.normalize(rules_curr)\n",
    "            pt_curr = -100000000000\n",
    "            maps[chain].append([-1,[pt_curr/3,pt_curr/3,pt_curr/3],rules_curr,[self.rules[i] for i in rules_curr]])\n",
    "            acc[chain] = 0\n",
    "            for iter in range(Niteration):\n",
    "                if iter>=split:\n",
    "                    p = np.array(range(1+len(maps[chain])))\n",
    "                    p = np.array(list(accumulate(p)))\n",
    "                    p = p/p[-1]\n",
    "                    index = find_lt(p,random())\n",
    "                    rules_curr = maps[chain][index][2].copy()\n",
    "                    rules_curr_norm = maps[chain][index][2].copy()\n",
    "                rules_new, rules_norm = self.propose(rules_curr.copy(), rules_curr_norm.copy(),q)\n",
    "                cfmatrix,prob =  self.compute_prob(rules_new)\n",
    "                T = T0**(1 - iter/Niteration)\n",
    "                pt_new = sum(prob)\n",
    "                alpha = np.exp(float(pt_new -pt_curr)/T)\n",
    "                \n",
    "                if (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)> acc[chain]:\n",
    "                    most_accurate_model[chain] = rules_new[:]\n",
    "                    acc[chain] = (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)\n",
    "                    if print_message:\n",
    "                        print('found a more accurate model: accuracy = {}'.format(acc[chain]))\n",
    "                        \n",
    "                if pt_new > sum(maps[chain][-1][1]):\n",
    "                    maps[chain].append([iter,prob,rules_new,[self.rules[i] for i in rules_new]])\n",
    "                    if print_message:\n",
    "                        print('\\n** chain = {}, max at iter = {} ** \\n accuracy = {}, TP = {},FP = {}, TN = {}, FN = {}\\n pt_new is {}, prior_ChsRules={}, likelihood_1 = {}, likelihood_2 = {}\\n '.format(chain, iter,(cfmatrix[0]+cfmatrix[2]+0.0)/len(self.Y),cfmatrix[0],cfmatrix[1],cfmatrix[2],cfmatrix[3],sum(prob), prob[0], prob[1], prob[2]))\n",
    "                        # print '\\n** chain = {}, max at iter = {} ** \\n obj = {}, prior = {}, llh = {} '.format(chain, iter,prior+llh,prior,llh)\n",
    "                        self.print_rules(rules_new)\n",
    "                        print(rules_new)\n",
    "                if random() <= alpha:\n",
    "                    rules_curr_norm,rules_curr,pt_curr = rules_norm.copy(),rules_new.copy(),pt_new\n",
    "        # print('\\tTook %0.3fs to generate an optimal rule set' % (time.time() - start_time))\n",
    "        if keep_most_accurate_model:\n",
    "            acc_list = [acc[chain] for chain in range(Nchain)]\n",
    "            index = acc_list.index(max(acc_list))\n",
    "            return [self.rules[i] for i in most_accurate_model[index]] \n",
    "        else:\n",
    "            pt_max = [sum(maps[chain][-1][1]) for chain in range(Nchain)]\n",
    "            index = pt_max.index(max(pt_max))\n",
    "            return maps[index][-1][3]\n",
    "\n",
    "    def propose(self, rules_curr,rules_norm,q):\n",
    "        nRules = len(self.rules)\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules_curr],axis = 1)>0).astype(int)\n",
    "        incorr = np.where(self.Y!=Yhat)[0]\n",
    "        N = len(rules_curr)\n",
    "        if len(incorr)==0:\n",
    "            clean = True\n",
    "            move = ['clean']\n",
    "            # it means the BOA correctly classified all points but there could be redundant patterns, so cleaning is needed\n",
    "        else:\n",
    "            clean = False\n",
    "            ex = sample(incorr.tolist(),1)[0]\n",
    "            t = random()\n",
    "            if self.Y[ex]==1 or N==1:\n",
    "                if t<1.0/2 or N==1:\n",
    "                    move = ['add']       # action: add\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "            else:\n",
    "                if t<1.0/2:\n",
    "                    move = ['cut']       # action: cut\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "        if move[0]=='cut':\n",
    "            \"\"\" cut \"\"\"\n",
    "            if random()<q:\n",
    "                candidate = list(set(np.where(self.RMatrix[ex,:]==1)[0]).intersection(rules_curr))\n",
    "                if len(candidate)==0:\n",
    "                    candidate = rules_curr\n",
    "                cut_rule = sample(candidate,1)[0]\n",
    "            else:\n",
    "                p = []\n",
    "                all_sum = np.sum(self.RMatrix[:,rules_curr],axis = 1)\n",
    "                for index,rule in enumerate(rules_curr):\n",
    "                    Yhat= ((all_sum - np.array(self.RMatrix[:,rule]))>0).astype(int)\n",
    "                    TP,FP,TN,FN  = getConfusion(Yhat,self.Y)\n",
    "                    p.append(TP.astype(float)/(TP+FP+1))\n",
    "                    # p.append(log_betabin(TP,TP+FP,self.alpha_1,self.beta_1) + log_betabin(FN,FN+TN,self.alpha_2,self.beta_2))\n",
    "                p = [x - min(p) for x in p]\n",
    "                p = np.exp(p)\n",
    "                p = np.insert(p,0,0)\n",
    "                p = np.array(list(accumulate(p)))\n",
    "                if p[-1]==0:\n",
    "                    index = sample(range(len(rules_curr)),1)[0]\n",
    "                else:\n",
    "                    p = p/p[-1]\n",
    "                index = find_lt(p,random())\n",
    "                cut_rule = rules_curr[index]\n",
    "            rules_curr.remove(cut_rule)\n",
    "            rules_norm = self.normalize(rules_curr)\n",
    "            move.remove('cut')\n",
    "            \n",
    "        if len(move)>0 and move[0]=='add':\n",
    "            \"\"\" add \"\"\"\n",
    "            if random()<q:\n",
    "                add_rule = sample(range(nRules),1)[0]\n",
    "            else: \n",
    "                Yhat_neg_index = list(np.where(np.sum(self.RMatrix[:,rules_curr],axis = 1)<1)[0])\n",
    "                mat = np.multiply(self.RMatrix[Yhat_neg_index,:].transpose(),self.Y[Yhat_neg_index])\n",
    "                # TP = np.array(np.sum(mat,axis = 0).tolist()[0])\n",
    "                TP = np.sum(mat,axis = 1)\n",
    "                FP = np.array((np.sum(self.RMatrix[Yhat_neg_index,:],axis = 0) - TP))\n",
    "                TN = np.sum(self.Y[Yhat_neg_index]==0)-FP\n",
    "                FN = sum(self.Y[Yhat_neg_index]) - TP\n",
    "                p = (TP.astype(float)/(TP+FP+1))\n",
    "                p[rules_curr]=0\n",
    "                add_rule = sample(np.where(p==max(p))[0].tolist(),1)[0]\n",
    "            if add_rule not in rules_curr:\n",
    "                rules_curr.append(add_rule)\n",
    "                rules_norm = self.normalize(rules_curr)\n",
    "\n",
    "        if len(move)>0 and move[0]=='clean':\n",
    "            remove = []\n",
    "            for i,rule in enumerate(rules_norm):\n",
    "                Yhat = (np.sum(self.RMatrix[:,[rule for j,rule in enumerate(rules_norm) if (j!=i and j not in remove)]],axis = 1)>0).astype(int)\n",
    "                TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "                if TP+FP==0:\n",
    "                    remove.append(i)\n",
    "            for x in remove:\n",
    "                rules_norm.remove(x)\n",
    "            return rules_curr, rules_norm\n",
    "        return rules_curr, rules_norm\n",
    "\n",
    "    def compute_prob(self,rules):\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules],axis = 1)>0).astype(int)\n",
    "        TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "        Kn_count = list(np.bincount([self.rules_len[x] for x in rules], minlength = self.maxlen+1))\n",
    "        prior_ChsRules= sum([log_betabin(Kn_count[i],self.patternSpace[i],self.alpha_l[i],self.beta_l[i]) for i in range(1,len(Kn_count),1)])            \n",
    "        likelihood_1 =  log_betabin(TP,TP+FP,self.alpha_1,self.beta_1)\n",
    "        likelihood_2 = log_betabin(TN,FN+TN,self.alpha_2,self.beta_2)\n",
    "        return [TP,FP,TN,FN],[prior_ChsRules,likelihood_1,likelihood_2]\n",
    "\n",
    "    def normalize_add(self, rules_new, rule_index):\n",
    "        rules = rules_new.copy()\n",
    "        for rule in rules_new:\n",
    "            if set(self.rules[rule]).issubset(self.rules[rule_index]):\n",
    "                return rules_new.copy()\n",
    "            if set(self.rules[rule_index]).issubset(self.rules[rule]):\n",
    "                rules.remove(rule)\n",
    "        rules.append(rule_index)\n",
    "        return rules\n",
    "\n",
    "    def normalize(self, rules_new):\n",
    "        try:\n",
    "            rules_len = [len(self.rules[index]) for index in rules_new]\n",
    "            rules = [rules_new[i] for i in np.argsort(rules_len)[::-1][:len(rules_len)]]\n",
    "            p1 = 0\n",
    "            while p1<len(rules):\n",
    "                for p2 in range(p1+1,len(rules),1):\n",
    "                    if set(self.rules[rules[p2]]).issubset(set(self.rules[rules[p1]])):\n",
    "                        rules.remove(rules[p1])\n",
    "                        p1 -= 1\n",
    "                        break\n",
    "                p1 += 1\n",
    "            return rules\n",
    "        except:\n",
    "            return rules_new.copy()\n",
    "\n",
    "\n",
    "    def print_rules(self, rules_max):\n",
    "        for rule_index in rules_max:\n",
    "            print(self.rules[rule_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTook 3.591s to generate 19772 rules\n",
      "Screening rules using information gain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-958436519346>:206: RuntimeWarning: divide by zero encountered in log\n",
      "  cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
      "<ipython-input-2-958436519346>:206: RuntimeWarning: invalid value encountered in multiply\n",
      "  cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
      "<ipython-input-2-958436519346>:208: RuntimeWarning: divide by zero encountered in log\n",
      "  cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
      "<ipython-input-2-958436519346>:208: RuntimeWarning: invalid value encountered in multiply\n",
      "  cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
      "<ipython-input-2-958436519346>:265: RuntimeWarning: overflow encountered in exp\n",
      "  alpha = np.exp(float(pt_new -pt_curr)/T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTook 0.548s to generate 2000 rules\n",
      "Computing sizes for pattern space ...\n",
      "\tTook 0.000s to compute patternspace\n",
      "No or wrong input for alpha_l and beta_l. The model will use default parameters!\n",
      "found a more accurate model: accuracy = 0.5417910447761194\n",
      "\n",
      "** chain = 0, max at iter = 0 ** \n",
      " accuracy = 0.5417910447761194, TP = 249.0,FP = 117.0, TN = 114.0, FN = 190.0\n",
      " pt_new is -832.27859463412, prior_ChsRules=-55.76454119002847, likelihood_1 = -340.0904328557817, likelihood_2 = -436.42362058830986\n",
      " \n",
      "['5_X', '1_B', '9_O_neg']\n",
      "['5_X_neg', '5_O', '1_O']\n",
      "['9_O_neg', '5_O', '3_X_neg']\n",
      "['4_B_neg', '5_X_neg', '9_O']\n",
      "['3_O_neg', '5_O_neg', '7_O_neg']\n",
      "[1852, 839, 65, 33, 1958]\n",
      "found a more accurate model: accuracy = 0.6686567164179105\n",
      "\n",
      "** chain = 0, max at iter = 1 ** \n",
      " accuracy = 0.6686567164179105, TP = 334.0,FP = 117.0, TN = 114.0, FN = 105.0\n",
      " pt_new is -714.3872831851045, prior_ChsRules=-66.49072063146559, likelihood_1 = -351.919381133579, likelihood_2 = -295.9771814200599\n",
      " \n",
      "['5_X', '1_B', '9_O_neg']\n",
      "['5_X_neg', '5_O', '1_O']\n",
      "['9_O_neg', '5_O', '3_X_neg']\n",
      "['4_B_neg', '5_X_neg', '9_O']\n",
      "['3_O_neg', '5_O_neg', '7_O_neg']\n",
      "['5_O_neg', '1_O_neg', '9_O_neg']\n",
      "[1852, 839, 65, 33, 1958, 1831]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bisect_left' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/x/Desktop/Repositories/boa_m/BOA/example.py\u001b[0m in \u001b[0;36mline 448\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=445'>446</a>\u001b[0m model\u001b[39m.\u001b[39mgenerate_rules(supp,maxlen,N)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=446'>447</a>\u001b[0m model\u001b[39m.\u001b[39mset_parameters(alpha_1,beta_1,alpha_2,beta_2,\u001b[39mNone\u001b[39;00m,\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=447'>448</a>\u001b[0m rules \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(Niteration,Nchain,print_message\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=449'>450</a>\u001b[0m \u001b[39m# test\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=450'>451</a>\u001b[0m Yhat \u001b[39m=\u001b[39m predict(rules,df\u001b[39m.\u001b[39miloc[test_index])\n",
      "\u001b[1;32m/Users/x/Desktop/Repositories/boa_m/BOA/example.py\u001b[0m in \u001b[0;36mline 261\u001b[0m, in \u001b[0;36mBOA.fit\u001b[0;34m(self, Niteration, Nchain, q, init, keep_most_accurate_model, print_message)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=258'>259</a>\u001b[0m     rules_curr \u001b[39m=\u001b[39m maps[chain][index][\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=259'>260</a>\u001b[0m     rules_curr_norm \u001b[39m=\u001b[39m maps[chain][index][\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=260'>261</a>\u001b[0m rules_new, rules_norm \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropose(rules_curr\u001b[39m.\u001b[39;49mcopy(), rules_curr_norm\u001b[39m.\u001b[39;49mcopy(),q)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=261'>262</a>\u001b[0m cfmatrix,prob \u001b[39m=\u001b[39m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_prob(rules_new)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=262'>263</a>\u001b[0m T \u001b[39m=\u001b[39m T0\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39miter\u001b[39m\u001b[39m/\u001b[39mNiteration)\n",
      "\u001b[1;32m/Users/x/Desktop/Repositories/boa_m/BOA/example.py\u001b[0m in \u001b[0;36mline 338\u001b[0m, in \u001b[0;36mBOA.propose\u001b[0;34m(self, rules_curr, rules_norm, q)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=335'>336</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=336'>337</a>\u001b[0m         p \u001b[39m=\u001b[39m p\u001b[39m/\u001b[39mp[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=337'>338</a>\u001b[0m     index \u001b[39m=\u001b[39m find_lt(p,random\u001b[39m.\u001b[39;49mrandom())\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=338'>339</a>\u001b[0m     cut_rule \u001b[39m=\u001b[39m rules_curr[index]\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=339'>340</a>\u001b[0m rules_curr\u001b[39m.\u001b[39mremove(cut_rule)\n",
      "\u001b[1;32m/Users/x/Desktop/Repositories/boa_m/BOA/example.py\u001b[0m in \u001b[0;36mline 29\u001b[0m, in \u001b[0;36mfind_lt\u001b[0;34m(a, x)\n\u001b[1;32m     <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=26'>27</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_lt\u001b[39m(a, x):\n\u001b[1;32m     <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=27'>28</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Find rightmost value less than x\"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=28'>29</a>\u001b[0m     i \u001b[39m=\u001b[39m bisect_left(a, x)\n\u001b[1;32m     <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=29'>30</a>\u001b[0m     \u001b[39mif\u001b[39;00m i:\n\u001b[1;32m     <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=30'>31</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bisect_left' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# from fim import fpgrowth,fim --> this package is difficult to install. So the rule miner can be replaced by a random forest\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import chain, combinations\n",
    "import itertools\n",
    "from numpy.random import random\n",
    "from random import sample\n",
    "import time\n",
    "import operator\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csc_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def accumulate(iterable, func=operator.add):\n",
    "    'Return running totals'\n",
    "    # accumulate([1,2,3,4,5]) --> 1 3 6 10 15\n",
    "    # accumulate([1,2,3,4,5], operator.mul) --> 1 2 6 24 120\n",
    "    it = iter(iterable)\n",
    "    total = next(it)\n",
    "    yield total\n",
    "    for element in it:\n",
    "        total = func(total, element)\n",
    "        yield total\n",
    "\n",
    "def find_lt(a, x):\n",
    "    \"\"\" Find rightmost value less than x\"\"\"\n",
    "    i = bisect_left(a, x)\n",
    "    if i:\n",
    "        return int(i-1)\n",
    "    print('in find_lt,{}'.format(a))\n",
    "    raise ValueError\n",
    "\n",
    "\n",
    "def log_gampoiss(k,alpha,beta):\n",
    "    import math\n",
    "    k = int(k)\n",
    "    return math.lgamma(k+alpha)+alpha*np.log(beta)-math.lgamma(alpha)-math.lgamma(k+1)-(alpha+k)*np.log(1+beta)\n",
    "\n",
    "\n",
    "def log_betabin(k,n,alpha,beta):\n",
    "    import math\n",
    "    try:\n",
    "        Const =  math.lgamma(alpha + beta) - math.lgamma(alpha) - math.lgamma(beta)\n",
    "    except:\n",
    "        print('alpha = {}, beta = {}'.format(alpha,beta))\n",
    "    if isinstance(k,list) or isinstance(k,np.ndarray):\n",
    "        if len(k)!=len(n):\n",
    "            print('length of k is %d and length of n is %d'%(len(k),len(n)))\n",
    "            raise ValueError\n",
    "        lbeta = []\n",
    "        for ki,ni in zip(k,n):\n",
    "            # lbeta.append(math.lgamma(ni+1)- math.lgamma(ki+1) - math.lgamma(ni-ki+1) + math.lgamma(ki+alpha) + math.lgamma(ni-ki+beta) - math.lgamma(ni+alpha+beta) + Const)\n",
    "            lbeta.append(math.lgamma(ki+alpha) + math.lgamma(ni-ki+beta) - math.lgamma(ni+alpha+beta) + Const)\n",
    "        return np.array(lbeta)\n",
    "    else:\n",
    "        return math.lgamma(k+alpha) + math.lgamma(n-k+beta) - math.lgamma(n+alpha+beta) + Const\n",
    "        # return math.lgamma(n+1)- math.lgamma(k+1) - math.lgamma(n-k+1) + math.lgamma(k+alpha) + math.lgamma(n-k+beta) - math.lgamma(n+alpha+beta) + Const\n",
    "\n",
    "def getConfusion(Yhat,Y):\n",
    "    if len(Yhat)!=len(Y):\n",
    "        raise NameError('Yhat has different length')\n",
    "    TP = np.dot(np.array(Y),np.array(Yhat))\n",
    "    FP = np.sum(Yhat) - TP\n",
    "    TN = len(Y) - np.sum(Y)-FP\n",
    "    FN = len(Yhat) - np.sum(Yhat) - TN\n",
    "    return TP,FP,TN,FN\n",
    "\n",
    "def predict(rules,df):\n",
    "    Z = [[] for rule in rules]\n",
    "    dfn = 1-df #df has negative associations\n",
    "    dfn.columns = [name.strip() + '_neg' for name in df.columns]\n",
    "    df = pd.concat([df,dfn],axis = 1)\n",
    "    for i,rule in enumerate(rules):\n",
    "        Z[i] = (np.sum(df[list(rule)],axis=1)==len(rule)).astype(int)\n",
    "    Yhat = (np.sum(Z,axis=0)>0).astype(int)\n",
    "    return Yhat\n",
    "\n",
    "def extract_rules(tree, feature_names):\n",
    "    left      = tree.tree_.children_left\n",
    "    right     = tree.tree_.children_right\n",
    "    threshold = tree.tree_.threshold\n",
    "    features  = [feature_names[i] for i in tree.tree_.feature]\n",
    "    # get ids of child nodes\n",
    "    idx = np.argwhere(left == -1)[:,0]     \n",
    "\n",
    "    def recurse(left, right, child, lineage=None):          \n",
    "        if lineage is None:\n",
    "            lineage = []\n",
    "        if child in left:\n",
    "            parent = np.where(left == child)[0].item()\n",
    "            suffix = '_neg'\n",
    "        else:\n",
    "            parent = np.where(right == child)[0].item()\n",
    "            suffix = ''\n",
    "\n",
    "        #           lineage.append((parent, split, threshold[parent], features[parent]))\n",
    "        lineage.append((features[parent].strip()+suffix))\n",
    "\n",
    "        if parent == 0:\n",
    "            lineage.reverse()\n",
    "            return lineage\n",
    "        else:\n",
    "            return recurse(left, right, parent, lineage)   \n",
    "    rules = []\n",
    "    for child in idx:\n",
    "        rule = []\n",
    "        for node in recurse(left, right, child):\n",
    "            rule.append(node)\n",
    "        rules.append(rule)\n",
    "    return rules\n",
    "\n",
    "\n",
    "class BOA(object):\n",
    "    def __init__(self, binary_data,Y):\n",
    "        self.df = binary_data  \n",
    "        self.Y = Y\n",
    "        self.attributeLevelNum = defaultdict(int) \n",
    "        self.attributeNames = []\n",
    "        for i,name in enumerate(binary_data.columns):\n",
    "          attribute = name.split('_')[0]\n",
    "          self.attributeLevelNum[attribute] += 1\n",
    "          self.attributeNames.append(attribute)\n",
    "        self.attributeNames = list(set(self.attributeNames))\n",
    "        \n",
    "\n",
    "    def getPatternSpace(self):\n",
    "        print('Computing sizes for pattern space ...')\n",
    "        start_time = time.time()\n",
    "        \"\"\" compute the rule space from the levels in each attribute \"\"\"\n",
    "        for item in self.attributeNames:\n",
    "            self.attributeLevelNum[item+'_neg'] = self.attributeLevelNum[item]\n",
    "        self.patternSpace = np.zeros(self.maxlen+1)\n",
    "        tmp = [ item + '_neg' for item in self.attributeNames]\n",
    "        self.attributeNames.extend(tmp)\n",
    "        for k in range(1,self.maxlen+1,1):\n",
    "            for subset in combinations(self.attributeNames,k):\n",
    "                tmp = 1\n",
    "                for i in subset:\n",
    "                    tmp = tmp * self.attributeLevelNum[i]\n",
    "                self.patternSpace[k] = self.patternSpace[k] + tmp\n",
    "        print('\\tTook %0.3fs to compute patternspace' % (time.time() - start_time))               \n",
    "\n",
    "# This function generates rules that satisfy supp and maxlen using fpgrowth, then it selects the top N rules that make data have the biggest decrease in entropy\n",
    "# there are two ways to generate rules. fpgrowth can handle cases where the maxlen is small. If maxlen<=3, fpgrowth can generates rules much faster than randomforest. \n",
    "# If maxlen is big, fpgrowh tends to generate too many rules that overflow the memories. \n",
    "    def generate_rules(self,supp,maxlen,N, method = 'randomforest'):\n",
    "        self.maxlen = maxlen\n",
    "        self.supp = supp\n",
    "        df = 1-self.df #df has negative associations\n",
    "        df.columns = [name.strip() + '_neg' for name in self.df.columns]\n",
    "        df = pd.concat([self.df,df],axis = 1)\n",
    "#         if method =='fpgrowth' and maxlen<=3:\n",
    "#             itemMatrix = [[item for item in df.columns if row[item] ==1] for i,row in df.iterrows() ]  \n",
    "#             pindex = np.where(self.Y==1)[0]\n",
    "#             nindex = np.where(self.Y!=1)[0]\n",
    "#             print('Generating rules using fpgrowth')\n",
    "#             start_time = time.time()\n",
    "#             rules= fpgrowth([itemMatrix[i] for i in pindex],supp = supp,zmin = 1,zmax = maxlen)\n",
    "#             rules = [tuple(np.sort(rule[0])) for rule in rules]\n",
    "#             rules = list(set(rules))\n",
    "#             start_time = time.time()\n",
    "#             print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "#         else:\n",
    "        rules = []\n",
    "        start_time = time.time()\n",
    "        for length in range(1,maxlen+1,1):\n",
    "            n_estimators = min(pow(df.shape[1],length),4000)\n",
    "            clf = RandomForestClassifier(n_estimators = n_estimators,max_depth = length)\n",
    "            clf.fit(self.df,self.Y)\n",
    "            for n in range(n_estimators):\n",
    "                rules.extend(extract_rules(clf.estimators_[n],df.columns))\n",
    "        rules = [list(x) for x in set(tuple(x) for x in rules)]\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "        self.screen_rules(rules,df,N) # select the top N rules using secondary criteria, information gain\n",
    "        self.getPatternSpace()\n",
    "\n",
    "    def screen_rules(self,rules,df,N):\n",
    "        print('Screening rules using information gain')\n",
    "        start_time = time.time()\n",
    "        itemInd = {}\n",
    "        for i,name in enumerate(df.columns):\n",
    "            itemInd[name] = i\n",
    "        indices = np.array(list(itertools.chain.from_iterable([[itemInd[x] for x in rule] for rule in rules])))\n",
    "        len_rules = [len(rule) for rule in rules]\n",
    "        indptr =list(accumulate(len_rules))\n",
    "        indptr.insert(0,0)\n",
    "        indptr = np.array(indptr)\n",
    "        data = np.ones(len(indices))\n",
    "        ruleMatrix = csc_matrix((data,indices,indptr),shape = (len(df.columns),len(rules)))\n",
    "        mat = np.matrix(df) * ruleMatrix\n",
    "        lenMatrix = np.matrix([len_rules for i in range(df.shape[0])])\n",
    "        Z =  (mat ==lenMatrix).astype(int)\n",
    "        Zpos = [Z[i] for i in np.where(self.Y>0)][0]\n",
    "        TP = np.array(np.sum(Zpos,axis=0).tolist()[0])\n",
    "        supp_select = np.where(TP>=self.supp*sum(self.Y)/100)[0]\n",
    "        FP = np.array(np.sum(Z,axis = 0))[0] - TP\n",
    "        TN = len(self.Y) - np.sum(self.Y) - FP\n",
    "        FN = np.sum(self.Y) - TP\n",
    "        p1 = TP.astype(float)/(TP+FP)\n",
    "        p2 = FN.astype(float)/(FN+TN)\n",
    "        pp = (TP+FP).astype(float)/(TP+FP+TN+FN)\n",
    "        tpr = TP.astype(float)/(TP+FN)\n",
    "        fpr = FP.astype(float)/(FP+TN)\n",
    "        cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
    "        cond_entropy[p1*(1-p1)==0] = -((1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2)))[p1*(1-p1)==0]\n",
    "        cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
    "        cond_entropy[p1*(1-p1)*p2*(1-p2)==0] = 0\n",
    "        select = np.argsort(cond_entropy[supp_select])[::-1][-N:]\n",
    "        self.rules = [rules[i] for i in supp_select[select]]\n",
    "        self.RMatrix = np.array(Z[:,supp_select[select]])\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(self.rules)))\n",
    "\n",
    "    def set_parameters(self, a1=100,b1=10,a2=100,b2=10,al=None,bl=None):\n",
    "        # input al and bl are lists\n",
    "        # a1/(a1 + b1) and a2/(a2 + b2) should be a  bigger than 0.5. They correspond to alpha_+, beta_+ and alpha_- and beta_- from the paper. In fact, these four values play an important role and it needs to be tuned while following the a1/(a1 + b1) >0.5 and a2/(a2 + b2) >0.5 principle\n",
    "        self.alpha_1 = a1\n",
    "        self.beta_1 = b1\n",
    "        self.alpha_2 = a2\n",
    "        self.beta_2 = b2\n",
    "        if al ==None or bl==None or len(al)!=self.maxlen or len(bl)!=self.maxlen:\n",
    "            print('No or wrong input for alpha_l and beta_l. The model will use default parameters!')\n",
    "            self.C = [1.0/self.maxlen for i in range(self.maxlen)]\n",
    "            self.C.insert(0,-1)\n",
    "            self.alpha_l = [10 for i in range(self.maxlen+1)]\n",
    "            self.beta_l= [10*self.patternSpace[i]/self.C[i] for i in range(self.maxlen+1)]\n",
    "        else:\n",
    "            self.alpha_l=[1] + list(al)\n",
    "            self.beta_l = [1] + list(bl)\n",
    "\n",
    "    def fit(self, Niteration = 300, Nchain = 1, q = 0.1, init = [], keep_most_accurate_model = True,print_message=True):\n",
    "        # print('Searching for an optimal solution...')\n",
    "        start_time = time.time()\n",
    "        nRules = len(self.rules)\n",
    "        self.rules_len = [len(rule) for rule in self.rules]\n",
    "        maps = defaultdict(list)\n",
    "        T0 = 1000\n",
    "        split = 0.7*Niteration\n",
    "        acc = {}\n",
    "        most_accurate_model = defaultdict(list)\n",
    "        for chain in range(Nchain):\n",
    "            # initialize with a random pattern set\n",
    "            if init !=[]:\n",
    "                rules_curr = init.copy()\n",
    "            else:\n",
    "                N = sample(range(1,min(8,nRules),1),1)[0]\n",
    "                rules_curr = sample(range(nRules),N)\n",
    "            rules_curr_norm = self.normalize(rules_curr)\n",
    "            pt_curr = -100000000000\n",
    "            maps[chain].append([-1,[pt_curr/3,pt_curr/3,pt_curr/3],rules_curr,[self.rules[i] for i in rules_curr]])\n",
    "            acc[chain] = 0\n",
    "            for iter in range(Niteration):\n",
    "                if iter>=split:\n",
    "                    p = np.array(range(1+len(maps[chain])))\n",
    "                    p = np.array(list(accumulate(p)))\n",
    "                    p = p/p[-1]\n",
    "                    index = find_lt(p,random.random())\n",
    "                    rules_curr = maps[chain][index][2].copy()\n",
    "                    rules_curr_norm = maps[chain][index][2].copy()\n",
    "                rules_new, rules_norm = self.propose(rules_curr.copy(), rules_curr_norm.copy(),q)\n",
    "                cfmatrix,prob =  self.compute_prob(rules_new)\n",
    "                T = T0**(1 - iter/Niteration)\n",
    "                pt_new = sum(prob)\n",
    "                alpha = np.exp(float(pt_new -pt_curr)/T)\n",
    "                \n",
    "                if (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)> acc[chain]:\n",
    "                    most_accurate_model[chain] = rules_new[:]\n",
    "                    acc[chain] = (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)\n",
    "                    if print_message:\n",
    "                        print('found a more accurate model: accuracy = {}'.format(acc[chain]))\n",
    "                        \n",
    "                if pt_new > sum(maps[chain][-1][1]):\n",
    "                    maps[chain].append([iter,prob,rules_new,[self.rules[i] for i in rules_new]])\n",
    "                    if print_message:\n",
    "                        print('\\n** chain = {}, max at iter = {} ** \\n accuracy = {}, TP = {},FP = {}, TN = {}, FN = {}\\n pt_new is {}, prior_ChsRules={}, likelihood_1 = {}, likelihood_2 = {}\\n '.format(chain, iter,(cfmatrix[0]+cfmatrix[2]+0.0)/len(self.Y),cfmatrix[0],cfmatrix[1],cfmatrix[2],cfmatrix[3],sum(prob), prob[0], prob[1], prob[2]))\n",
    "                        # print '\\n** chain = {}, max at iter = {} ** \\n obj = {}, prior = {}, llh = {} '.format(chain, iter,prior+llh,prior,llh)\n",
    "                        self.print_rules(rules_new)\n",
    "                        print(rules_new)\n",
    "                if random.random() <= alpha:\n",
    "                    rules_curr_norm,rules_curr,pt_curr = rules_norm.copy(),rules_new.copy(),pt_new\n",
    "        # print('\\tTook %0.3fs to generate an optimal rule set' % (time.time() - start_time))\n",
    "        if keep_most_accurate_model:\n",
    "            acc_list = [acc[chain] for chain in range(Nchain)]\n",
    "            index = acc_list.index(max(acc_list))\n",
    "            return [self.rules[i] for i in most_accurate_model[index]] \n",
    "        else:\n",
    "            pt_max = [sum(maps[chain][-1][1]) for chain in range(Nchain)]\n",
    "            index = pt_max.index(max(pt_max))\n",
    "            return maps[index][-1][3]\n",
    "\n",
    "    def propose(self, rules_curr,rules_norm,q):\n",
    "        nRules = len(self.rules)\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules_curr],axis = 1)>0).astype(int)\n",
    "        incorr = np.where(self.Y!=Yhat)[0]\n",
    "        N = len(rules_curr)\n",
    "        if len(incorr)==0:\n",
    "            clean = True\n",
    "            move = ['clean']\n",
    "            # it means the BOA correctly classified all points but there could be redundant patterns, so cleaning is needed\n",
    "        else:\n",
    "            clean = False\n",
    "            ex = sample(incorr.tolist(),1)[0]\n",
    "            t = random.random()\n",
    "            if self.Y[ex]==1 or N==1:\n",
    "                if t<1.0/2 or N==1:\n",
    "                    move = ['add']       # action: add\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "            else:\n",
    "                if t<1.0/2:\n",
    "                    move = ['cut']       # action: cut\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "        if move[0]=='cut':\n",
    "            \"\"\" cut \"\"\"\n",
    "            if random.random()<q:\n",
    "                candidate = list(set(np.where(self.RMatrix[ex,:]==1)[0]).intersection(rules_curr))\n",
    "                if len(candidate)==0:\n",
    "                    candidate = rules_curr\n",
    "                cut_rule = sample(candidate,1)[0]\n",
    "            else:\n",
    "                p = []\n",
    "                all_sum = np.sum(self.RMatrix[:,rules_curr],axis = 1)\n",
    "                for index,rule in enumerate(rules_curr):\n",
    "                    Yhat= ((all_sum - np.array(self.RMatrix[:,rule]))>0).astype(int)\n",
    "                    TP,FP,TN,FN  = getConfusion(Yhat,self.Y)\n",
    "                    p.append(TP.astype(float)/(TP+FP+1))\n",
    "                    # p.append(log_betabin(TP,TP+FP,self.alpha_1,self.beta_1) + log_betabin(FN,FN+TN,self.alpha_2,self.beta_2))\n",
    "                p = [x - min(p) for x in p]\n",
    "                p = np.exp(p)\n",
    "                p = np.insert(p,0,0)\n",
    "                p = np.array(list(accumulate(p)))\n",
    "                if p[-1]==0:\n",
    "                    index = sample(range(len(rules_curr)),1)[0]\n",
    "                else:\n",
    "                    p = p/p[-1]\n",
    "                index = find_lt(p,random.random())\n",
    "                cut_rule = rules_curr[index]\n",
    "            rules_curr.remove(cut_rule)\n",
    "            rules_norm = self.normalize(rules_curr)\n",
    "            move.remove('cut')\n",
    "            \n",
    "        if len(move)>0 and move[0]=='add':\n",
    "            \"\"\" add \"\"\"\n",
    "            if random.random()<q:\n",
    "                add_rule = sample(range(nRules),1)[0]\n",
    "            else: \n",
    "                Yhat_neg_index = list(np.where(np.sum(self.RMatrix[:,rules_curr],axis = 1)<1)[0])\n",
    "                mat = np.multiply(self.RMatrix[Yhat_neg_index,:].transpose(),self.Y[Yhat_neg_index])\n",
    "                # TP = np.array(np.sum(mat,axis = 0).tolist()[0])\n",
    "                TP = np.sum(mat,axis = 1)\n",
    "                FP = np.array((np.sum(self.RMatrix[Yhat_neg_index,:],axis = 0) - TP))\n",
    "                TN = np.sum(self.Y[Yhat_neg_index]==0)-FP\n",
    "                FN = sum(self.Y[Yhat_neg_index]) - TP\n",
    "                p = (TP.astype(float)/(TP+FP+1))\n",
    "                p[rules_curr]=0\n",
    "                add_rule = sample(np.where(p==max(p))[0].tolist(),1)[0]\n",
    "            if add_rule not in rules_curr:\n",
    "                rules_curr.append(add_rule)\n",
    "                rules_norm = self.normalize(rules_curr)\n",
    "\n",
    "        if len(move)>0 and move[0]=='clean':\n",
    "            remove = []\n",
    "            for i,rule in enumerate(rules_norm):\n",
    "                Yhat = (np.sum(self.RMatrix[:,[rule for j,rule in enumerate(rules_norm) if (j!=i and j not in remove)]],axis = 1)>0).astype(int)\n",
    "                TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "                if TP+FP==0:\n",
    "                    remove.append(i)\n",
    "            for x in remove:\n",
    "                rules_norm.remove(x)\n",
    "            return rules_curr, rules_norm\n",
    "        return rules_curr, rules_norm\n",
    "\n",
    "    def compute_prob(self,rules):\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules],axis = 1)>0).astype(int)\n",
    "        TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "        Kn_count = list(np.bincount([self.rules_len[x] for x in rules], minlength = self.maxlen+1))\n",
    "        prior_ChsRules= sum([log_betabin(Kn_count[i],self.patternSpace[i],self.alpha_l[i],self.beta_l[i]) for i in range(1,len(Kn_count),1)])            \n",
    "        likelihood_1 =  log_betabin(TP,TP+FP,self.alpha_1,self.beta_1)\n",
    "        likelihood_2 = log_betabin(TN,FN+TN,self.alpha_2,self.beta_2)\n",
    "        return [TP,FP,TN,FN],[prior_ChsRules,likelihood_1,likelihood_2]\n",
    "\n",
    "    def normalize_add(self, rules_new, rule_index):\n",
    "        rules = rules_new.copy()\n",
    "        for rule in rules_new:\n",
    "            if set(self.rules[rule]).issubset(self.rules[rule_index]):\n",
    "                return rules_new.copy()\n",
    "            if set(self.rules[rule_index]).issubset(self.rules[rule]):\n",
    "                rules.remove(rule)\n",
    "        rules.append(rule_index)\n",
    "        return rules\n",
    "\n",
    "    def normalize(self, rules_new):\n",
    "        try:\n",
    "            rules_len = [len(self.rules[index]) for index in rules_new]\n",
    "            rules = [rules_new[i] for i in np.argsort(rules_len)[::-1][:len(rules_len)]]\n",
    "            p1 = 0\n",
    "            while p1<len(rules):\n",
    "                for p2 in range(p1+1,len(rules),1):\n",
    "                    if set(self.rules[rules[p2]]).issubset(set(self.rules[rules[p1]])):\n",
    "                        rules.remove(rules[p1])\n",
    "                        p1 -= 1\n",
    "                        break\n",
    "                p1 += 1\n",
    "            return rules\n",
    "        except:\n",
    "            return rules_new.copy()\n",
    "\n",
    "\n",
    "    def print_rules(self, rules_max):\n",
    "        for rule_index in rules_max:\n",
    "            print(self.rules[rule_index])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" parameters \"\"\"\n",
    "# The following parameters are recommended to change depending on the size and complexity of the data\n",
    "N = 2000      # number of rules to be used in SA_patternbased and also the output of generate_rules\n",
    "Niteration = 500  # number of iterations in each chain\n",
    "Nchain = 2         # number of chains in the simulated annealing search algorithm\n",
    "\n",
    "supp = 5           # 5% is a generally good number. The higher this supp, the 'larger' a pattern is\n",
    "maxlen = 3         # maxmum length of a pattern\n",
    "\n",
    "# \\rho = alpha/(alpha+beta). Make sure \\rho is close to one when choosing alpha and beta. \n",
    "alpha_1 = 500       # alpha_+\n",
    "beta_1 = 1          # beta_+\n",
    "alpha_2 = 500         # alpha_-\n",
    "beta_2 = 1       # beta_-\n",
    "\n",
    "\"\"\" input file \"\"\"\n",
    "# notice that in the example, X is already binary coded. \n",
    "# Data has to be binary coded and the column name shd have the form: attributename_attributevalue\n",
    "filepathX = 'tictactoe_X.txt' # input file X\n",
    "filepathY = 'tictactoe_Y.txt' # input file Y\n",
    "df = pd.read_csv(filepathX,header=0,sep=\" \")\n",
    "Y = np.loadtxt(open(filepathY,\"rb\"),delimiter=\" \")\n",
    "\n",
    "import random\n",
    "lenY = len(Y)\n",
    "train_index = random.sample(range(lenY), int(0.70 * lenY))\n",
    "test_index = [i for i in range(lenY) if i not in train_index]\n",
    "\n",
    "model = BOA(df.iloc[train_index],Y[train_index])\n",
    "model.generate_rules(supp,maxlen,N)\n",
    "model.set_parameters(alpha_1,beta_1,alpha_2,beta_2,None,None)\n",
    "rules = model.fit(Niteration,Nchain,print_message=True)\n",
    "\n",
    "# test\n",
    "Yhat = predict(rules,df.iloc[test_index])\n",
    "TP,FP,TN,FN = getConfusion(Yhat,Y[test_index])\n",
    "tpr = float(TP)/(TP+FN)\n",
    "fpr = float(FP)/(FP+TN)\n",
    "print('TP = {}, FP = {}, TN = {}, FN = {} \\n accuracy = {}, tpr = {}, fpr = {}'.format(TP,FP,TN,FN, float(TP+TN)/(TP+TN+FP+FN),tpr,fpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTook 3.722s to generate 19128 rules\n",
      "Screening rules using information gain\n",
      "\tTook 0.493s to generate 2000 rules\n",
      "Computing sizes for pattern space ...\n",
      "\tTook 0.000s to compute patternspace\n",
      "No or wrong input for alpha_l and beta_l. The model will use default parameters!\n",
      "found a more accurate model: accuracy = 0.6268656716417911\n",
      "\n",
      "** chain = 0, max at iter = 0 ** \n",
      " accuracy = 0.6268656716417911, TP = 216.0,FP = 27.0, TN = 204.0, FN = 223.0\n",
      " pt_new is -642.5921753983212, prior_ChsRules=-20.473457780984972, likelihood_1 = -113.81219883246558, likelihood_2 = -508.3065187848706\n",
      " \n",
      "['5_O_neg', '6_O']\n",
      "['9_O_neg', '1_O_neg', '5_O_neg']\n",
      "[502, 1833]\n",
      "found a more accurate model: accuracy = 0.7149253731343284\n",
      "\n",
      "** chain = 0, max at iter = 1 ** \n",
      " accuracy = 0.7149253731343284, TP = 275.0,FP = 27.0, TN = 204.0, FN = 164.0\n",
      " pt_new is -565.1817153618795, prior_ChsRules=-31.50979800647292, likelihood_1 = -115.99009123329961, likelihood_2 = -417.68182612210694\n",
      " \n",
      "['5_O_neg', '6_O']\n",
      "['9_O_neg', '1_O_neg', '5_O_neg']\n",
      "['5_O_neg', '7_O_neg', '3_O_neg']\n",
      "[502, 1833, 1923]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-958436519346>:206: RuntimeWarning: divide by zero encountered in log\n",
      "  cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
      "<ipython-input-3-958436519346>:206: RuntimeWarning: invalid value encountered in multiply\n",
      "  cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
      "<ipython-input-3-958436519346>:208: RuntimeWarning: divide by zero encountered in log\n",
      "  cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
      "<ipython-input-3-958436519346>:208: RuntimeWarning: invalid value encountered in multiply\n",
      "  cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
      "<ipython-input-3-958436519346>:265: RuntimeWarning: overflow encountered in exp\n",
      "  alpha = np.exp(float(pt_new -pt_curr)/T)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bisect_left' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/x/Desktop/Repositories/boa_m/BOA/example.py\u001b[0m in \u001b[0;36mline 448\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=445'>446</a>\u001b[0m model\u001b[39m.\u001b[39mgenerate_rules(supp,maxlen,N)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=446'>447</a>\u001b[0m model\u001b[39m.\u001b[39mset_parameters(alpha_1,beta_1,alpha_2,beta_2,\u001b[39mNone\u001b[39;00m,\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=447'>448</a>\u001b[0m rules \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(Niteration,Nchain,print_message\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=449'>450</a>\u001b[0m \u001b[39m# test\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=450'>451</a>\u001b[0m Yhat \u001b[39m=\u001b[39m predict(rules,df\u001b[39m.\u001b[39miloc[test_index])\n",
      "\u001b[1;32m/Users/x/Desktop/Repositories/boa_m/BOA/example.py\u001b[0m in \u001b[0;36mline 261\u001b[0m, in \u001b[0;36mBOA.fit\u001b[0;34m(self, Niteration, Nchain, q, init, keep_most_accurate_model, print_message)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=258'>259</a>\u001b[0m     rules_curr \u001b[39m=\u001b[39m maps[chain][index][\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=259'>260</a>\u001b[0m     rules_curr_norm \u001b[39m=\u001b[39m maps[chain][index][\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=260'>261</a>\u001b[0m rules_new, rules_norm \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropose(rules_curr\u001b[39m.\u001b[39;49mcopy(), rules_curr_norm\u001b[39m.\u001b[39;49mcopy(),q)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=261'>262</a>\u001b[0m cfmatrix,prob \u001b[39m=\u001b[39m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_prob(rules_new)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=262'>263</a>\u001b[0m T \u001b[39m=\u001b[39m T0\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39miter\u001b[39m\u001b[39m/\u001b[39mNiteration)\n",
      "\u001b[1;32m/Users/x/Desktop/Repositories/boa_m/BOA/example.py\u001b[0m in \u001b[0;36mline 338\u001b[0m, in \u001b[0;36mBOA.propose\u001b[0;34m(self, rules_curr, rules_norm, q)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=335'>336</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=336'>337</a>\u001b[0m         p \u001b[39m=\u001b[39m p\u001b[39m/\u001b[39mp[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=337'>338</a>\u001b[0m     index \u001b[39m=\u001b[39m find_lt(p,random\u001b[39m.\u001b[39;49mrandom())\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=338'>339</a>\u001b[0m     cut_rule \u001b[39m=\u001b[39m rules_curr[index]\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=339'>340</a>\u001b[0m rules_curr\u001b[39m.\u001b[39mremove(cut_rule)\n",
      "\u001b[1;32m/Users/x/Desktop/Repositories/boa_m/BOA/example.py\u001b[0m in \u001b[0;36mline 29\u001b[0m, in \u001b[0;36mfind_lt\u001b[0;34m(a, x)\n\u001b[1;32m     <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=26'>27</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_lt\u001b[39m(a, x):\n\u001b[1;32m     <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=27'>28</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Find rightmost value less than x\"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=28'>29</a>\u001b[0m     i \u001b[39m=\u001b[39m bisect_left(a, x)\n\u001b[1;32m     <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=29'>30</a>\u001b[0m     \u001b[39mif\u001b[39;00m i:\n\u001b[1;32m     <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=30'>31</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bisect_left' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# from fim import fpgrowth,fim --> this package is difficult to install. So the rule miner can be replaced by a random forest\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import chain, combinations\n",
    "import itertools\n",
    "from numpy.random import random\n",
    "from random import sample\n",
    "import time\n",
    "import operator\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csc_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def accumulate(iterable, func=operator.add):\n",
    "    'Return running totals'\n",
    "    # accumulate([1,2,3,4,5]) --> 1 3 6 10 15\n",
    "    # accumulate([1,2,3,4,5], operator.mul) --> 1 2 6 24 120\n",
    "    it = iter(iterable)\n",
    "    total = next(it)\n",
    "    yield total\n",
    "    for element in it:\n",
    "        total = func(total, element)\n",
    "        yield total\n",
    "\n",
    "def find_lt(a, x):\n",
    "    \"\"\" Find rightmost value less than x\"\"\"\n",
    "    i = bisect_left(a, x)\n",
    "    if i:\n",
    "        return int(i-1)\n",
    "    print('in find_lt,{}'.format(a))\n",
    "    raise ValueError\n",
    "\n",
    "\n",
    "def log_gampoiss(k,alpha,beta):\n",
    "    import math\n",
    "    k = int(k)\n",
    "    return math.lgamma(k+alpha)+alpha*np.log(beta)-math.lgamma(alpha)-math.lgamma(k+1)-(alpha+k)*np.log(1+beta)\n",
    "\n",
    "\n",
    "def log_betabin(k,n,alpha,beta):\n",
    "    import math\n",
    "    try:\n",
    "        Const =  math.lgamma(alpha + beta) - math.lgamma(alpha) - math.lgamma(beta)\n",
    "    except:\n",
    "        print('alpha = {}, beta = {}'.format(alpha,beta))\n",
    "    if isinstance(k,list) or isinstance(k,np.ndarray):\n",
    "        if len(k)!=len(n):\n",
    "            print('length of k is %d and length of n is %d'%(len(k),len(n)))\n",
    "            raise ValueError\n",
    "        lbeta = []\n",
    "        for ki,ni in zip(k,n):\n",
    "            # lbeta.append(math.lgamma(ni+1)- math.lgamma(ki+1) - math.lgamma(ni-ki+1) + math.lgamma(ki+alpha) + math.lgamma(ni-ki+beta) - math.lgamma(ni+alpha+beta) + Const)\n",
    "            lbeta.append(math.lgamma(ki+alpha) + math.lgamma(ni-ki+beta) - math.lgamma(ni+alpha+beta) + Const)\n",
    "        return np.array(lbeta)\n",
    "    else:\n",
    "        return math.lgamma(k+alpha) + math.lgamma(n-k+beta) - math.lgamma(n+alpha+beta) + Const\n",
    "        # return math.lgamma(n+1)- math.lgamma(k+1) - math.lgamma(n-k+1) + math.lgamma(k+alpha) + math.lgamma(n-k+beta) - math.lgamma(n+alpha+beta) + Const\n",
    "\n",
    "def getConfusion(Yhat,Y):\n",
    "    if len(Yhat)!=len(Y):\n",
    "        raise NameError('Yhat has different length')\n",
    "    TP = np.dot(np.array(Y),np.array(Yhat))\n",
    "    FP = np.sum(Yhat) - TP\n",
    "    TN = len(Y) - np.sum(Y)-FP\n",
    "    FN = len(Yhat) - np.sum(Yhat) - TN\n",
    "    return TP,FP,TN,FN\n",
    "\n",
    "def predict(rules,df):\n",
    "    Z = [[] for rule in rules]\n",
    "    dfn = 1-df #df has negative associations\n",
    "    dfn.columns = [name.strip() + '_neg' for name in df.columns]\n",
    "    df = pd.concat([df,dfn],axis = 1)\n",
    "    for i,rule in enumerate(rules):\n",
    "        Z[i] = (np.sum(df[list(rule)],axis=1)==len(rule)).astype(int)\n",
    "    Yhat = (np.sum(Z,axis=0)>0).astype(int)\n",
    "    return Yhat\n",
    "\n",
    "def extract_rules(tree, feature_names):\n",
    "    left      = tree.tree_.children_left\n",
    "    right     = tree.tree_.children_right\n",
    "    threshold = tree.tree_.threshold\n",
    "    features  = [feature_names[i] for i in tree.tree_.feature]\n",
    "    # get ids of child nodes\n",
    "    idx = np.argwhere(left == -1)[:,0]     \n",
    "\n",
    "    def recurse(left, right, child, lineage=None):          \n",
    "        if lineage is None:\n",
    "            lineage = []\n",
    "        if child in left:\n",
    "            parent = np.where(left == child)[0].item()\n",
    "            suffix = '_neg'\n",
    "        else:\n",
    "            parent = np.where(right == child)[0].item()\n",
    "            suffix = ''\n",
    "\n",
    "        #           lineage.append((parent, split, threshold[parent], features[parent]))\n",
    "        lineage.append((features[parent].strip()+suffix))\n",
    "\n",
    "        if parent == 0:\n",
    "            lineage.reverse()\n",
    "            return lineage\n",
    "        else:\n",
    "            return recurse(left, right, parent, lineage)   \n",
    "    rules = []\n",
    "    for child in idx:\n",
    "        rule = []\n",
    "        for node in recurse(left, right, child):\n",
    "            rule.append(node)\n",
    "        rules.append(rule)\n",
    "    return rules\n",
    "\n",
    "\n",
    "class BOA(object):\n",
    "    def __init__(self, binary_data,Y):\n",
    "        self.df = binary_data  \n",
    "        self.Y = Y\n",
    "        self.attributeLevelNum = defaultdict(int) \n",
    "        self.attributeNames = []\n",
    "        for i,name in enumerate(binary_data.columns):\n",
    "          attribute = name.split('_')[0]\n",
    "          self.attributeLevelNum[attribute] += 1\n",
    "          self.attributeNames.append(attribute)\n",
    "        self.attributeNames = list(set(self.attributeNames))\n",
    "        \n",
    "\n",
    "    def getPatternSpace(self):\n",
    "        print('Computing sizes for pattern space ...')\n",
    "        start_time = time.time()\n",
    "        \"\"\" compute the rule space from the levels in each attribute \"\"\"\n",
    "        for item in self.attributeNames:\n",
    "            self.attributeLevelNum[item+'_neg'] = self.attributeLevelNum[item]\n",
    "        self.patternSpace = np.zeros(self.maxlen+1)\n",
    "        tmp = [ item + '_neg' for item in self.attributeNames]\n",
    "        self.attributeNames.extend(tmp)\n",
    "        for k in range(1,self.maxlen+1,1):\n",
    "            for subset in combinations(self.attributeNames,k):\n",
    "                tmp = 1\n",
    "                for i in subset:\n",
    "                    tmp = tmp * self.attributeLevelNum[i]\n",
    "                self.patternSpace[k] = self.patternSpace[k] + tmp\n",
    "        print('\\tTook %0.3fs to compute patternspace' % (time.time() - start_time))               \n",
    "\n",
    "# This function generates rules that satisfy supp and maxlen using fpgrowth, then it selects the top N rules that make data have the biggest decrease in entropy\n",
    "# there are two ways to generate rules. fpgrowth can handle cases where the maxlen is small. If maxlen<=3, fpgrowth can generates rules much faster than randomforest. \n",
    "# If maxlen is big, fpgrowh tends to generate too many rules that overflow the memories. \n",
    "    def generate_rules(self,supp,maxlen,N, method = 'randomforest'):\n",
    "        self.maxlen = maxlen\n",
    "        self.supp = supp\n",
    "        df = 1-self.df #df has negative associations\n",
    "        df.columns = [name.strip() + '_neg' for name in self.df.columns]\n",
    "        df = pd.concat([self.df,df],axis = 1)\n",
    "#         if method =='fpgrowth' and maxlen<=3:\n",
    "#             itemMatrix = [[item for item in df.columns if row[item] ==1] for i,row in df.iterrows() ]  \n",
    "#             pindex = np.where(self.Y==1)[0]\n",
    "#             nindex = np.where(self.Y!=1)[0]\n",
    "#             print('Generating rules using fpgrowth')\n",
    "#             start_time = time.time()\n",
    "#             rules= fpgrowth([itemMatrix[i] for i in pindex],supp = supp,zmin = 1,zmax = maxlen)\n",
    "#             rules = [tuple(np.sort(rule[0])) for rule in rules]\n",
    "#             rules = list(set(rules))\n",
    "#             start_time = time.time()\n",
    "#             print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "#         else:\n",
    "        rules = []\n",
    "        start_time = time.time()\n",
    "        for length in range(1,maxlen+1,1):\n",
    "            n_estimators = min(pow(df.shape[1],length),4000)\n",
    "            clf = RandomForestClassifier(n_estimators = n_estimators,max_depth = length)\n",
    "            clf.fit(self.df,self.Y)\n",
    "            for n in range(n_estimators):\n",
    "                rules.extend(extract_rules(clf.estimators_[n],df.columns))\n",
    "        rules = [list(x) for x in set(tuple(x) for x in rules)]\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "        self.screen_rules(rules,df,N) # select the top N rules using secondary criteria, information gain\n",
    "        self.getPatternSpace()\n",
    "\n",
    "    def screen_rules(self,rules,df,N):\n",
    "        print('Screening rules using information gain')\n",
    "        start_time = time.time()\n",
    "        itemInd = {}\n",
    "        for i,name in enumerate(df.columns):\n",
    "            itemInd[name] = i\n",
    "        indices = np.array(list(itertools.chain.from_iterable([[itemInd[x] for x in rule] for rule in rules])))\n",
    "        len_rules = [len(rule) for rule in rules]\n",
    "        indptr =list(accumulate(len_rules))\n",
    "        indptr.insert(0,0)\n",
    "        indptr = np.array(indptr)\n",
    "        data = np.ones(len(indices))\n",
    "        ruleMatrix = csc_matrix((data,indices,indptr),shape = (len(df.columns),len(rules)))\n",
    "        mat = np.matrix(df) * ruleMatrix\n",
    "        lenMatrix = np.matrix([len_rules for i in range(df.shape[0])])\n",
    "        Z =  (mat ==lenMatrix).astype(int)\n",
    "        Zpos = [Z[i] for i in np.where(self.Y>0)][0]\n",
    "        TP = np.array(np.sum(Zpos,axis=0).tolist()[0])\n",
    "        supp_select = np.where(TP>=self.supp*sum(self.Y)/100)[0]\n",
    "        FP = np.array(np.sum(Z,axis = 0))[0] - TP\n",
    "        TN = len(self.Y) - np.sum(self.Y) - FP\n",
    "        FN = np.sum(self.Y) - TP\n",
    "        p1 = TP.astype(float)/(TP+FP)\n",
    "        p2 = FN.astype(float)/(FN+TN)\n",
    "        pp = (TP+FP).astype(float)/(TP+FP+TN+FN)\n",
    "        tpr = TP.astype(float)/(TP+FN)\n",
    "        fpr = FP.astype(float)/(FP+TN)\n",
    "        cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
    "        cond_entropy[p1*(1-p1)==0] = -((1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2)))[p1*(1-p1)==0]\n",
    "        cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
    "        cond_entropy[p1*(1-p1)*p2*(1-p2)==0] = 0\n",
    "        select = np.argsort(cond_entropy[supp_select])[::-1][-N:]\n",
    "        self.rules = [rules[i] for i in supp_select[select]]\n",
    "        self.RMatrix = np.array(Z[:,supp_select[select]])\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(self.rules)))\n",
    "\n",
    "    def set_parameters(self, a1=100,b1=10,a2=100,b2=10,al=None,bl=None):\n",
    "        # input al and bl are lists\n",
    "        # a1/(a1 + b1) and a2/(a2 + b2) should be a  bigger than 0.5. They correspond to alpha_+, beta_+ and alpha_- and beta_- from the paper. In fact, these four values play an important role and it needs to be tuned while following the a1/(a1 + b1) >0.5 and a2/(a2 + b2) >0.5 principle\n",
    "        self.alpha_1 = a1\n",
    "        self.beta_1 = b1\n",
    "        self.alpha_2 = a2\n",
    "        self.beta_2 = b2\n",
    "        if al ==None or bl==None or len(al)!=self.maxlen or len(bl)!=self.maxlen:\n",
    "            print('No or wrong input for alpha_l and beta_l. The model will use default parameters!')\n",
    "            self.C = [1.0/self.maxlen for i in range(self.maxlen)]\n",
    "            self.C.insert(0,-1)\n",
    "            self.alpha_l = [10 for i in range(self.maxlen+1)]\n",
    "            self.beta_l= [10*self.patternSpace[i]/self.C[i] for i in range(self.maxlen+1)]\n",
    "        else:\n",
    "            self.alpha_l=[1] + list(al)\n",
    "            self.beta_l = [1] + list(bl)\n",
    "\n",
    "    def fit(self, Niteration = 300, Nchain = 1, q = 0.1, init = [], keep_most_accurate_model = True,print_message=True):\n",
    "        # print('Searching for an optimal solution...')\n",
    "        start_time = time.time()\n",
    "        nRules = len(self.rules)\n",
    "        self.rules_len = [len(rule) for rule in self.rules]\n",
    "        maps = defaultdict(list)\n",
    "        T0 = 1000\n",
    "        split = 0.7*Niteration\n",
    "        acc = {}\n",
    "        most_accurate_model = defaultdict(list)\n",
    "        for chain in range(Nchain):\n",
    "            # initialize with a random pattern set\n",
    "            if init !=[]:\n",
    "                rules_curr = init.copy()\n",
    "            else:\n",
    "                N = sample(range(1,min(8,nRules),1),1)[0]\n",
    "                rules_curr = sample(range(nRules),N)\n",
    "            rules_curr_norm = self.normalize(rules_curr)\n",
    "            pt_curr = -100000000000\n",
    "            maps[chain].append([-1,[pt_curr/3,pt_curr/3,pt_curr/3],rules_curr,[self.rules[i] for i in rules_curr]])\n",
    "            acc[chain] = 0\n",
    "            for iter in range(Niteration):\n",
    "                if iter>=split:\n",
    "                    p = np.array(range(1+len(maps[chain])))\n",
    "                    p = np.array(list(accumulate(p)))\n",
    "                    p = p/p[-1]\n",
    "                    index = find_lt(p,random.random())\n",
    "                    rules_curr = maps[chain][index][2].copy()\n",
    "                    rules_curr_norm = maps[chain][index][2].copy()\n",
    "                rules_new, rules_norm = self.propose(rules_curr.copy(), rules_curr_norm.copy(),q)\n",
    "                cfmatrix,prob =  self.compute_prob(rules_new)\n",
    "                T = T0**(1 - iter/Niteration)\n",
    "                pt_new = sum(prob)\n",
    "                alpha = np.exp(float(pt_new -pt_curr)/T)\n",
    "                \n",
    "                if (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)> acc[chain]:\n",
    "                    most_accurate_model[chain] = rules_new[:]\n",
    "                    acc[chain] = (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)\n",
    "                    if print_message:\n",
    "                        print('found a more accurate model: accuracy = {}'.format(acc[chain]))\n",
    "                        \n",
    "                if pt_new > sum(maps[chain][-1][1]):\n",
    "                    maps[chain].append([iter,prob,rules_new,[self.rules[i] for i in rules_new]])\n",
    "                    if print_message:\n",
    "                        print('\\n** chain = {}, max at iter = {} ** \\n accuracy = {}, TP = {},FP = {}, TN = {}, FN = {}\\n pt_new is {}, prior_ChsRules={}, likelihood_1 = {}, likelihood_2 = {}\\n '.format(chain, iter,(cfmatrix[0]+cfmatrix[2]+0.0)/len(self.Y),cfmatrix[0],cfmatrix[1],cfmatrix[2],cfmatrix[3],sum(prob), prob[0], prob[1], prob[2]))\n",
    "                        # print '\\n** chain = {}, max at iter = {} ** \\n obj = {}, prior = {}, llh = {} '.format(chain, iter,prior+llh,prior,llh)\n",
    "                        self.print_rules(rules_new)\n",
    "                        print(rules_new)\n",
    "                if random.random() <= alpha:\n",
    "                    rules_curr_norm,rules_curr,pt_curr = rules_norm.copy(),rules_new.copy(),pt_new\n",
    "        # print('\\tTook %0.3fs to generate an optimal rule set' % (time.time() - start_time))\n",
    "        if keep_most_accurate_model:\n",
    "            acc_list = [acc[chain] for chain in range(Nchain)]\n",
    "            index = acc_list.index(max(acc_list))\n",
    "            return [self.rules[i] for i in most_accurate_model[index]] \n",
    "        else:\n",
    "            pt_max = [sum(maps[chain][-1][1]) for chain in range(Nchain)]\n",
    "            index = pt_max.index(max(pt_max))\n",
    "            return maps[index][-1][3]\n",
    "\n",
    "    def propose(self, rules_curr,rules_norm,q):\n",
    "        nRules = len(self.rules)\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules_curr],axis = 1)>0).astype(int)\n",
    "        incorr = np.where(self.Y!=Yhat)[0]\n",
    "        N = len(rules_curr)\n",
    "        if len(incorr)==0:\n",
    "            clean = True\n",
    "            move = ['clean']\n",
    "            # it means the BOA correctly classified all points but there could be redundant patterns, so cleaning is needed\n",
    "        else:\n",
    "            clean = False\n",
    "            ex = sample(incorr.tolist(),1)[0]\n",
    "            t = random.random()\n",
    "            if self.Y[ex]==1 or N==1:\n",
    "                if t<1.0/2 or N==1:\n",
    "                    move = ['add']       # action: add\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "            else:\n",
    "                if t<1.0/2:\n",
    "                    move = ['cut']       # action: cut\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "        if move[0]=='cut':\n",
    "            \"\"\" cut \"\"\"\n",
    "            if random.random()<q:\n",
    "                candidate = list(set(np.where(self.RMatrix[ex,:]==1)[0]).intersection(rules_curr))\n",
    "                if len(candidate)==0:\n",
    "                    candidate = rules_curr\n",
    "                cut_rule = sample(candidate,1)[0]\n",
    "            else:\n",
    "                p = []\n",
    "                all_sum = np.sum(self.RMatrix[:,rules_curr],axis = 1)\n",
    "                for index,rule in enumerate(rules_curr):\n",
    "                    Yhat= ((all_sum - np.array(self.RMatrix[:,rule]))>0).astype(int)\n",
    "                    TP,FP,TN,FN  = getConfusion(Yhat,self.Y)\n",
    "                    p.append(TP.astype(float)/(TP+FP+1))\n",
    "                    # p.append(log_betabin(TP,TP+FP,self.alpha_1,self.beta_1) + log_betabin(FN,FN+TN,self.alpha_2,self.beta_2))\n",
    "                p = [x - min(p) for x in p]\n",
    "                p = np.exp(p)\n",
    "                p = np.insert(p,0,0)\n",
    "                p = np.array(list(accumulate(p)))\n",
    "                if p[-1]==0:\n",
    "                    index = sample(range(len(rules_curr)),1)[0]\n",
    "                else:\n",
    "                    p = p/p[-1]\n",
    "                index = find_lt(p,random.random())\n",
    "                cut_rule = rules_curr[index]\n",
    "            rules_curr.remove(cut_rule)\n",
    "            rules_norm = self.normalize(rules_curr)\n",
    "            move.remove('cut')\n",
    "            \n",
    "        if len(move)>0 and move[0]=='add':\n",
    "            \"\"\" add \"\"\"\n",
    "            if random.random()<q:\n",
    "                add_rule = sample(range(nRules),1)[0]\n",
    "            else: \n",
    "                Yhat_neg_index = list(np.where(np.sum(self.RMatrix[:,rules_curr],axis = 1)<1)[0])\n",
    "                mat = np.multiply(self.RMatrix[Yhat_neg_index,:].transpose(),self.Y[Yhat_neg_index])\n",
    "                # TP = np.array(np.sum(mat,axis = 0).tolist()[0])\n",
    "                TP = np.sum(mat,axis = 1)\n",
    "                FP = np.array((np.sum(self.RMatrix[Yhat_neg_index,:],axis = 0) - TP))\n",
    "                TN = np.sum(self.Y[Yhat_neg_index]==0)-FP\n",
    "                FN = sum(self.Y[Yhat_neg_index]) - TP\n",
    "                p = (TP.astype(float)/(TP+FP+1))\n",
    "                p[rules_curr]=0\n",
    "                add_rule = sample(np.where(p==max(p))[0].tolist(),1)[0]\n",
    "            if add_rule not in rules_curr:\n",
    "                rules_curr.append(add_rule)\n",
    "                rules_norm = self.normalize(rules_curr)\n",
    "\n",
    "        if len(move)>0 and move[0]=='clean':\n",
    "            remove = []\n",
    "            for i,rule in enumerate(rules_norm):\n",
    "                Yhat = (np.sum(self.RMatrix[:,[rule for j,rule in enumerate(rules_norm) if (j!=i and j not in remove)]],axis = 1)>0).astype(int)\n",
    "                TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "                if TP+FP==0:\n",
    "                    remove.append(i)\n",
    "            for x in remove:\n",
    "                rules_norm.remove(x)\n",
    "            return rules_curr, rules_norm\n",
    "        return rules_curr, rules_norm\n",
    "\n",
    "    def compute_prob(self,rules):\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules],axis = 1)>0).astype(int)\n",
    "        TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "        Kn_count = list(np.bincount([self.rules_len[x] for x in rules], minlength = self.maxlen+1))\n",
    "        prior_ChsRules= sum([log_betabin(Kn_count[i],self.patternSpace[i],self.alpha_l[i],self.beta_l[i]) for i in range(1,len(Kn_count),1)])            \n",
    "        likelihood_1 =  log_betabin(TP,TP+FP,self.alpha_1,self.beta_1)\n",
    "        likelihood_2 = log_betabin(TN,FN+TN,self.alpha_2,self.beta_2)\n",
    "        return [TP,FP,TN,FN],[prior_ChsRules,likelihood_1,likelihood_2]\n",
    "\n",
    "    def normalize_add(self, rules_new, rule_index):\n",
    "        rules = rules_new.copy()\n",
    "        for rule in rules_new:\n",
    "            if set(self.rules[rule]).issubset(self.rules[rule_index]):\n",
    "                return rules_new.copy()\n",
    "            if set(self.rules[rule_index]).issubset(self.rules[rule]):\n",
    "                rules.remove(rule)\n",
    "        rules.append(rule_index)\n",
    "        return rules\n",
    "\n",
    "    def normalize(self, rules_new):\n",
    "        try:\n",
    "            rules_len = [len(self.rules[index]) for index in rules_new]\n",
    "            rules = [rules_new[i] for i in np.argsort(rules_len)[::-1][:len(rules_len)]]\n",
    "            p1 = 0\n",
    "            while p1<len(rules):\n",
    "                for p2 in range(p1+1,len(rules),1):\n",
    "                    if set(self.rules[rules[p2]]).issubset(set(self.rules[rules[p1]])):\n",
    "                        rules.remove(rules[p1])\n",
    "                        p1 -= 1\n",
    "                        break\n",
    "                p1 += 1\n",
    "            return rules\n",
    "        except:\n",
    "            return rules_new.copy()\n",
    "\n",
    "\n",
    "    def print_rules(self, rules_max):\n",
    "        for rule_index in rules_max:\n",
    "            print(self.rules[rule_index])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" parameters \"\"\"\n",
    "# The following parameters are recommended to change depending on the size and complexity of the data\n",
    "N = 2000      # number of rules to be used in SA_patternbased and also the output of generate_rules\n",
    "Niteration = 500  # number of iterations in each chain\n",
    "Nchain = 2         # number of chains in the simulated annealing search algorithm\n",
    "\n",
    "supp = 5           # 5% is a generally good number. The higher this supp, the 'larger' a pattern is\n",
    "maxlen = 3         # maxmum length of a pattern\n",
    "\n",
    "# \\rho = alpha/(alpha+beta). Make sure \\rho is close to one when choosing alpha and beta. \n",
    "alpha_1 = 500       # alpha_+\n",
    "beta_1 = 1          # beta_+\n",
    "alpha_2 = 500         # alpha_-\n",
    "beta_2 = 1       # beta_-\n",
    "\n",
    "\"\"\" input file \"\"\"\n",
    "# notice that in the example, X is already binary coded. \n",
    "# Data has to be binary coded and the column name shd have the form: attributename_attributevalue\n",
    "filepathX = 'tictactoe_X.txt' # input file X\n",
    "filepathY = 'tictactoe_Y.txt' # input file Y\n",
    "df = pd.read_csv(filepathX,header=0,sep=\" \")\n",
    "Y = np.loadtxt(open(filepathY,\"rb\"),delimiter=\" \")\n",
    "\n",
    "import random\n",
    "lenY = len(Y)\n",
    "train_index = random.sample(range(lenY), int(0.70 * lenY))\n",
    "test_index = [i for i in range(lenY) if i not in train_index]\n",
    "\n",
    "model = BOA(df.iloc[train_index],Y[train_index])\n",
    "model.generate_rules(supp,maxlen,N)\n",
    "model.set_parameters(alpha_1,beta_1,alpha_2,beta_2,None,None)\n",
    "rules = model.fit(Niteration,Nchain,print_message=True)\n",
    "\n",
    "# test\n",
    "Yhat = predict(rules,df.iloc[test_index])\n",
    "TP,FP,TN,FN = getConfusion(Yhat,Y[test_index])\n",
    "tpr = float(TP)/(TP+FN)\n",
    "fpr = float(FP)/(FP+TN)\n",
    "print('TP = {}, FP = {}, TN = {}, FN = {} \\n accuracy = {}, tpr = {}, fpr = {}'.format(TP,FP,TN,FN, float(TP+TN)/(TP+TN+FP+FN),tpr,fpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTook 3.613s to generate 18984 rules\n",
      "Screening rules using information gain\n",
      "\tTook 0.496s to generate 2000 rules\n",
      "Computing sizes for pattern space ...\n",
      "\tTook 0.002s to compute patternspace\n",
      "No or wrong input for alpha_l and beta_l. The model will use default parameters!\n",
      "found a more accurate model: accuracy = 0.6582089552238806\n",
      "\n",
      "** chain = 0, max at iter = 0 ** \n",
      " accuracy = 0.6582089552238806, TP = 333.0,FP = 124.0, TN = 108.0, FN = 105.0\n",
      " pt_new is -717.0230701141477, prior_ChsRules=-55.76454119002847, likelihood_1 = -366.24195365895866, likelihood_2 = -295.01657526516055\n",
      " \n",
      "['5_X_neg', '5_B_neg', '1_O_neg']\n",
      "['5_X', '4_B_neg', '7_O_neg']\n",
      "['3_O_neg', '2_X_neg', '5_X']\n",
      "['9_O_neg', '8_O', '5_X']\n",
      "['5_O_neg', '9_O_neg', '1_O_neg']\n",
      "[162, 778, 1541, 501, 1997]\n",
      "found a more accurate model: accuracy = 0.7059701492537314\n",
      "\n",
      "** chain = 0, max at iter = 1 ** \n",
      " accuracy = 0.7059701492537314, TP = 365.0,FP = 124.0, TN = 108.0, FN = 73.0\n",
      " pt_new is -666.2738164123116, prior_ChsRules=-66.49072063146559, likelihood_1 = -370.6400967281397, likelihood_2 = -229.14299905270627\n",
      " \n",
      "['5_X_neg', '5_B_neg', '1_O_neg']\n",
      "['5_X', '4_B_neg', '7_O_neg']\n",
      "['3_O_neg', '2_X_neg', '5_X']\n",
      "['9_O_neg', '8_O', '5_X']\n",
      "['5_O_neg', '9_O_neg', '1_O_neg']\n",
      "['7_O_neg', '3_O_neg', '5_O_neg']\n",
      "[162, 778, 1541, 501, 1997, 1898]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-958436519346>:206: RuntimeWarning: divide by zero encountered in log\n",
      "  cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
      "<ipython-input-4-958436519346>:206: RuntimeWarning: invalid value encountered in multiply\n",
      "  cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
      "<ipython-input-4-958436519346>:208: RuntimeWarning: divide by zero encountered in log\n",
      "  cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
      "<ipython-input-4-958436519346>:208: RuntimeWarning: invalid value encountered in multiply\n",
      "  cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
      "<ipython-input-4-958436519346>:265: RuntimeWarning: overflow encountered in exp\n",
      "  alpha = np.exp(float(pt_new -pt_curr)/T)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bisect_left' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/x/Desktop/Repositories/boa_m/BOA/example.py\u001b[0m in \u001b[0;36mline 448\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=445'>446</a>\u001b[0m model\u001b[39m.\u001b[39mgenerate_rules(supp,maxlen,N)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=446'>447</a>\u001b[0m model\u001b[39m.\u001b[39mset_parameters(alpha_1,beta_1,alpha_2,beta_2,\u001b[39mNone\u001b[39;00m,\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=447'>448</a>\u001b[0m rules \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(Niteration,Nchain,print_message\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=449'>450</a>\u001b[0m \u001b[39m# test\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=450'>451</a>\u001b[0m Yhat \u001b[39m=\u001b[39m predict(rules,df\u001b[39m.\u001b[39miloc[test_index])\n",
      "\u001b[1;32m/Users/x/Desktop/Repositories/boa_m/BOA/example.py\u001b[0m in \u001b[0;36mline 261\u001b[0m, in \u001b[0;36mBOA.fit\u001b[0;34m(self, Niteration, Nchain, q, init, keep_most_accurate_model, print_message)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=258'>259</a>\u001b[0m     rules_curr \u001b[39m=\u001b[39m maps[chain][index][\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=259'>260</a>\u001b[0m     rules_curr_norm \u001b[39m=\u001b[39m maps[chain][index][\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=260'>261</a>\u001b[0m rules_new, rules_norm \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropose(rules_curr\u001b[39m.\u001b[39;49mcopy(), rules_curr_norm\u001b[39m.\u001b[39;49mcopy(),q)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=261'>262</a>\u001b[0m cfmatrix,prob \u001b[39m=\u001b[39m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_prob(rules_new)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=262'>263</a>\u001b[0m T \u001b[39m=\u001b[39m T0\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39miter\u001b[39m\u001b[39m/\u001b[39mNiteration)\n",
      "\u001b[1;32m/Users/x/Desktop/Repositories/boa_m/BOA/example.py\u001b[0m in \u001b[0;36mline 338\u001b[0m, in \u001b[0;36mBOA.propose\u001b[0;34m(self, rules_curr, rules_norm, q)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=335'>336</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=336'>337</a>\u001b[0m         p \u001b[39m=\u001b[39m p\u001b[39m/\u001b[39mp[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=337'>338</a>\u001b[0m     index \u001b[39m=\u001b[39m find_lt(p,random\u001b[39m.\u001b[39;49mrandom())\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=338'>339</a>\u001b[0m     cut_rule \u001b[39m=\u001b[39m rules_curr[index]\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=339'>340</a>\u001b[0m rules_curr\u001b[39m.\u001b[39mremove(cut_rule)\n",
      "\u001b[1;32m/Users/x/Desktop/Repositories/boa_m/BOA/example.py\u001b[0m in \u001b[0;36mline 29\u001b[0m, in \u001b[0;36mfind_lt\u001b[0;34m(a, x)\n\u001b[1;32m     <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=26'>27</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_lt\u001b[39m(a, x):\n\u001b[1;32m     <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=27'>28</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Find rightmost value less than x\"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=28'>29</a>\u001b[0m     i \u001b[39m=\u001b[39m bisect_left(a, x)\n\u001b[1;32m     <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=29'>30</a>\u001b[0m     \u001b[39mif\u001b[39;00m i:\n\u001b[1;32m     <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=30'>31</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bisect_left' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# from fim import fpgrowth,fim --> this package is difficult to install. So the rule miner can be replaced by a random forest\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import chain, combinations\n",
    "import itertools\n",
    "from numpy.random import random\n",
    "from random import sample\n",
    "import time\n",
    "import operator\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csc_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def accumulate(iterable, func=operator.add):\n",
    "    'Return running totals'\n",
    "    # accumulate([1,2,3,4,5]) --> 1 3 6 10 15\n",
    "    # accumulate([1,2,3,4,5], operator.mul) --> 1 2 6 24 120\n",
    "    it = iter(iterable)\n",
    "    total = next(it)\n",
    "    yield total\n",
    "    for element in it:\n",
    "        total = func(total, element)\n",
    "        yield total\n",
    "\n",
    "def find_lt(a, x):\n",
    "    \"\"\" Find rightmost value less than x\"\"\"\n",
    "    i = bisect_left(a, x)\n",
    "    if i:\n",
    "        return int(i-1)\n",
    "    print('in find_lt,{}'.format(a))\n",
    "    raise ValueError\n",
    "\n",
    "\n",
    "def log_gampoiss(k,alpha,beta):\n",
    "    import math\n",
    "    k = int(k)\n",
    "    return math.lgamma(k+alpha)+alpha*np.log(beta)-math.lgamma(alpha)-math.lgamma(k+1)-(alpha+k)*np.log(1+beta)\n",
    "\n",
    "\n",
    "def log_betabin(k,n,alpha,beta):\n",
    "    import math\n",
    "    try:\n",
    "        Const =  math.lgamma(alpha + beta) - math.lgamma(alpha) - math.lgamma(beta)\n",
    "    except:\n",
    "        print('alpha = {}, beta = {}'.format(alpha,beta))\n",
    "    if isinstance(k,list) or isinstance(k,np.ndarray):\n",
    "        if len(k)!=len(n):\n",
    "            print('length of k is %d and length of n is %d'%(len(k),len(n)))\n",
    "            raise ValueError\n",
    "        lbeta = []\n",
    "        for ki,ni in zip(k,n):\n",
    "            # lbeta.append(math.lgamma(ni+1)- math.lgamma(ki+1) - math.lgamma(ni-ki+1) + math.lgamma(ki+alpha) + math.lgamma(ni-ki+beta) - math.lgamma(ni+alpha+beta) + Const)\n",
    "            lbeta.append(math.lgamma(ki+alpha) + math.lgamma(ni-ki+beta) - math.lgamma(ni+alpha+beta) + Const)\n",
    "        return np.array(lbeta)\n",
    "    else:\n",
    "        return math.lgamma(k+alpha) + math.lgamma(n-k+beta) - math.lgamma(n+alpha+beta) + Const\n",
    "        # return math.lgamma(n+1)- math.lgamma(k+1) - math.lgamma(n-k+1) + math.lgamma(k+alpha) + math.lgamma(n-k+beta) - math.lgamma(n+alpha+beta) + Const\n",
    "\n",
    "def getConfusion(Yhat,Y):\n",
    "    if len(Yhat)!=len(Y):\n",
    "        raise NameError('Yhat has different length')\n",
    "    TP = np.dot(np.array(Y),np.array(Yhat))\n",
    "    FP = np.sum(Yhat) - TP\n",
    "    TN = len(Y) - np.sum(Y)-FP\n",
    "    FN = len(Yhat) - np.sum(Yhat) - TN\n",
    "    return TP,FP,TN,FN\n",
    "\n",
    "def predict(rules,df):\n",
    "    Z = [[] for rule in rules]\n",
    "    dfn = 1-df #df has negative associations\n",
    "    dfn.columns = [name.strip() + '_neg' for name in df.columns]\n",
    "    df = pd.concat([df,dfn],axis = 1)\n",
    "    for i,rule in enumerate(rules):\n",
    "        Z[i] = (np.sum(df[list(rule)],axis=1)==len(rule)).astype(int)\n",
    "    Yhat = (np.sum(Z,axis=0)>0).astype(int)\n",
    "    return Yhat\n",
    "\n",
    "def extract_rules(tree, feature_names):\n",
    "    left      = tree.tree_.children_left\n",
    "    right     = tree.tree_.children_right\n",
    "    threshold = tree.tree_.threshold\n",
    "    features  = [feature_names[i] for i in tree.tree_.feature]\n",
    "    # get ids of child nodes\n",
    "    idx = np.argwhere(left == -1)[:,0]     \n",
    "\n",
    "    def recurse(left, right, child, lineage=None):          \n",
    "        if lineage is None:\n",
    "            lineage = []\n",
    "        if child in left:\n",
    "            parent = np.where(left == child)[0].item()\n",
    "            suffix = '_neg'\n",
    "        else:\n",
    "            parent = np.where(right == child)[0].item()\n",
    "            suffix = ''\n",
    "\n",
    "        #           lineage.append((parent, split, threshold[parent], features[parent]))\n",
    "        lineage.append((features[parent].strip()+suffix))\n",
    "\n",
    "        if parent == 0:\n",
    "            lineage.reverse()\n",
    "            return lineage\n",
    "        else:\n",
    "            return recurse(left, right, parent, lineage)   \n",
    "    rules = []\n",
    "    for child in idx:\n",
    "        rule = []\n",
    "        for node in recurse(left, right, child):\n",
    "            rule.append(node)\n",
    "        rules.append(rule)\n",
    "    return rules\n",
    "\n",
    "\n",
    "class BOA(object):\n",
    "    def __init__(self, binary_data,Y):\n",
    "        self.df = binary_data  \n",
    "        self.Y = Y\n",
    "        self.attributeLevelNum = defaultdict(int) \n",
    "        self.attributeNames = []\n",
    "        for i,name in enumerate(binary_data.columns):\n",
    "          attribute = name.split('_')[0]\n",
    "          self.attributeLevelNum[attribute] += 1\n",
    "          self.attributeNames.append(attribute)\n",
    "        self.attributeNames = list(set(self.attributeNames))\n",
    "        \n",
    "\n",
    "    def getPatternSpace(self):\n",
    "        print('Computing sizes for pattern space ...')\n",
    "        start_time = time.time()\n",
    "        \"\"\" compute the rule space from the levels in each attribute \"\"\"\n",
    "        for item in self.attributeNames:\n",
    "            self.attributeLevelNum[item+'_neg'] = self.attributeLevelNum[item]\n",
    "        self.patternSpace = np.zeros(self.maxlen+1)\n",
    "        tmp = [ item + '_neg' for item in self.attributeNames]\n",
    "        self.attributeNames.extend(tmp)\n",
    "        for k in range(1,self.maxlen+1,1):\n",
    "            for subset in combinations(self.attributeNames,k):\n",
    "                tmp = 1\n",
    "                for i in subset:\n",
    "                    tmp = tmp * self.attributeLevelNum[i]\n",
    "                self.patternSpace[k] = self.patternSpace[k] + tmp\n",
    "        print('\\tTook %0.3fs to compute patternspace' % (time.time() - start_time))               \n",
    "\n",
    "# This function generates rules that satisfy supp and maxlen using fpgrowth, then it selects the top N rules that make data have the biggest decrease in entropy\n",
    "# there are two ways to generate rules. fpgrowth can handle cases where the maxlen is small. If maxlen<=3, fpgrowth can generates rules much faster than randomforest. \n",
    "# If maxlen is big, fpgrowh tends to generate too many rules that overflow the memories. \n",
    "    def generate_rules(self,supp,maxlen,N, method = 'randomforest'):\n",
    "        self.maxlen = maxlen\n",
    "        self.supp = supp\n",
    "        df = 1-self.df #df has negative associations\n",
    "        df.columns = [name.strip() + '_neg' for name in self.df.columns]\n",
    "        df = pd.concat([self.df,df],axis = 1)\n",
    "#         if method =='fpgrowth' and maxlen<=3:\n",
    "#             itemMatrix = [[item for item in df.columns if row[item] ==1] for i,row in df.iterrows() ]  \n",
    "#             pindex = np.where(self.Y==1)[0]\n",
    "#             nindex = np.where(self.Y!=1)[0]\n",
    "#             print('Generating rules using fpgrowth')\n",
    "#             start_time = time.time()\n",
    "#             rules= fpgrowth([itemMatrix[i] for i in pindex],supp = supp,zmin = 1,zmax = maxlen)\n",
    "#             rules = [tuple(np.sort(rule[0])) for rule in rules]\n",
    "#             rules = list(set(rules))\n",
    "#             start_time = time.time()\n",
    "#             print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "#         else:\n",
    "        rules = []\n",
    "        start_time = time.time()\n",
    "        for length in range(1,maxlen+1,1):\n",
    "            n_estimators = min(pow(df.shape[1],length),4000)\n",
    "            clf = RandomForestClassifier(n_estimators = n_estimators,max_depth = length)\n",
    "            clf.fit(self.df,self.Y)\n",
    "            for n in range(n_estimators):\n",
    "                rules.extend(extract_rules(clf.estimators_[n],df.columns))\n",
    "        rules = [list(x) for x in set(tuple(x) for x in rules)]\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "        self.screen_rules(rules,df,N) # select the top N rules using secondary criteria, information gain\n",
    "        self.getPatternSpace()\n",
    "\n",
    "    def screen_rules(self,rules,df,N):\n",
    "        print('Screening rules using information gain')\n",
    "        start_time = time.time()\n",
    "        itemInd = {}\n",
    "        for i,name in enumerate(df.columns):\n",
    "            itemInd[name] = i\n",
    "        indices = np.array(list(itertools.chain.from_iterable([[itemInd[x] for x in rule] for rule in rules])))\n",
    "        len_rules = [len(rule) for rule in rules]\n",
    "        indptr =list(accumulate(len_rules))\n",
    "        indptr.insert(0,0)\n",
    "        indptr = np.array(indptr)\n",
    "        data = np.ones(len(indices))\n",
    "        ruleMatrix = csc_matrix((data,indices,indptr),shape = (len(df.columns),len(rules)))\n",
    "        mat = np.matrix(df) * ruleMatrix\n",
    "        lenMatrix = np.matrix([len_rules for i in range(df.shape[0])])\n",
    "        Z =  (mat ==lenMatrix).astype(int)\n",
    "        Zpos = [Z[i] for i in np.where(self.Y>0)][0]\n",
    "        TP = np.array(np.sum(Zpos,axis=0).tolist()[0])\n",
    "        supp_select = np.where(TP>=self.supp*sum(self.Y)/100)[0]\n",
    "        FP = np.array(np.sum(Z,axis = 0))[0] - TP\n",
    "        TN = len(self.Y) - np.sum(self.Y) - FP\n",
    "        FN = np.sum(self.Y) - TP\n",
    "        p1 = TP.astype(float)/(TP+FP)\n",
    "        p2 = FN.astype(float)/(FN+TN)\n",
    "        pp = (TP+FP).astype(float)/(TP+FP+TN+FN)\n",
    "        tpr = TP.astype(float)/(TP+FN)\n",
    "        fpr = FP.astype(float)/(FP+TN)\n",
    "        cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
    "        cond_entropy[p1*(1-p1)==0] = -((1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2)))[p1*(1-p1)==0]\n",
    "        cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
    "        cond_entropy[p1*(1-p1)*p2*(1-p2)==0] = 0\n",
    "        select = np.argsort(cond_entropy[supp_select])[::-1][-N:]\n",
    "        self.rules = [rules[i] for i in supp_select[select]]\n",
    "        self.RMatrix = np.array(Z[:,supp_select[select]])\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(self.rules)))\n",
    "\n",
    "    def set_parameters(self, a1=100,b1=10,a2=100,b2=10,al=None,bl=None):\n",
    "        # input al and bl are lists\n",
    "        # a1/(a1 + b1) and a2/(a2 + b2) should be a  bigger than 0.5. They correspond to alpha_+, beta_+ and alpha_- and beta_- from the paper. In fact, these four values play an important role and it needs to be tuned while following the a1/(a1 + b1) >0.5 and a2/(a2 + b2) >0.5 principle\n",
    "        self.alpha_1 = a1\n",
    "        self.beta_1 = b1\n",
    "        self.alpha_2 = a2\n",
    "        self.beta_2 = b2\n",
    "        if al ==None or bl==None or len(al)!=self.maxlen or len(bl)!=self.maxlen:\n",
    "            print('No or wrong input for alpha_l and beta_l. The model will use default parameters!')\n",
    "            self.C = [1.0/self.maxlen for i in range(self.maxlen)]\n",
    "            self.C.insert(0,-1)\n",
    "            self.alpha_l = [10 for i in range(self.maxlen+1)]\n",
    "            self.beta_l= [10*self.patternSpace[i]/self.C[i] for i in range(self.maxlen+1)]\n",
    "        else:\n",
    "            self.alpha_l=[1] + list(al)\n",
    "            self.beta_l = [1] + list(bl)\n",
    "\n",
    "    def fit(self, Niteration = 300, Nchain = 1, q = 0.1, init = [], keep_most_accurate_model = True,print_message=True):\n",
    "        # print('Searching for an optimal solution...')\n",
    "        start_time = time.time()\n",
    "        nRules = len(self.rules)\n",
    "        self.rules_len = [len(rule) for rule in self.rules]\n",
    "        maps = defaultdict(list)\n",
    "        T0 = 1000\n",
    "        split = 0.7*Niteration\n",
    "        acc = {}\n",
    "        most_accurate_model = defaultdict(list)\n",
    "        for chain in range(Nchain):\n",
    "            # initialize with a random pattern set\n",
    "            if init !=[]:\n",
    "                rules_curr = init.copy()\n",
    "            else:\n",
    "                N = sample(range(1,min(8,nRules),1),1)[0]\n",
    "                rules_curr = sample(range(nRules),N)\n",
    "            rules_curr_norm = self.normalize(rules_curr)\n",
    "            pt_curr = -100000000000\n",
    "            maps[chain].append([-1,[pt_curr/3,pt_curr/3,pt_curr/3],rules_curr,[self.rules[i] for i in rules_curr]])\n",
    "            acc[chain] = 0\n",
    "            for iter in range(Niteration):\n",
    "                if iter>=split:\n",
    "                    p = np.array(range(1+len(maps[chain])))\n",
    "                    p = np.array(list(accumulate(p)))\n",
    "                    p = p/p[-1]\n",
    "                    index = find_lt(p,random.random())\n",
    "                    rules_curr = maps[chain][index][2].copy()\n",
    "                    rules_curr_norm = maps[chain][index][2].copy()\n",
    "                rules_new, rules_norm = self.propose(rules_curr.copy(), rules_curr_norm.copy(),q)\n",
    "                cfmatrix,prob =  self.compute_prob(rules_new)\n",
    "                T = T0**(1 - iter/Niteration)\n",
    "                pt_new = sum(prob)\n",
    "                alpha = np.exp(float(pt_new -pt_curr)/T)\n",
    "                \n",
    "                if (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)> acc[chain]:\n",
    "                    most_accurate_model[chain] = rules_new[:]\n",
    "                    acc[chain] = (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)\n",
    "                    if print_message:\n",
    "                        print('found a more accurate model: accuracy = {}'.format(acc[chain]))\n",
    "                        \n",
    "                if pt_new > sum(maps[chain][-1][1]):\n",
    "                    maps[chain].append([iter,prob,rules_new,[self.rules[i] for i in rules_new]])\n",
    "                    if print_message:\n",
    "                        print('\\n** chain = {}, max at iter = {} ** \\n accuracy = {}, TP = {},FP = {}, TN = {}, FN = {}\\n pt_new is {}, prior_ChsRules={}, likelihood_1 = {}, likelihood_2 = {}\\n '.format(chain, iter,(cfmatrix[0]+cfmatrix[2]+0.0)/len(self.Y),cfmatrix[0],cfmatrix[1],cfmatrix[2],cfmatrix[3],sum(prob), prob[0], prob[1], prob[2]))\n",
    "                        # print '\\n** chain = {}, max at iter = {} ** \\n obj = {}, prior = {}, llh = {} '.format(chain, iter,prior+llh,prior,llh)\n",
    "                        self.print_rules(rules_new)\n",
    "                        print(rules_new)\n",
    "                if random.random() <= alpha:\n",
    "                    rules_curr_norm,rules_curr,pt_curr = rules_norm.copy(),rules_new.copy(),pt_new\n",
    "        # print('\\tTook %0.3fs to generate an optimal rule set' % (time.time() - start_time))\n",
    "        if keep_most_accurate_model:\n",
    "            acc_list = [acc[chain] for chain in range(Nchain)]\n",
    "            index = acc_list.index(max(acc_list))\n",
    "            return [self.rules[i] for i in most_accurate_model[index]] \n",
    "        else:\n",
    "            pt_max = [sum(maps[chain][-1][1]) for chain in range(Nchain)]\n",
    "            index = pt_max.index(max(pt_max))\n",
    "            return maps[index][-1][3]\n",
    "\n",
    "    def propose(self, rules_curr,rules_norm,q):\n",
    "        nRules = len(self.rules)\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules_curr],axis = 1)>0).astype(int)\n",
    "        incorr = np.where(self.Y!=Yhat)[0]\n",
    "        N = len(rules_curr)\n",
    "        if len(incorr)==0:\n",
    "            clean = True\n",
    "            move = ['clean']\n",
    "            # it means the BOA correctly classified all points but there could be redundant patterns, so cleaning is needed\n",
    "        else:\n",
    "            clean = False\n",
    "            ex = sample(incorr.tolist(),1)[0]\n",
    "            t = random.random()\n",
    "            if self.Y[ex]==1 or N==1:\n",
    "                if t<1.0/2 or N==1:\n",
    "                    move = ['add']       # action: add\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "            else:\n",
    "                if t<1.0/2:\n",
    "                    move = ['cut']       # action: cut\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "        if move[0]=='cut':\n",
    "            \"\"\" cut \"\"\"\n",
    "            if random.random()<q:\n",
    "                candidate = list(set(np.where(self.RMatrix[ex,:]==1)[0]).intersection(rules_curr))\n",
    "                if len(candidate)==0:\n",
    "                    candidate = rules_curr\n",
    "                cut_rule = sample(candidate,1)[0]\n",
    "            else:\n",
    "                p = []\n",
    "                all_sum = np.sum(self.RMatrix[:,rules_curr],axis = 1)\n",
    "                for index,rule in enumerate(rules_curr):\n",
    "                    Yhat= ((all_sum - np.array(self.RMatrix[:,rule]))>0).astype(int)\n",
    "                    TP,FP,TN,FN  = getConfusion(Yhat,self.Y)\n",
    "                    p.append(TP.astype(float)/(TP+FP+1))\n",
    "                    # p.append(log_betabin(TP,TP+FP,self.alpha_1,self.beta_1) + log_betabin(FN,FN+TN,self.alpha_2,self.beta_2))\n",
    "                p = [x - min(p) for x in p]\n",
    "                p = np.exp(p)\n",
    "                p = np.insert(p,0,0)\n",
    "                p = np.array(list(accumulate(p)))\n",
    "                if p[-1]==0:\n",
    "                    index = sample(range(len(rules_curr)),1)[0]\n",
    "                else:\n",
    "                    p = p/p[-1]\n",
    "                index = find_lt(p,random.random())\n",
    "                cut_rule = rules_curr[index]\n",
    "            rules_curr.remove(cut_rule)\n",
    "            rules_norm = self.normalize(rules_curr)\n",
    "            move.remove('cut')\n",
    "            \n",
    "        if len(move)>0 and move[0]=='add':\n",
    "            \"\"\" add \"\"\"\n",
    "            if random.random()<q:\n",
    "                add_rule = sample(range(nRules),1)[0]\n",
    "            else: \n",
    "                Yhat_neg_index = list(np.where(np.sum(self.RMatrix[:,rules_curr],axis = 1)<1)[0])\n",
    "                mat = np.multiply(self.RMatrix[Yhat_neg_index,:].transpose(),self.Y[Yhat_neg_index])\n",
    "                # TP = np.array(np.sum(mat,axis = 0).tolist()[0])\n",
    "                TP = np.sum(mat,axis = 1)\n",
    "                FP = np.array((np.sum(self.RMatrix[Yhat_neg_index,:],axis = 0) - TP))\n",
    "                TN = np.sum(self.Y[Yhat_neg_index]==0)-FP\n",
    "                FN = sum(self.Y[Yhat_neg_index]) - TP\n",
    "                p = (TP.astype(float)/(TP+FP+1))\n",
    "                p[rules_curr]=0\n",
    "                add_rule = sample(np.where(p==max(p))[0].tolist(),1)[0]\n",
    "            if add_rule not in rules_curr:\n",
    "                rules_curr.append(add_rule)\n",
    "                rules_norm = self.normalize(rules_curr)\n",
    "\n",
    "        if len(move)>0 and move[0]=='clean':\n",
    "            remove = []\n",
    "            for i,rule in enumerate(rules_norm):\n",
    "                Yhat = (np.sum(self.RMatrix[:,[rule for j,rule in enumerate(rules_norm) if (j!=i and j not in remove)]],axis = 1)>0).astype(int)\n",
    "                TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "                if TP+FP==0:\n",
    "                    remove.append(i)\n",
    "            for x in remove:\n",
    "                rules_norm.remove(x)\n",
    "            return rules_curr, rules_norm\n",
    "        return rules_curr, rules_norm\n",
    "\n",
    "    def compute_prob(self,rules):\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules],axis = 1)>0).astype(int)\n",
    "        TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "        Kn_count = list(np.bincount([self.rules_len[x] for x in rules], minlength = self.maxlen+1))\n",
    "        prior_ChsRules= sum([log_betabin(Kn_count[i],self.patternSpace[i],self.alpha_l[i],self.beta_l[i]) for i in range(1,len(Kn_count),1)])            \n",
    "        likelihood_1 =  log_betabin(TP,TP+FP,self.alpha_1,self.beta_1)\n",
    "        likelihood_2 = log_betabin(TN,FN+TN,self.alpha_2,self.beta_2)\n",
    "        return [TP,FP,TN,FN],[prior_ChsRules,likelihood_1,likelihood_2]\n",
    "\n",
    "    def normalize_add(self, rules_new, rule_index):\n",
    "        rules = rules_new.copy()\n",
    "        for rule in rules_new:\n",
    "            if set(self.rules[rule]).issubset(self.rules[rule_index]):\n",
    "                return rules_new.copy()\n",
    "            if set(self.rules[rule_index]).issubset(self.rules[rule]):\n",
    "                rules.remove(rule)\n",
    "        rules.append(rule_index)\n",
    "        return rules\n",
    "\n",
    "    def normalize(self, rules_new):\n",
    "        try:\n",
    "            rules_len = [len(self.rules[index]) for index in rules_new]\n",
    "            rules = [rules_new[i] for i in np.argsort(rules_len)[::-1][:len(rules_len)]]\n",
    "            p1 = 0\n",
    "            while p1<len(rules):\n",
    "                for p2 in range(p1+1,len(rules),1):\n",
    "                    if set(self.rules[rules[p2]]).issubset(set(self.rules[rules[p1]])):\n",
    "                        rules.remove(rules[p1])\n",
    "                        p1 -= 1\n",
    "                        break\n",
    "                p1 += 1\n",
    "            return rules\n",
    "        except:\n",
    "            return rules_new.copy()\n",
    "\n",
    "\n",
    "    def print_rules(self, rules_max):\n",
    "        for rule_index in rules_max:\n",
    "            print(self.rules[rule_index])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" parameters \"\"\"\n",
    "# The following parameters are recommended to change depending on the size and complexity of the data\n",
    "N = 2000      # number of rules to be used in SA_patternbased and also the output of generate_rules\n",
    "Niteration = 500  # number of iterations in each chain\n",
    "Nchain = 2         # number of chains in the simulated annealing search algorithm\n",
    "\n",
    "supp = 5           # 5% is a generally good number. The higher this supp, the 'larger' a pattern is\n",
    "maxlen = 3         # maxmum length of a pattern\n",
    "\n",
    "# \\rho = alpha/(alpha+beta). Make sure \\rho is close to one when choosing alpha and beta. \n",
    "alpha_1 = 500       # alpha_+\n",
    "beta_1 = 1          # beta_+\n",
    "alpha_2 = 500         # alpha_-\n",
    "beta_2 = 1       # beta_-\n",
    "\n",
    "\"\"\" input file \"\"\"\n",
    "# notice that in the example, X is already binary coded. \n",
    "# Data has to be binary coded and the column name shd have the form: attributename_attributevalue\n",
    "filepathX = 'tictactoe_X.txt' # input file X\n",
    "filepathY = 'tictactoe_Y.txt' # input file Y\n",
    "df = pd.read_csv(filepathX,header=0,sep=\" \")\n",
    "Y = np.loadtxt(open(filepathY,\"rb\"),delimiter=\" \")\n",
    "\n",
    "import random\n",
    "lenY = len(Y)\n",
    "train_index = random.sample(range(lenY), int(0.70 * lenY))\n",
    "test_index = [i for i in range(lenY) if i not in train_index]\n",
    "\n",
    "model = BOA(df.iloc[train_index],Y[train_index])\n",
    "model.generate_rules(supp,maxlen,N)\n",
    "model.set_parameters(alpha_1,beta_1,alpha_2,beta_2,None,None)\n",
    "rules = model.fit(Niteration,Nchain,print_message=True)\n",
    "\n",
    "# test\n",
    "Yhat = predict(rules,df.iloc[test_index])\n",
    "TP,FP,TN,FN = getConfusion(Yhat,Y[test_index])\n",
    "tpr = float(TP)/(TP+FN)\n",
    "fpr = float(FP)/(FP+TN)\n",
    "print('TP = {}, FP = {}, TN = {}, FN = {} \\n accuracy = {}, tpr = {}, fpr = {}'.format(TP,FP,TN,FN, float(TP+TN)/(TP+TN+FP+FN),tpr,fpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# from fim import fpgrowth,fim --> this package is difficult to install. So the rule miner can be replaced by a random forest\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import chain, combinations\n",
    "import itertools\n",
    "from numpy.random import random\n",
    "from random import sample\n",
    "import time\n",
    "import operator\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csc_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "class BOA(object):\n",
    "    def __init__(self, binary_data,Y):\n",
    "        self.df = binary_data  \n",
    "        self.Y = Y\n",
    "        self.attributeLevelNum = defaultdict(int) \n",
    "        self.attributeNames = []\n",
    "        for i,name in enumerate(binary_data.columns):\n",
    "          attribute = name.split('_')[0]\n",
    "          self.attributeLevelNum[attribute] += 1\n",
    "          self.attributeNames.append(attribute)\n",
    "        self.attributeNames = list(set(self.attributeNames))\n",
    "        \n",
    "\n",
    "    def getPatternSpace(self):\n",
    "        print('Computing sizes for pattern space ...')\n",
    "        start_time = time.time()\n",
    "        \"\"\" compute the rule space from the levels in each attribute \"\"\"\n",
    "        for item in self.attributeNames:\n",
    "            self.attributeLevelNum[item+'_neg'] = self.attributeLevelNum[item]\n",
    "        self.patternSpace = np.zeros(self.maxlen+1)\n",
    "        tmp = [ item + '_neg' for item in self.attributeNames]\n",
    "        self.attributeNames.extend(tmp)\n",
    "        for k in range(1,self.maxlen+1,1):\n",
    "            for subset in combinations(self.attributeNames,k):\n",
    "                tmp = 1\n",
    "                for i in subset:\n",
    "                    tmp = tmp * self.attributeLevelNum[i]\n",
    "                self.patternSpace[k] = self.patternSpace[k] + tmp\n",
    "        print('\\tTook %0.3fs to compute patternspace' % (time.time() - start_time))               \n",
    "\n",
    "# This function generates rules that satisfy supp and maxlen using fpgrowth, then it selects the top N rules that make data have the biggest decrease in entropy\n",
    "# there are two ways to generate rules. fpgrowth can handle cases where the maxlen is small. If maxlen<=3, fpgrowth can generates rules much faster than randomforest. \n",
    "# If maxlen is big, fpgrowh tends to generate too many rules that overflow the memories. \n",
    "    def generate_rules(self,supp,maxlen,N, method = 'randomforest'):\n",
    "        self.maxlen = maxlen\n",
    "        self.supp = supp\n",
    "        df = 1-self.df #df has negative associations\n",
    "        df.columns = [name.strip() + '_neg' for name in self.df.columns]\n",
    "        df = pd.concat([self.df,df],axis = 1)\n",
    "#         if method =='fpgrowth' and maxlen<=3:\n",
    "#             itemMatrix = [[item for item in df.columns if row[item] ==1] for i,row in df.iterrows() ]  \n",
    "#             pindex = np.where(self.Y==1)[0]\n",
    "#             nindex = np.where(self.Y!=1)[0]\n",
    "#             print('Generating rules using fpgrowth')\n",
    "#             start_time = time.time()\n",
    "#             rules= fpgrowth([itemMatrix[i] for i in pindex],supp = supp,zmin = 1,zmax = maxlen)\n",
    "#             rules = [tuple(np.sort(rule[0])) for rule in rules]\n",
    "#             rules = list(set(rules))\n",
    "#             start_time = time.time()\n",
    "#             print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "#         else:\n",
    "        rules = []\n",
    "        start_time = time.time()\n",
    "        for length in range(1,maxlen+1,1):\n",
    "            n_estimators = min(pow(df.shape[1],length),4000)\n",
    "            clf = RandomForestClassifier(n_estimators = n_estimators,max_depth = length)\n",
    "            clf.fit(self.df,self.Y)\n",
    "            for n in range(n_estimators):\n",
    "                rules.extend(extract_rules(clf.estimators_[n],df.columns))\n",
    "        rules = [list(x) for x in set(tuple(x) for x in rules)]\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "        self.screen_rules(rules,df,N) # select the top N rules using secondary criteria, information gain\n",
    "        self.getPatternSpace()\n",
    "\n",
    "    def screen_rules(self,rules,df,N):\n",
    "        print('Screening rules using information gain')\n",
    "        start_time = time.time()\n",
    "        itemInd = {}\n",
    "        for i,name in enumerate(df.columns):\n",
    "            itemInd[name] = i\n",
    "        indices = np.array(list(itertools.chain.from_iterable([[itemInd[x] for x in rule] for rule in rules])))\n",
    "        len_rules = [len(rule) for rule in rules]\n",
    "        indptr =list(accumulate(len_rules))\n",
    "        indptr.insert(0,0)\n",
    "        indptr = np.array(indptr)\n",
    "        data = np.ones(len(indices))\n",
    "        ruleMatrix = csc_matrix((data,indices,indptr),shape = (len(df.columns),len(rules)))\n",
    "        mat = np.matrix(df) * ruleMatrix\n",
    "        lenMatrix = np.matrix([len_rules for i in range(df.shape[0])])\n",
    "        Z =  (mat ==lenMatrix).astype(int)\n",
    "        Zpos = [Z[i] for i in np.where(self.Y>0)][0]\n",
    "        TP = np.array(np.sum(Zpos,axis=0).tolist()[0])\n",
    "        supp_select = np.where(TP>=self.supp*sum(self.Y)/100)[0]\n",
    "        FP = np.array(np.sum(Z,axis = 0))[0] - TP\n",
    "        TN = len(self.Y) - np.sum(self.Y) - FP\n",
    "        FN = np.sum(self.Y) - TP\n",
    "        p1 = TP.astype(float)/(TP+FP)\n",
    "        p2 = FN.astype(float)/(FN+TN)\n",
    "        pp = (TP+FP).astype(float)/(TP+FP+TN+FN)\n",
    "        tpr = TP.astype(float)/(TP+FN)\n",
    "        fpr = FP.astype(float)/(FP+TN)\n",
    "        cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
    "        cond_entropy[p1*(1-p1)==0] = -((1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2)))[p1*(1-p1)==0]\n",
    "        cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
    "        cond_entropy[p1*(1-p1)*p2*(1-p2)==0] = 0\n",
    "        select = np.argsort(cond_entropy[supp_select])[::-1][-N:]\n",
    "        self.rules = [rules[i] for i in supp_select[select]]\n",
    "        self.RMatrix = np.array(Z[:,supp_select[select]])\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(self.rules)))\n",
    "\n",
    "    def set_parameters(self, a1=100,b1=10,a2=100,b2=10,al=None,bl=None):\n",
    "        # input al and bl are lists\n",
    "        # a1/(a1 + b1) and a2/(a2 + b2) should be a  bigger than 0.5. They correspond to alpha_+, beta_+ and alpha_- and beta_- from the paper. In fact, these four values play an important role and it needs to be tuned while following the a1/(a1 + b1) >0.5 and a2/(a2 + b2) >0.5 principle\n",
    "        self.alpha_1 = a1\n",
    "        self.beta_1 = b1\n",
    "        self.alpha_2 = a2\n",
    "        self.beta_2 = b2\n",
    "        if al ==None or bl==None or len(al)!=self.maxlen or len(bl)!=self.maxlen:\n",
    "            print('No or wrong input for alpha_l and beta_l. The model will use default parameters!')\n",
    "            self.C = [1.0/self.maxlen for i in range(self.maxlen)]\n",
    "            self.C.insert(0,-1)\n",
    "            self.alpha_l = [10 for i in range(self.maxlen+1)]\n",
    "            self.beta_l= [10*self.patternSpace[i]/self.C[i] for i in range(self.maxlen+1)]\n",
    "        else:\n",
    "            self.alpha_l=[1] + list(al)\n",
    "            self.beta_l = [1] + list(bl)\n",
    "\n",
    "    def fit(self, Niteration = 300, Nchain = 1, q = 0.1, init = [], keep_most_accurate_model = True,print_message=True):\n",
    "        # print('Searching for an optimal solution...')\n",
    "        start_time = time.time()\n",
    "        nRules = len(self.rules)\n",
    "        self.rules_len = [len(rule) for rule in self.rules]\n",
    "        maps = defaultdict(list)\n",
    "        T0 = 1000\n",
    "        split = 0.7*Niteration\n",
    "        acc = {}\n",
    "        most_accurate_model = defaultdict(list)\n",
    "        for chain in range(Nchain):\n",
    "            # initialize with a random pattern set\n",
    "            if init !=[]:\n",
    "                rules_curr = init.copy()\n",
    "            else:\n",
    "                N = sample(range(1,min(8,nRules),1),1)[0]\n",
    "                rules_curr = sample(range(nRules),N)\n",
    "            rules_curr_norm = self.normalize(rules_curr)\n",
    "            pt_curr = -100000000000\n",
    "            maps[chain].append([-1,[pt_curr/3,pt_curr/3,pt_curr/3],rules_curr,[self.rules[i] for i in rules_curr]])\n",
    "            acc[chain] = 0\n",
    "            for iter in range(Niteration):\n",
    "                if iter>=split:\n",
    "                    p = np.array(range(1+len(maps[chain])))\n",
    "                    p = np.array(list(accumulate(p)))\n",
    "                    p = p/p[-1]\n",
    "                    index = find_lt(p,random())\n",
    "                    rules_curr = maps[chain][index][2].copy()\n",
    "                    rules_curr_norm = maps[chain][index][2].copy()\n",
    "                rules_new, rules_norm = self.propose(rules_curr.copy(), rules_curr_norm.copy(),q)\n",
    "                cfmatrix,prob =  self.compute_prob(rules_new)\n",
    "                T = T0**(1 - iter/Niteration)\n",
    "                pt_new = sum(prob)\n",
    "                alpha = np.exp(float(pt_new -pt_curr)/T)\n",
    "                \n",
    "                if (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)> acc[chain]:\n",
    "                    most_accurate_model[chain] = rules_new[:]\n",
    "                    acc[chain] = (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)\n",
    "                    if print_message:\n",
    "                        print('found a more accurate model: accuracy = {}'.format(acc[chain]))\n",
    "                        \n",
    "                if pt_new > sum(maps[chain][-1][1]):\n",
    "                    maps[chain].append([iter,prob,rules_new,[self.rules[i] for i in rules_new]])\n",
    "                    if print_message:\n",
    "                        print('\\n** chain = {}, max at iter = {} ** \\n accuracy = {}, TP = {},FP = {}, TN = {}, FN = {}\\n pt_new is {}, prior_ChsRules={}, likelihood_1 = {}, likelihood_2 = {}\\n '.format(chain, iter,(cfmatrix[0]+cfmatrix[2]+0.0)/len(self.Y),cfmatrix[0],cfmatrix[1],cfmatrix[2],cfmatrix[3],sum(prob), prob[0], prob[1], prob[2]))\n",
    "                        # print '\\n** chain = {}, max at iter = {} ** \\n obj = {}, prior = {}, llh = {} '.format(chain, iter,prior+llh,prior,llh)\n",
    "                        self.print_rules(rules_new)\n",
    "                        print(rules_new)\n",
    "                if random() <= alpha:\n",
    "                    rules_curr_norm,rules_curr,pt_curr = rules_norm.copy(),rules_new.copy(),pt_new\n",
    "        # print('\\tTook %0.3fs to generate an optimal rule set' % (time.time() - start_time))\n",
    "        if keep_most_accurate_model:\n",
    "            acc_list = [acc[chain] for chain in range(Nchain)]\n",
    "            index = acc_list.index(max(acc_list))\n",
    "            return [self.rules[i] for i in most_accurate_model[index]] \n",
    "        else:\n",
    "            pt_max = [sum(maps[chain][-1][1]) for chain in range(Nchain)]\n",
    "            index = pt_max.index(max(pt_max))\n",
    "            return maps[index][-1][3]\n",
    "\n",
    "    def propose(self, rules_curr,rules_norm,q):\n",
    "        nRules = len(self.rules)\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules_curr],axis = 1)>0).astype(int)\n",
    "        incorr = np.where(self.Y!=Yhat)[0]\n",
    "        N = len(rules_curr)\n",
    "        if len(incorr)==0:\n",
    "            clean = True\n",
    "            move = ['clean']\n",
    "            # it means the BOA correctly classified all points but there could be redundant patterns, so cleaning is needed\n",
    "        else:\n",
    "            clean = False\n",
    "            ex = sample(incorr.tolist(),1)[0]\n",
    "            t = random()\n",
    "            if self.Y[ex]==1 or N==1:\n",
    "                if t<1.0/2 or N==1:\n",
    "                    move = ['add']       # action: add\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "            else:\n",
    "                if t<1.0/2:\n",
    "                    move = ['cut']       # action: cut\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "        if move[0]=='cut':\n",
    "            \"\"\" cut \"\"\"\n",
    "            if random()<q:\n",
    "                candidate = list(set(np.where(self.RMatrix[ex,:]==1)[0]).intersection(rules_curr))\n",
    "                if len(candidate)==0:\n",
    "                    candidate = rules_curr\n",
    "                cut_rule = sample(candidate,1)[0]\n",
    "            else:\n",
    "                p = []\n",
    "                all_sum = np.sum(self.RMatrix[:,rules_curr],axis = 1)\n",
    "                for index,rule in enumerate(rules_curr):\n",
    "                    Yhat= ((all_sum - np.array(self.RMatrix[:,rule]))>0).astype(int)\n",
    "                    TP,FP,TN,FN  = getConfusion(Yhat,self.Y)\n",
    "                    p.append(TP.astype(float)/(TP+FP+1))\n",
    "                    # p.append(log_betabin(TP,TP+FP,self.alpha_1,self.beta_1) + log_betabin(FN,FN+TN,self.alpha_2,self.beta_2))\n",
    "                p = [x - min(p) for x in p]\n",
    "                p = np.exp(p)\n",
    "                p = np.insert(p,0,0)\n",
    "                p = np.array(list(accumulate(p)))\n",
    "                if p[-1]==0:\n",
    "                    index = sample(range(len(rules_curr)),1)[0]\n",
    "                else:\n",
    "                    p = p/p[-1]\n",
    "                index = find_lt(p,random())\n",
    "                cut_rule = rules_curr[index]\n",
    "            rules_curr.remove(cut_rule)\n",
    "            rules_norm = self.normalize(rules_curr)\n",
    "            move.remove('cut')\n",
    "            \n",
    "        if len(move)>0 and move[0]=='add':\n",
    "            \"\"\" add \"\"\"\n",
    "            if random()<q:\n",
    "                add_rule = sample(range(nRules),1)[0]\n",
    "            else: \n",
    "                Yhat_neg_index = list(np.where(np.sum(self.RMatrix[:,rules_curr],axis = 1)<1)[0])\n",
    "                mat = np.multiply(self.RMatrix[Yhat_neg_index,:].transpose(),self.Y[Yhat_neg_index])\n",
    "                # TP = np.array(np.sum(mat,axis = 0).tolist()[0])\n",
    "                TP = np.sum(mat,axis = 1)\n",
    "                FP = np.array((np.sum(self.RMatrix[Yhat_neg_index,:],axis = 0) - TP))\n",
    "                TN = np.sum(self.Y[Yhat_neg_index]==0)-FP\n",
    "                FN = sum(self.Y[Yhat_neg_index]) - TP\n",
    "                p = (TP.astype(float)/(TP+FP+1))\n",
    "                p[rules_curr]=0\n",
    "                add_rule = sample(np.where(p==max(p))[0].tolist(),1)[0]\n",
    "            if add_rule not in rules_curr:\n",
    "                rules_curr.append(add_rule)\n",
    "                rules_norm = self.normalize(rules_curr)\n",
    "\n",
    "        if len(move)>0 and move[0]=='clean':\n",
    "            remove = []\n",
    "            for i,rule in enumerate(rules_norm):\n",
    "                Yhat = (np.sum(self.RMatrix[:,[rule for j,rule in enumerate(rules_norm) if (j!=i and j not in remove)]],axis = 1)>0).astype(int)\n",
    "                TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "                if TP+FP==0:\n",
    "                    remove.append(i)\n",
    "            for x in remove:\n",
    "                rules_norm.remove(x)\n",
    "            return rules_curr, rules_norm\n",
    "        return rules_curr, rules_norm\n",
    "\n",
    "    def compute_prob(self,rules):\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules],axis = 1)>0).astype(int)\n",
    "        TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "        Kn_count = list(np.bincount([self.rules_len[x] for x in rules], minlength = self.maxlen+1))\n",
    "        prior_ChsRules= sum([log_betabin(Kn_count[i],self.patternSpace[i],self.alpha_l[i],self.beta_l[i]) for i in range(1,len(Kn_count),1)])            \n",
    "        likelihood_1 =  log_betabin(TP,TP+FP,self.alpha_1,self.beta_1)\n",
    "        likelihood_2 = log_betabin(TN,FN+TN,self.alpha_2,self.beta_2)\n",
    "        return [TP,FP,TN,FN],[prior_ChsRules,likelihood_1,likelihood_2]\n",
    "\n",
    "    def normalize_add(self, rules_new, rule_index):\n",
    "        rules = rules_new.copy()\n",
    "        for rule in rules_new:\n",
    "            if set(self.rules[rule]).issubset(self.rules[rule_index]):\n",
    "                return rules_new.copy()\n",
    "            if set(self.rules[rule_index]).issubset(self.rules[rule]):\n",
    "                rules.remove(rule)\n",
    "        rules.append(rule_index)\n",
    "        return rules\n",
    "\n",
    "    def normalize(self, rules_new):\n",
    "        try:\n",
    "            rules_len = [len(self.rules[index]) for index in rules_new]\n",
    "            rules = [rules_new[i] for i in np.argsort(rules_len)[::-1][:len(rules_len)]]\n",
    "            p1 = 0\n",
    "            while p1<len(rules):\n",
    "                for p2 in range(p1+1,len(rules),1):\n",
    "                    if set(self.rules[rules[p2]]).issubset(set(self.rules[rules[p1]])):\n",
    "                        rules.remove(rules[p1])\n",
    "                        p1 -= 1\n",
    "                        break\n",
    "                p1 += 1\n",
    "            return rules\n",
    "        except:\n",
    "            return rules_new.copy()\n",
    "\n",
    "\n",
    "    def print_rules(self, rules_max):\n",
    "        for rule_index in rules_max:\n",
    "            print(self.rules[rule_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# from fim import fpgrowth,fim --> this package is difficult to install. So the rule miner can be replaced by a random forest\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import chain, combinations\n",
    "import itertools\n",
    "from numpy.random import random\n",
    "from random import sample\n",
    "import time\n",
    "import operator\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csc_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "class BOA(object):\n",
    "    def __init__(self, binary_data,Y):\n",
    "        self.df = binary_data  \n",
    "        self.Y = Y\n",
    "        self.attributeLevelNum = defaultdict(int) \n",
    "        self.attributeNames = []\n",
    "        for i,name in enumerate(binary_data.columns):\n",
    "          attribute = name.split('_')[0]\n",
    "          self.attributeLevelNum[attribute] += 1\n",
    "          self.attributeNames.append(attribute)\n",
    "        self.attributeNames = list(set(self.attributeNames))\n",
    "        \n",
    "\n",
    "    def getPatternSpace(self):\n",
    "        print('Computing sizes for pattern space ...')\n",
    "        start_time = time.time()\n",
    "        \"\"\" compute the rule space from the levels in each attribute \"\"\"\n",
    "        for item in self.attributeNames:\n",
    "            self.attributeLevelNum[item+'_neg'] = self.attributeLevelNum[item]\n",
    "        self.patternSpace = np.zeros(self.maxlen+1)\n",
    "        tmp = [ item + '_neg' for item in self.attributeNames]\n",
    "        self.attributeNames.extend(tmp)\n",
    "        for k in range(1,self.maxlen+1,1):\n",
    "            for subset in combinations(self.attributeNames,k):\n",
    "                tmp = 1\n",
    "                for i in subset:\n",
    "                    tmp = tmp * self.attributeLevelNum[i]\n",
    "                self.patternSpace[k] = self.patternSpace[k] + tmp\n",
    "        print('\\tTook %0.3fs to compute patternspace' % (time.time() - start_time))               \n",
    "\n",
    "# This function generates rules that satisfy supp and maxlen using fpgrowth, then it selects the top N rules that make data have the biggest decrease in entropy\n",
    "# there are two ways to generate rules. fpgrowth can handle cases where the maxlen is small. If maxlen<=3, fpgrowth can generates rules much faster than randomforest. \n",
    "# If maxlen is big, fpgrowh tends to generate too many rules that overflow the memories. \n",
    "    def generate_rules(self,supp,maxlen,N, method = 'randomforest'):\n",
    "        self.maxlen = maxlen\n",
    "        self.supp = supp\n",
    "        df = 1-self.df #df has negative associations\n",
    "        df.columns = [name.strip() + '_neg' for name in self.df.columns]\n",
    "        df = pd.concat([self.df,df],axis = 1)\n",
    "#         if method =='fpgrowth' and maxlen<=3:\n",
    "#             itemMatrix = [[item for item in df.columns if row[item] ==1] for i,row in df.iterrows() ]  \n",
    "#             pindex = np.where(self.Y==1)[0]\n",
    "#             nindex = np.where(self.Y!=1)[0]\n",
    "#             print('Generating rules using fpgrowth')\n",
    "#             start_time = time.time()\n",
    "#             rules= fpgrowth([itemMatrix[i] for i in pindex],supp = supp,zmin = 1,zmax = maxlen)\n",
    "#             rules = [tuple(np.sort(rule[0])) for rule in rules]\n",
    "#             rules = list(set(rules))\n",
    "#             start_time = time.time()\n",
    "#             print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "#         else:\n",
    "        rules = []\n",
    "        start_time = time.time()\n",
    "        for length in range(1,maxlen+1,1):\n",
    "            n_estimators = min(pow(df.shape[1],length),4000)\n",
    "            clf = RandomForestClassifier(n_estimators = n_estimators,max_depth = length)\n",
    "            clf.fit(self.df,self.Y)\n",
    "            for n in range(n_estimators):\n",
    "                rules.extend(extract_rules(clf.estimators_[n],df.columns))\n",
    "        rules = [list(x) for x in set(tuple(x) for x in rules)]\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "        self.screen_rules(rules,df,N) # select the top N rules using secondary criteria, information gain\n",
    "        self.getPatternSpace()\n",
    "\n",
    "    def screen_rules(self,rules,df,N):\n",
    "        print('Screening rules using information gain')\n",
    "        start_time = time.time()\n",
    "        itemInd = {}\n",
    "        for i,name in enumerate(df.columns):\n",
    "            itemInd[name] = i\n",
    "        indices = np.array(list(itertools.chain.from_iterable([[itemInd[x] for x in rule] for rule in rules])))\n",
    "        len_rules = [len(rule) for rule in rules]\n",
    "        indptr =list(accumulate(len_rules))\n",
    "        indptr.insert(0,0)\n",
    "        indptr = np.array(indptr)\n",
    "        data = np.ones(len(indices))\n",
    "        ruleMatrix = csc_matrix((data,indices,indptr),shape = (len(df.columns),len(rules)))\n",
    "        mat = np.matrix(df) * ruleMatrix\n",
    "        lenMatrix = np.matrix([len_rules for i in range(df.shape[0])])\n",
    "        Z =  (mat ==lenMatrix).astype(int)\n",
    "        Zpos = [Z[i] for i in np.where(self.Y>0)][0]\n",
    "        TP = np.array(np.sum(Zpos,axis=0).tolist()[0])\n",
    "        supp_select = np.where(TP>=self.supp*sum(self.Y)/100)[0]\n",
    "        FP = np.array(np.sum(Z,axis = 0))[0] - TP\n",
    "        TN = len(self.Y) - np.sum(self.Y) - FP\n",
    "        FN = np.sum(self.Y) - TP\n",
    "        p1 = TP.astype(float)/(TP+FP)\n",
    "        p2 = FN.astype(float)/(FN+TN)\n",
    "        pp = (TP+FP).astype(float)/(TP+FP+TN+FN)\n",
    "        tpr = TP.astype(float)/(TP+FN)\n",
    "        fpr = FP.astype(float)/(FP+TN)\n",
    "        cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
    "        cond_entropy[p1*(1-p1)==0] = -((1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2)))[p1*(1-p1)==0]\n",
    "        cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
    "        cond_entropy[p1*(1-p1)*p2*(1-p2)==0] = 0\n",
    "        select = np.argsort(cond_entropy[supp_select])[::-1][-N:]\n",
    "        self.rules = [rules[i] for i in supp_select[select]]\n",
    "        self.RMatrix = np.array(Z[:,supp_select[select]])\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(self.rules)))\n",
    "\n",
    "    def set_parameters(self, a1=100,b1=10,a2=100,b2=10,al=None,bl=None):\n",
    "        # input al and bl are lists\n",
    "        # a1/(a1 + b1) and a2/(a2 + b2) should be a  bigger than 0.5. They correspond to alpha_+, beta_+ and alpha_- and beta_- from the paper. In fact, these four values play an important role and it needs to be tuned while following the a1/(a1 + b1) >0.5 and a2/(a2 + b2) >0.5 principle\n",
    "        self.alpha_1 = a1\n",
    "        self.beta_1 = b1\n",
    "        self.alpha_2 = a2\n",
    "        self.beta_2 = b2\n",
    "        if al ==None or bl==None or len(al)!=self.maxlen or len(bl)!=self.maxlen:\n",
    "            print('No or wrong input for alpha_l and beta_l. The model will use default parameters!')\n",
    "            self.C = [1.0/self.maxlen for i in range(self.maxlen)]\n",
    "            self.C.insert(0,-1)\n",
    "            self.alpha_l = [10 for i in range(self.maxlen+1)]\n",
    "            self.beta_l= [10*self.patternSpace[i]/self.C[i] for i in range(self.maxlen+1)]\n",
    "        else:\n",
    "            self.alpha_l=[1] + list(al)\n",
    "            self.beta_l = [1] + list(bl)\n",
    "\n",
    "    def fit(self, Niteration = 300, Nchain = 1, q = 0.1, init = [], keep_most_accurate_model = True,print_message=True):\n",
    "        # print('Searching for an optimal solution...')\n",
    "        start_time = time.time()\n",
    "        nRules = len(self.rules)\n",
    "        self.rules_len = [len(rule) for rule in self.rules]\n",
    "        maps = defaultdict(list)\n",
    "        T0 = 1000\n",
    "        split = 0.7*Niteration\n",
    "        acc = {}\n",
    "        most_accurate_model = defaultdict(list)\n",
    "        for chain in range(Nchain):\n",
    "            # initialize with a random pattern set\n",
    "            if init !=[]:\n",
    "                rules_curr = init.copy()\n",
    "            else:\n",
    "                N = sample(range(1,min(8,nRules),1),1)[0]\n",
    "                rules_curr = sample(range(nRules),N)\n",
    "            rules_curr_norm = self.normalize(rules_curr)\n",
    "            pt_curr = -100000000000\n",
    "            maps[chain].append([-1,[pt_curr/3,pt_curr/3,pt_curr/3],rules_curr,[self.rules[i] for i in rules_curr]])\n",
    "            acc[chain] = 0\n",
    "            for iter in range(Niteration):\n",
    "                if iter>=split:\n",
    "                    p = np.array(range(1+len(maps[chain])))\n",
    "                    p = np.array(list(accumulate(p)))\n",
    "                    p = p/p[-1]\n",
    "                    index = find_lt(p,random())\n",
    "                    rules_curr = maps[chain][index][2].copy()\n",
    "                    rules_curr_norm = maps[chain][index][2].copy()\n",
    "                rules_new, rules_norm = self.propose(rules_curr.copy(), rules_curr_norm.copy(),q)\n",
    "                cfmatrix,prob =  self.compute_prob(rules_new)\n",
    "                T = T0**(1 - iter/Niteration)\n",
    "                pt_new = sum(prob)\n",
    "                alpha = np.exp(float(pt_new -pt_curr)/T)\n",
    "                \n",
    "                if (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)> acc[chain]:\n",
    "                    most_accurate_model[chain] = rules_new[:]\n",
    "                    acc[chain] = (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)\n",
    "                    if print_message:\n",
    "                        print('found a more accurate model: accuracy = {}'.format(acc[chain]))\n",
    "                        \n",
    "                if pt_new > sum(maps[chain][-1][1]):\n",
    "                    maps[chain].append([iter,prob,rules_new,[self.rules[i] for i in rules_new]])\n",
    "                    if print_message:\n",
    "                        print('\\n** chain = {}, max at iter = {} ** \\n accuracy = {}, TP = {},FP = {}, TN = {}, FN = {}\\n pt_new is {}, prior_ChsRules={}, likelihood_1 = {}, likelihood_2 = {}\\n '.format(chain, iter,(cfmatrix[0]+cfmatrix[2]+0.0)/len(self.Y),cfmatrix[0],cfmatrix[1],cfmatrix[2],cfmatrix[3],sum(prob), prob[0], prob[1], prob[2]))\n",
    "                        # print '\\n** chain = {}, max at iter = {} ** \\n obj = {}, prior = {}, llh = {} '.format(chain, iter,prior+llh,prior,llh)\n",
    "                        self.print_rules(rules_new)\n",
    "                        print(rules_new)\n",
    "                if random() <= alpha:\n",
    "                    rules_curr_norm,rules_curr,pt_curr = rules_norm.copy(),rules_new.copy(),pt_new\n",
    "        # print('\\tTook %0.3fs to generate an optimal rule set' % (time.time() - start_time))\n",
    "        if keep_most_accurate_model:\n",
    "            acc_list = [acc[chain] for chain in range(Nchain)]\n",
    "            index = acc_list.index(max(acc_list))\n",
    "            return [self.rules[i] for i in most_accurate_model[index]] \n",
    "        else:\n",
    "            pt_max = [sum(maps[chain][-1][1]) for chain in range(Nchain)]\n",
    "            index = pt_max.index(max(pt_max))\n",
    "            return maps[index][-1][3]\n",
    "\n",
    "    def propose(self, rules_curr,rules_norm,q):\n",
    "        nRules = len(self.rules)\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules_curr],axis = 1)>0).astype(int)\n",
    "        incorr = np.where(self.Y!=Yhat)[0]\n",
    "        N = len(rules_curr)\n",
    "        if len(incorr)==0:\n",
    "            clean = True\n",
    "            move = ['clean']\n",
    "            # it means the BOA correctly classified all points but there could be redundant patterns, so cleaning is needed\n",
    "        else:\n",
    "            clean = False\n",
    "            ex = sample(incorr.tolist(),1)[0]\n",
    "            t = random()\n",
    "            if self.Y[ex]==1 or N==1:\n",
    "                if t<1.0/2 or N==1:\n",
    "                    move = ['add']       # action: add\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "            else:\n",
    "                if t<1.0/2:\n",
    "                    move = ['cut']       # action: cut\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "        if move[0]=='cut':\n",
    "            \"\"\" cut \"\"\"\n",
    "            if random()<q:\n",
    "                candidate = list(set(np.where(self.RMatrix[ex,:]==1)[0]).intersection(rules_curr))\n",
    "                if len(candidate)==0:\n",
    "                    candidate = rules_curr\n",
    "                cut_rule = sample(candidate,1)[0]\n",
    "            else:\n",
    "                p = []\n",
    "                all_sum = np.sum(self.RMatrix[:,rules_curr],axis = 1)\n",
    "                for index,rule in enumerate(rules_curr):\n",
    "                    Yhat= ((all_sum - np.array(self.RMatrix[:,rule]))>0).astype(int)\n",
    "                    TP,FP,TN,FN  = getConfusion(Yhat,self.Y)\n",
    "                    p.append(TP.astype(float)/(TP+FP+1))\n",
    "                    # p.append(log_betabin(TP,TP+FP,self.alpha_1,self.beta_1) + log_betabin(FN,FN+TN,self.alpha_2,self.beta_2))\n",
    "                p = [x - min(p) for x in p]\n",
    "                p = np.exp(p)\n",
    "                p = np.insert(p,0,0)\n",
    "                p = np.array(list(accumulate(p)))\n",
    "                if p[-1]==0:\n",
    "                    index = sample(range(len(rules_curr)),1)[0]\n",
    "                else:\n",
    "                    p = p/p[-1]\n",
    "                index = find_lt(p,random())\n",
    "                cut_rule = rules_curr[index]\n",
    "            rules_curr.remove(cut_rule)\n",
    "            rules_norm = self.normalize(rules_curr)\n",
    "            move.remove('cut')\n",
    "            \n",
    "        if len(move)>0 and move[0]=='add':\n",
    "            \"\"\" add \"\"\"\n",
    "            if random()<q:\n",
    "                add_rule = sample(range(nRules),1)[0]\n",
    "            else: \n",
    "                Yhat_neg_index = list(np.where(np.sum(self.RMatrix[:,rules_curr],axis = 1)<1)[0])\n",
    "                mat = np.multiply(self.RMatrix[Yhat_neg_index,:].transpose(),self.Y[Yhat_neg_index])\n",
    "                # TP = np.array(np.sum(mat,axis = 0).tolist()[0])\n",
    "                TP = np.sum(mat,axis = 1)\n",
    "                FP = np.array((np.sum(self.RMatrix[Yhat_neg_index,:],axis = 0) - TP))\n",
    "                TN = np.sum(self.Y[Yhat_neg_index]==0)-FP\n",
    "                FN = sum(self.Y[Yhat_neg_index]) - TP\n",
    "                p = (TP.astype(float)/(TP+FP+1))\n",
    "                p[rules_curr]=0\n",
    "                add_rule = sample(np.where(p==max(p))[0].tolist(),1)[0]\n",
    "            if add_rule not in rules_curr:\n",
    "                rules_curr.append(add_rule)\n",
    "                rules_norm = self.normalize(rules_curr)\n",
    "\n",
    "        if len(move)>0 and move[0]=='clean':\n",
    "            remove = []\n",
    "            for i,rule in enumerate(rules_norm):\n",
    "                Yhat = (np.sum(self.RMatrix[:,[rule for j,rule in enumerate(rules_norm) if (j!=i and j not in remove)]],axis = 1)>0).astype(int)\n",
    "                TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "                if TP+FP==0:\n",
    "                    remove.append(i)\n",
    "            for x in remove:\n",
    "                rules_norm.remove(x)\n",
    "            return rules_curr, rules_norm\n",
    "        return rules_curr, rules_norm\n",
    "\n",
    "    def compute_prob(self,rules):\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules],axis = 1)>0).astype(int)\n",
    "        TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "        Kn_count = list(np.bincount([self.rules_len[x] for x in rules], minlength = self.maxlen+1))\n",
    "        prior_ChsRules= sum([log_betabin(Kn_count[i],self.patternSpace[i],self.alpha_l[i],self.beta_l[i]) for i in range(1,len(Kn_count),1)])            \n",
    "        likelihood_1 =  log_betabin(TP,TP+FP,self.alpha_1,self.beta_1)\n",
    "        likelihood_2 = log_betabin(TN,FN+TN,self.alpha_2,self.beta_2)\n",
    "        return [TP,FP,TN,FN],[prior_ChsRules,likelihood_1,likelihood_2]\n",
    "\n",
    "    def normalize_add(self, rules_new, rule_index):\n",
    "        rules = rules_new.copy()\n",
    "        for rule in rules_new:\n",
    "            if set(self.rules[rule]).issubset(self.rules[rule_index]):\n",
    "                return rules_new.copy()\n",
    "            if set(self.rules[rule_index]).issubset(self.rules[rule]):\n",
    "                rules.remove(rule)\n",
    "        rules.append(rule_index)\n",
    "        return rules\n",
    "\n",
    "    def normalize(self, rules_new):\n",
    "        try:\n",
    "            rules_len = [len(self.rules[index]) for index in rules_new]\n",
    "            rules = [rules_new[i] for i in np.argsort(rules_len)[::-1][:len(rules_len)]]\n",
    "            p1 = 0\n",
    "            while p1<len(rules):\n",
    "                for p2 in range(p1+1,len(rules),1):\n",
    "                    if set(self.rules[rules[p2]]).issubset(set(self.rules[rules[p1]])):\n",
    "                        rules.remove(rules[p1])\n",
    "                        p1 -= 1\n",
    "                        break\n",
    "                p1 += 1\n",
    "            return rules\n",
    "        except:\n",
    "            return rules_new.copy()\n",
    "\n",
    "\n",
    "    def print_rules(self, rules_max):\n",
    "        for rule_index in rules_max:\n",
    "            print(self.rules[rule_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTook 3.659s to generate 18608 rules\n",
      "Screening rules using information gain\n",
      "\tTook 0.508s to generate 2000 rules\n",
      "Computing sizes for pattern space ...\n",
      "\tTook 0.000s to compute patternspace\n",
      "No or wrong input for alpha_l and beta_l. The model will use default parameters!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-7b66c4721dab>:107: RuntimeWarning: divide by zero encountered in log\n",
      "  cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
      "<ipython-input-7-7b66c4721dab>:107: RuntimeWarning: invalid value encountered in multiply\n",
      "  cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
      "<ipython-input-7-7b66c4721dab>:109: RuntimeWarning: divide by zero encountered in log\n",
      "  cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
      "<ipython-input-7-7b66c4721dab>:109: RuntimeWarning: invalid value encountered in multiply\n",
      "  cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable. Did you mean: 'random.random(...)'?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/x/Desktop/Repositories/BOA_model.py\u001b[0m in \u001b[0;36mline 348\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=345'>346</a>\u001b[0m model\u001b[39m.\u001b[39mgenerate_rules(supp,maxlen,N)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=346'>347</a>\u001b[0m model\u001b[39m.\u001b[39mset_parameters(alpha_1,beta_1,alpha_2,beta_2,\u001b[39mNone\u001b[39;00m,\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=347'>348</a>\u001b[0m rules \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(Niteration,Nchain,print_message\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=349'>350</a>\u001b[0m \u001b[39m# test\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=350'>351</a>\u001b[0m Yhat \u001b[39m=\u001b[39m predict(rules,df\u001b[39m.\u001b[39miloc[test_index])\n",
      "\u001b[1;32m/Users/x/Desktop/Repositories/BOA_model.py\u001b[0m in \u001b[0;36mline 162\u001b[0m, in \u001b[0;36mBOA.fit\u001b[0;34m(self, Niteration, Nchain, q, init, keep_most_accurate_model, print_message)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=159'>160</a>\u001b[0m     rules_curr \u001b[39m=\u001b[39m maps[chain][index][\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=160'>161</a>\u001b[0m     rules_curr_norm \u001b[39m=\u001b[39m maps[chain][index][\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=161'>162</a>\u001b[0m rules_new, rules_norm \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropose(rules_curr\u001b[39m.\u001b[39;49mcopy(), rules_curr_norm\u001b[39m.\u001b[39;49mcopy(),q)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=162'>163</a>\u001b[0m cfmatrix,prob \u001b[39m=\u001b[39m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_prob(rules_new)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=163'>164</a>\u001b[0m T \u001b[39m=\u001b[39m T0\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39miter\u001b[39m\u001b[39m/\u001b[39mNiteration)\n",
      "\u001b[1;32m/Users/x/Desktop/Repositories/BOA_model.py\u001b[0m in \u001b[0;36mline 205\u001b[0m, in \u001b[0;36mBOA.propose\u001b[0;34m(self, rules_curr, rules_norm, q)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=202'>203</a>\u001b[0m clean \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=203'>204</a>\u001b[0m ex \u001b[39m=\u001b[39m sample(incorr\u001b[39m.\u001b[39mtolist(),\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=204'>205</a>\u001b[0m t \u001b[39m=\u001b[39m random()\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=205'>206</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mY[ex]\u001b[39m==\u001b[39m\u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m N\u001b[39m==\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=206'>207</a>\u001b[0m     \u001b[39mif\u001b[39;00m t\u001b[39m<\u001b[39m\u001b[39m1.0\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m N\u001b[39m==\u001b[39m\u001b[39m1\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable. Did you mean: 'random.random(...)'?"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# from fim import fpgrowth,fim --> this package is difficult to install. So the rule miner can be replaced by a random forest\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import chain, combinations\n",
    "import itertools\n",
    "from numpy.random import random\n",
    "from random import sample\n",
    "import time\n",
    "import operator\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csc_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "class BOA(object):\n",
    "    def __init__(self, binary_data,Y):\n",
    "        self.df = binary_data  \n",
    "        self.Y = Y\n",
    "        self.attributeLevelNum = defaultdict(int) \n",
    "        self.attributeNames = []\n",
    "        for i,name in enumerate(binary_data.columns):\n",
    "          attribute = name.split('_')[0]\n",
    "          self.attributeLevelNum[attribute] += 1\n",
    "          self.attributeNames.append(attribute)\n",
    "        self.attributeNames = list(set(self.attributeNames))\n",
    "        \n",
    "\n",
    "    def getPatternSpace(self):\n",
    "        print('Computing sizes for pattern space ...')\n",
    "        start_time = time.time()\n",
    "        \"\"\" compute the rule space from the levels in each attribute \"\"\"\n",
    "        for item in self.attributeNames:\n",
    "            self.attributeLevelNum[item+'_neg'] = self.attributeLevelNum[item]\n",
    "        self.patternSpace = np.zeros(self.maxlen+1)\n",
    "        tmp = [ item + '_neg' for item in self.attributeNames]\n",
    "        self.attributeNames.extend(tmp)\n",
    "        for k in range(1,self.maxlen+1,1):\n",
    "            for subset in combinations(self.attributeNames,k):\n",
    "                tmp = 1\n",
    "                for i in subset:\n",
    "                    tmp = tmp * self.attributeLevelNum[i]\n",
    "                self.patternSpace[k] = self.patternSpace[k] + tmp\n",
    "        print('\\tTook %0.3fs to compute patternspace' % (time.time() - start_time))               \n",
    "\n",
    "# This function generates rules that satisfy supp and maxlen using fpgrowth, then it selects the top N rules that make data have the biggest decrease in entropy\n",
    "# there are two ways to generate rules. fpgrowth can handle cases where the maxlen is small. If maxlen<=3, fpgrowth can generates rules much faster than randomforest. \n",
    "# If maxlen is big, fpgrowh tends to generate too many rules that overflow the memories. \n",
    "    def generate_rules(self,supp,maxlen,N, method = 'randomforest'):\n",
    "        self.maxlen = maxlen\n",
    "        self.supp = supp\n",
    "        df = 1-self.df #df has negative associations\n",
    "        df.columns = [name.strip() + '_neg' for name in self.df.columns]\n",
    "        df = pd.concat([self.df,df],axis = 1)\n",
    "#         if method =='fpgrowth' and maxlen<=3:\n",
    "#             itemMatrix = [[item for item in df.columns if row[item] ==1] for i,row in df.iterrows() ]  \n",
    "#             pindex = np.where(self.Y==1)[0]\n",
    "#             nindex = np.where(self.Y!=1)[0]\n",
    "#             print('Generating rules using fpgrowth')\n",
    "#             start_time = time.time()\n",
    "#             rules= fpgrowth([itemMatrix[i] for i in pindex],supp = supp,zmin = 1,zmax = maxlen)\n",
    "#             rules = [tuple(np.sort(rule[0])) for rule in rules]\n",
    "#             rules = list(set(rules))\n",
    "#             start_time = time.time()\n",
    "#             print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "#         else:\n",
    "        rules = []\n",
    "        start_time = time.time()\n",
    "        for length in range(1,maxlen+1,1):\n",
    "            n_estimators = min(pow(df.shape[1],length),4000)\n",
    "            clf = RandomForestClassifier(n_estimators = n_estimators,max_depth = length)\n",
    "            clf.fit(self.df,self.Y)\n",
    "            for n in range(n_estimators):\n",
    "                rules.extend(extract_rules(clf.estimators_[n],df.columns))\n",
    "        rules = [list(x) for x in set(tuple(x) for x in rules)]\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "        self.screen_rules(rules,df,N) # select the top N rules using secondary criteria, information gain\n",
    "        self.getPatternSpace()\n",
    "\n",
    "    def screen_rules(self,rules,df,N):\n",
    "        print('Screening rules using information gain')\n",
    "        start_time = time.time()\n",
    "        itemInd = {}\n",
    "        for i,name in enumerate(df.columns):\n",
    "            itemInd[name] = i\n",
    "        indices = np.array(list(itertools.chain.from_iterable([[itemInd[x] for x in rule] for rule in rules])))\n",
    "        len_rules = [len(rule) for rule in rules]\n",
    "        indptr =list(accumulate(len_rules))\n",
    "        indptr.insert(0,0)\n",
    "        indptr = np.array(indptr)\n",
    "        data = np.ones(len(indices))\n",
    "        ruleMatrix = csc_matrix((data,indices,indptr),shape = (len(df.columns),len(rules)))\n",
    "        mat = np.matrix(df) * ruleMatrix\n",
    "        lenMatrix = np.matrix([len_rules for i in range(df.shape[0])])\n",
    "        Z =  (mat ==lenMatrix).astype(int)\n",
    "        Zpos = [Z[i] for i in np.where(self.Y>0)][0]\n",
    "        TP = np.array(np.sum(Zpos,axis=0).tolist()[0])\n",
    "        supp_select = np.where(TP>=self.supp*sum(self.Y)/100)[0]\n",
    "        FP = np.array(np.sum(Z,axis = 0))[0] - TP\n",
    "        TN = len(self.Y) - np.sum(self.Y) - FP\n",
    "        FN = np.sum(self.Y) - TP\n",
    "        p1 = TP.astype(float)/(TP+FP)\n",
    "        p2 = FN.astype(float)/(FN+TN)\n",
    "        pp = (TP+FP).astype(float)/(TP+FP+TN+FN)\n",
    "        tpr = TP.astype(float)/(TP+FN)\n",
    "        fpr = FP.astype(float)/(FP+TN)\n",
    "        cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
    "        cond_entropy[p1*(1-p1)==0] = -((1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2)))[p1*(1-p1)==0]\n",
    "        cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
    "        cond_entropy[p1*(1-p1)*p2*(1-p2)==0] = 0\n",
    "        select = np.argsort(cond_entropy[supp_select])[::-1][-N:]\n",
    "        self.rules = [rules[i] for i in supp_select[select]]\n",
    "        self.RMatrix = np.array(Z[:,supp_select[select]])\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(self.rules)))\n",
    "\n",
    "    def set_parameters(self, a1=100,b1=10,a2=100,b2=10,al=None,bl=None):\n",
    "        # input al and bl are lists\n",
    "        # a1/(a1 + b1) and a2/(a2 + b2) should be a  bigger than 0.5. They correspond to alpha_+, beta_+ and alpha_- and beta_- from the paper. In fact, these four values play an important role and it needs to be tuned while following the a1/(a1 + b1) >0.5 and a2/(a2 + b2) >0.5 principle\n",
    "        self.alpha_1 = a1\n",
    "        self.beta_1 = b1\n",
    "        self.alpha_2 = a2\n",
    "        self.beta_2 = b2\n",
    "        if al ==None or bl==None or len(al)!=self.maxlen or len(bl)!=self.maxlen:\n",
    "            print('No or wrong input for alpha_l and beta_l. The model will use default parameters!')\n",
    "            self.C = [1.0/self.maxlen for i in range(self.maxlen)]\n",
    "            self.C.insert(0,-1)\n",
    "            self.alpha_l = [10 for i in range(self.maxlen+1)]\n",
    "            self.beta_l= [10*self.patternSpace[i]/self.C[i] for i in range(self.maxlen+1)]\n",
    "        else:\n",
    "            self.alpha_l=[1] + list(al)\n",
    "            self.beta_l = [1] + list(bl)\n",
    "\n",
    "    def fit(self, Niteration = 300, Nchain = 1, q = 0.1, init = [], keep_most_accurate_model = True,print_message=True):\n",
    "        # print('Searching for an optimal solution...')\n",
    "        start_time = time.time()\n",
    "        nRules = len(self.rules)\n",
    "        self.rules_len = [len(rule) for rule in self.rules]\n",
    "        maps = defaultdict(list)\n",
    "        T0 = 1000\n",
    "        split = 0.7*Niteration\n",
    "        acc = {}\n",
    "        most_accurate_model = defaultdict(list)\n",
    "        for chain in range(Nchain):\n",
    "            # initialize with a random pattern set\n",
    "            if init !=[]:\n",
    "                rules_curr = init.copy()\n",
    "            else:\n",
    "                N = sample(range(1,min(8,nRules),1),1)[0]\n",
    "                rules_curr = sample(range(nRules),N)\n",
    "            rules_curr_norm = self.normalize(rules_curr)\n",
    "            pt_curr = -100000000000\n",
    "            maps[chain].append([-1,[pt_curr/3,pt_curr/3,pt_curr/3],rules_curr,[self.rules[i] for i in rules_curr]])\n",
    "            acc[chain] = 0\n",
    "            for iter in range(Niteration):\n",
    "                if iter>=split:\n",
    "                    p = np.array(range(1+len(maps[chain])))\n",
    "                    p = np.array(list(accumulate(p)))\n",
    "                    p = p/p[-1]\n",
    "                    index = find_lt(p,random())\n",
    "                    rules_curr = maps[chain][index][2].copy()\n",
    "                    rules_curr_norm = maps[chain][index][2].copy()\n",
    "                rules_new, rules_norm = self.propose(rules_curr.copy(), rules_curr_norm.copy(),q)\n",
    "                cfmatrix,prob =  self.compute_prob(rules_new)\n",
    "                T = T0**(1 - iter/Niteration)\n",
    "                pt_new = sum(prob)\n",
    "                alpha = np.exp(float(pt_new -pt_curr)/T)\n",
    "                \n",
    "                if (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)> acc[chain]:\n",
    "                    most_accurate_model[chain] = rules_new[:]\n",
    "                    acc[chain] = (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)\n",
    "                    if print_message:\n",
    "                        print('found a more accurate model: accuracy = {}'.format(acc[chain]))\n",
    "                        \n",
    "                if pt_new > sum(maps[chain][-1][1]):\n",
    "                    maps[chain].append([iter,prob,rules_new,[self.rules[i] for i in rules_new]])\n",
    "                    if print_message:\n",
    "                        print('\\n** chain = {}, max at iter = {} ** \\n accuracy = {}, TP = {},FP = {}, TN = {}, FN = {}\\n pt_new is {}, prior_ChsRules={}, likelihood_1 = {}, likelihood_2 = {}\\n '.format(chain, iter,(cfmatrix[0]+cfmatrix[2]+0.0)/len(self.Y),cfmatrix[0],cfmatrix[1],cfmatrix[2],cfmatrix[3],sum(prob), prob[0], prob[1], prob[2]))\n",
    "                        # print '\\n** chain = {}, max at iter = {} ** \\n obj = {}, prior = {}, llh = {} '.format(chain, iter,prior+llh,prior,llh)\n",
    "                        self.print_rules(rules_new)\n",
    "                        print(rules_new)\n",
    "                if random() <= alpha:\n",
    "                    rules_curr_norm,rules_curr,pt_curr = rules_norm.copy(),rules_new.copy(),pt_new\n",
    "        # print('\\tTook %0.3fs to generate an optimal rule set' % (time.time() - start_time))\n",
    "        if keep_most_accurate_model:\n",
    "            acc_list = [acc[chain] for chain in range(Nchain)]\n",
    "            index = acc_list.index(max(acc_list))\n",
    "            return [self.rules[i] for i in most_accurate_model[index]] \n",
    "        else:\n",
    "            pt_max = [sum(maps[chain][-1][1]) for chain in range(Nchain)]\n",
    "            index = pt_max.index(max(pt_max))\n",
    "            return maps[index][-1][3]\n",
    "\n",
    "    def propose(self, rules_curr,rules_norm,q):\n",
    "        nRules = len(self.rules)\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules_curr],axis = 1)>0).astype(int)\n",
    "        incorr = np.where(self.Y!=Yhat)[0]\n",
    "        N = len(rules_curr)\n",
    "        if len(incorr)==0:\n",
    "            clean = True\n",
    "            move = ['clean']\n",
    "            # it means the BOA correctly classified all points but there could be redundant patterns, so cleaning is needed\n",
    "        else:\n",
    "            clean = False\n",
    "            ex = sample(incorr.tolist(),1)[0]\n",
    "            t = random()\n",
    "            if self.Y[ex]==1 or N==1:\n",
    "                if t<1.0/2 or N==1:\n",
    "                    move = ['add']       # action: add\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "            else:\n",
    "                if t<1.0/2:\n",
    "                    move = ['cut']       # action: cut\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "        if move[0]=='cut':\n",
    "            \"\"\" cut \"\"\"\n",
    "            if random()<q:\n",
    "                candidate = list(set(np.where(self.RMatrix[ex,:]==1)[0]).intersection(rules_curr))\n",
    "                if len(candidate)==0:\n",
    "                    candidate = rules_curr\n",
    "                cut_rule = sample(candidate,1)[0]\n",
    "            else:\n",
    "                p = []\n",
    "                all_sum = np.sum(self.RMatrix[:,rules_curr],axis = 1)\n",
    "                for index,rule in enumerate(rules_curr):\n",
    "                    Yhat= ((all_sum - np.array(self.RMatrix[:,rule]))>0).astype(int)\n",
    "                    TP,FP,TN,FN  = getConfusion(Yhat,self.Y)\n",
    "                    p.append(TP.astype(float)/(TP+FP+1))\n",
    "                    # p.append(log_betabin(TP,TP+FP,self.alpha_1,self.beta_1) + log_betabin(FN,FN+TN,self.alpha_2,self.beta_2))\n",
    "                p = [x - min(p) for x in p]\n",
    "                p = np.exp(p)\n",
    "                p = np.insert(p,0,0)\n",
    "                p = np.array(list(accumulate(p)))\n",
    "                if p[-1]==0:\n",
    "                    index = sample(range(len(rules_curr)),1)[0]\n",
    "                else:\n",
    "                    p = p/p[-1]\n",
    "                index = find_lt(p,random())\n",
    "                cut_rule = rules_curr[index]\n",
    "            rules_curr.remove(cut_rule)\n",
    "            rules_norm = self.normalize(rules_curr)\n",
    "            move.remove('cut')\n",
    "            \n",
    "        if len(move)>0 and move[0]=='add':\n",
    "            \"\"\" add \"\"\"\n",
    "            if random()<q:\n",
    "                add_rule = sample(range(nRules),1)[0]\n",
    "            else: \n",
    "                Yhat_neg_index = list(np.where(np.sum(self.RMatrix[:,rules_curr],axis = 1)<1)[0])\n",
    "                mat = np.multiply(self.RMatrix[Yhat_neg_index,:].transpose(),self.Y[Yhat_neg_index])\n",
    "                # TP = np.array(np.sum(mat,axis = 0).tolist()[0])\n",
    "                TP = np.sum(mat,axis = 1)\n",
    "                FP = np.array((np.sum(self.RMatrix[Yhat_neg_index,:],axis = 0) - TP))\n",
    "                TN = np.sum(self.Y[Yhat_neg_index]==0)-FP\n",
    "                FN = sum(self.Y[Yhat_neg_index]) - TP\n",
    "                p = (TP.astype(float)/(TP+FP+1))\n",
    "                p[rules_curr]=0\n",
    "                add_rule = sample(np.where(p==max(p))[0].tolist(),1)[0]\n",
    "            if add_rule not in rules_curr:\n",
    "                rules_curr.append(add_rule)\n",
    "                rules_norm = self.normalize(rules_curr)\n",
    "\n",
    "        if len(move)>0 and move[0]=='clean':\n",
    "            remove = []\n",
    "            for i,rule in enumerate(rules_norm):\n",
    "                Yhat = (np.sum(self.RMatrix[:,[rule for j,rule in enumerate(rules_norm) if (j!=i and j not in remove)]],axis = 1)>0).astype(int)\n",
    "                TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "                if TP+FP==0:\n",
    "                    remove.append(i)\n",
    "            for x in remove:\n",
    "                rules_norm.remove(x)\n",
    "            return rules_curr, rules_norm\n",
    "        return rules_curr, rules_norm\n",
    "\n",
    "    def compute_prob(self,rules):\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules],axis = 1)>0).astype(int)\n",
    "        TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "        Kn_count = list(np.bincount([self.rules_len[x] for x in rules], minlength = self.maxlen+1))\n",
    "        prior_ChsRules= sum([log_betabin(Kn_count[i],self.patternSpace[i],self.alpha_l[i],self.beta_l[i]) for i in range(1,len(Kn_count),1)])            \n",
    "        likelihood_1 =  log_betabin(TP,TP+FP,self.alpha_1,self.beta_1)\n",
    "        likelihood_2 = log_betabin(TN,FN+TN,self.alpha_2,self.beta_2)\n",
    "        return [TP,FP,TN,FN],[prior_ChsRules,likelihood_1,likelihood_2]\n",
    "\n",
    "    def normalize_add(self, rules_new, rule_index):\n",
    "        rules = rules_new.copy()\n",
    "        for rule in rules_new:\n",
    "            if set(self.rules[rule]).issubset(self.rules[rule_index]):\n",
    "                return rules_new.copy()\n",
    "            if set(self.rules[rule_index]).issubset(self.rules[rule]):\n",
    "                rules.remove(rule)\n",
    "        rules.append(rule_index)\n",
    "        return rules\n",
    "\n",
    "    def normalize(self, rules_new):\n",
    "        try:\n",
    "            rules_len = [len(self.rules[index]) for index in rules_new]\n",
    "            rules = [rules_new[i] for i in np.argsort(rules_len)[::-1][:len(rules_len)]]\n",
    "            p1 = 0\n",
    "            while p1<len(rules):\n",
    "                for p2 in range(p1+1,len(rules),1):\n",
    "                    if set(self.rules[rules[p2]]).issubset(set(self.rules[rules[p1]])):\n",
    "                        rules.remove(rules[p1])\n",
    "                        p1 -= 1\n",
    "                        break\n",
    "                p1 += 1\n",
    "            return rules\n",
    "        except:\n",
    "            return rules_new.copy()\n",
    "\n",
    "\n",
    "    def print_rules(self, rules_max):\n",
    "        for rule_index in rules_max:\n",
    "            print(self.rules[rule_index])\n",
    "            \n",
    "\n",
    "\"\"\" parameters \"\"\"\n",
    "# The following parameters are recommended to change depending on the size and complexity of the data\n",
    "N = 2000      # number of rules to be used in SA_patternbased and also the output of generate_rules\n",
    "Niteration = 500  # number of iterations in each chain\n",
    "Nchain = 2         # number of chains in the simulated annealing search algorithm\n",
    "\n",
    "supp = 5           # 5% is a generally good number. The higher this supp, the 'larger' a pattern is\n",
    "maxlen = 3         # maxmum length of a pattern\n",
    "\n",
    "# \\rho = alpha/(alpha+beta). Make sure \\rho is close to one when choosing alpha and beta. \n",
    "alpha_1 = 500       # alpha_+\n",
    "beta_1 = 1          # beta_+\n",
    "alpha_2 = 500         # alpha_-\n",
    "beta_2 = 1       # beta_-\n",
    "\n",
    "\"\"\" input file \"\"\"\n",
    "# notice that in the example, X is already binary coded. \n",
    "# Data has to be binary coded and the column name shd have the form: attributename_attributevalue\n",
    "filepathX = 'tictactoe_X.txt' # input file X\n",
    "filepathY = 'tictactoe_Y.txt' # input file Y\n",
    "df = pd.read_csv(filepathX,header=0,sep=\" \")\n",
    "Y = np.loadtxt(open(filepathY,\"rb\"),delimiter=\" \")\n",
    "\n",
    "import random\n",
    "lenY = len(Y)\n",
    "train_index = random.sample(range(lenY), int(0.70 * lenY))\n",
    "test_index = [i for i in range(lenY) if i not in train_index]\n",
    "\n",
    "model = BOA(df.iloc[train_index],Y[train_index])\n",
    "model.generate_rules(supp,maxlen,N)\n",
    "model.set_parameters(alpha_1,beta_1,alpha_2,beta_2,None,None)\n",
    "rules = model.fit(Niteration,Nchain,print_message=True)\n",
    "\n",
    "# test\n",
    "Yhat = predict(rules,df.iloc[test_index])\n",
    "TP,FP,TN,FN = getConfusion(Yhat,Y[test_index])\n",
    "tpr = float(TP)/(TP+FN)\n",
    "fpr = float(FP)/(FP+TN)\n",
    "print('TP = {}, FP = {}, TN = {}, FN = {} \\n accuracy = {}, tpr = {}, fpr = {}'.format(TP,FP,TN,FN, float(TP+TN)/(TP+TN+FP+FN),tpr,fpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTook 3.655s to generate 18594 rules\n",
      "Screening rules using information gain\n",
      "\tTook 0.502s to generate 2000 rules\n",
      "Computing sizes for pattern space ...\n",
      "\tTook 0.000s to compute patternspace\n",
      "No or wrong input for alpha_l and beta_l. The model will use default parameters!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-1d36f7cd83a1>:107: RuntimeWarning: divide by zero encountered in log\n",
      "  cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
      "<ipython-input-8-1d36f7cd83a1>:107: RuntimeWarning: invalid value encountered in multiply\n",
      "  cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
      "<ipython-input-8-1d36f7cd83a1>:109: RuntimeWarning: divide by zero encountered in log\n",
      "  cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
      "<ipython-input-8-1d36f7cd83a1>:109: RuntimeWarning: invalid value encountered in multiply\n",
      "  cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable. Did you mean: 'random.random(...)'?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/x/Desktop/Repositories/BOA_model.py\u001b[0m in \u001b[0;36mline 348\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=345'>346</a>\u001b[0m model\u001b[39m.\u001b[39mgenerate_rules(supp,maxlen,N)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=346'>347</a>\u001b[0m model\u001b[39m.\u001b[39mset_parameters(alpha_1,beta_1,alpha_2,beta_2,\u001b[39mNone\u001b[39;00m,\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=347'>348</a>\u001b[0m rules \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(Niteration,Nchain,print_message\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=349'>350</a>\u001b[0m \u001b[39m# test\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=350'>351</a>\u001b[0m Yhat \u001b[39m=\u001b[39m predict(rules,df\u001b[39m.\u001b[39miloc[test_index])\n",
      "\u001b[1;32m/Users/x/Desktop/Repositories/BOA_model.py\u001b[0m in \u001b[0;36mline 162\u001b[0m, in \u001b[0;36mBOA.fit\u001b[0;34m(self, Niteration, Nchain, q, init, keep_most_accurate_model, print_message)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=159'>160</a>\u001b[0m     rules_curr \u001b[39m=\u001b[39m maps[chain][index][\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=160'>161</a>\u001b[0m     rules_curr_norm \u001b[39m=\u001b[39m maps[chain][index][\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=161'>162</a>\u001b[0m rules_new, rules_norm \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropose(rules_curr\u001b[39m.\u001b[39;49mcopy(), rules_curr_norm\u001b[39m.\u001b[39;49mcopy(),q)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=162'>163</a>\u001b[0m cfmatrix,prob \u001b[39m=\u001b[39m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_prob(rules_new)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=163'>164</a>\u001b[0m T \u001b[39m=\u001b[39m T0\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39miter\u001b[39m\u001b[39m/\u001b[39mNiteration)\n",
      "\u001b[1;32m/Users/x/Desktop/Repositories/BOA_model.py\u001b[0m in \u001b[0;36mline 218\u001b[0m, in \u001b[0;36mBOA.propose\u001b[0;34m(self, rules_curr, rules_norm, q)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=215'>216</a>\u001b[0m \u001b[39mif\u001b[39;00m move[\u001b[39m0\u001b[39m]\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcut\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=216'>217</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" cut \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=217'>218</a>\u001b[0m     \u001b[39mif\u001b[39;00m random()\u001b[39m<\u001b[39mq:\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=218'>219</a>\u001b[0m         candidate \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(np\u001b[39m.\u001b[39mwhere(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mRMatrix[ex,:]\u001b[39m==\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mintersection(rules_curr))\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=219'>220</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(candidate)\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable. Did you mean: 'random.random(...)'?"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# from fim import fpgrowth,fim --> this package is difficult to install. So the rule miner can be replaced by a random forest\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import chain, combinations\n",
    "import itertools\n",
    "from numpy.random import random\n",
    "from random import sample\n",
    "import time\n",
    "import operator\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csc_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "class BOA(object):\n",
    "    def __init__(self, binary_data,Y):\n",
    "        self.df = binary_data  \n",
    "        self.Y = Y\n",
    "        self.attributeLevelNum = defaultdict(int) \n",
    "        self.attributeNames = []\n",
    "        for i,name in enumerate(binary_data.columns):\n",
    "          attribute = name.split('_')[0]\n",
    "          self.attributeLevelNum[attribute] += 1\n",
    "          self.attributeNames.append(attribute)\n",
    "        self.attributeNames = list(set(self.attributeNames))\n",
    "        \n",
    "\n",
    "    def getPatternSpace(self):\n",
    "        print('Computing sizes for pattern space ...')\n",
    "        start_time = time.time()\n",
    "        \"\"\" compute the rule space from the levels in each attribute \"\"\"\n",
    "        for item in self.attributeNames:\n",
    "            self.attributeLevelNum[item+'_neg'] = self.attributeLevelNum[item]\n",
    "        self.patternSpace = np.zeros(self.maxlen+1)\n",
    "        tmp = [ item + '_neg' for item in self.attributeNames]\n",
    "        self.attributeNames.extend(tmp)\n",
    "        for k in range(1,self.maxlen+1,1):\n",
    "            for subset in combinations(self.attributeNames,k):\n",
    "                tmp = 1\n",
    "                for i in subset:\n",
    "                    tmp = tmp * self.attributeLevelNum[i]\n",
    "                self.patternSpace[k] = self.patternSpace[k] + tmp\n",
    "        print('\\tTook %0.3fs to compute patternspace' % (time.time() - start_time))               \n",
    "\n",
    "# This function generates rules that satisfy supp and maxlen using fpgrowth, then it selects the top N rules that make data have the biggest decrease in entropy\n",
    "# there are two ways to generate rules. fpgrowth can handle cases where the maxlen is small. If maxlen<=3, fpgrowth can generates rules much faster than randomforest. \n",
    "# If maxlen is big, fpgrowh tends to generate too many rules that overflow the memories. \n",
    "    def generate_rules(self,supp,maxlen,N, method = 'randomforest'):\n",
    "        self.maxlen = maxlen\n",
    "        self.supp = supp\n",
    "        df = 1-self.df #df has negative associations\n",
    "        df.columns = [name.strip() + '_neg' for name in self.df.columns]\n",
    "        df = pd.concat([self.df,df],axis = 1)\n",
    "#         if method =='fpgrowth' and maxlen<=3:\n",
    "#             itemMatrix = [[item for item in df.columns if row[item] ==1] for i,row in df.iterrows() ]  \n",
    "#             pindex = np.where(self.Y==1)[0]\n",
    "#             nindex = np.where(self.Y!=1)[0]\n",
    "#             print('Generating rules using fpgrowth')\n",
    "#             start_time = time.time()\n",
    "#             rules= fpgrowth([itemMatrix[i] for i in pindex],supp = supp,zmin = 1,zmax = maxlen)\n",
    "#             rules = [tuple(np.sort(rule[0])) for rule in rules]\n",
    "#             rules = list(set(rules))\n",
    "#             start_time = time.time()\n",
    "#             print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "#         else:\n",
    "        rules = []\n",
    "        start_time = time.time()\n",
    "        for length in range(1,maxlen+1,1):\n",
    "            n_estimators = min(pow(df.shape[1],length),4000)\n",
    "            clf = RandomForestClassifier(n_estimators = n_estimators,max_depth = length)\n",
    "            clf.fit(self.df,self.Y)\n",
    "            for n in range(n_estimators):\n",
    "                rules.extend(extract_rules(clf.estimators_[n],df.columns))\n",
    "        rules = [list(x) for x in set(tuple(x) for x in rules)]\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "        self.screen_rules(rules,df,N) # select the top N rules using secondary criteria, information gain\n",
    "        self.getPatternSpace()\n",
    "\n",
    "    def screen_rules(self,rules,df,N):\n",
    "        print('Screening rules using information gain')\n",
    "        start_time = time.time()\n",
    "        itemInd = {}\n",
    "        for i,name in enumerate(df.columns):\n",
    "            itemInd[name] = i\n",
    "        indices = np.array(list(itertools.chain.from_iterable([[itemInd[x] for x in rule] for rule in rules])))\n",
    "        len_rules = [len(rule) for rule in rules]\n",
    "        indptr =list(accumulate(len_rules))\n",
    "        indptr.insert(0,0)\n",
    "        indptr = np.array(indptr)\n",
    "        data = np.ones(len(indices))\n",
    "        ruleMatrix = csc_matrix((data,indices,indptr),shape = (len(df.columns),len(rules)))\n",
    "        mat = np.matrix(df) * ruleMatrix\n",
    "        lenMatrix = np.matrix([len_rules for i in range(df.shape[0])])\n",
    "        Z =  (mat ==lenMatrix).astype(int)\n",
    "        Zpos = [Z[i] for i in np.where(self.Y>0)][0]\n",
    "        TP = np.array(np.sum(Zpos,axis=0).tolist()[0])\n",
    "        supp_select = np.where(TP>=self.supp*sum(self.Y)/100)[0]\n",
    "        FP = np.array(np.sum(Z,axis = 0))[0] - TP\n",
    "        TN = len(self.Y) - np.sum(self.Y) - FP\n",
    "        FN = np.sum(self.Y) - TP\n",
    "        p1 = TP.astype(float)/(TP+FP)\n",
    "        p2 = FN.astype(float)/(FN+TN)\n",
    "        pp = (TP+FP).astype(float)/(TP+FP+TN+FN)\n",
    "        tpr = TP.astype(float)/(TP+FN)\n",
    "        fpr = FP.astype(float)/(FP+TN)\n",
    "        cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
    "        cond_entropy[p1*(1-p1)==0] = -((1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2)))[p1*(1-p1)==0]\n",
    "        cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
    "        cond_entropy[p1*(1-p1)*p2*(1-p2)==0] = 0\n",
    "        select = np.argsort(cond_entropy[supp_select])[::-1][-N:]\n",
    "        self.rules = [rules[i] for i in supp_select[select]]\n",
    "        self.RMatrix = np.array(Z[:,supp_select[select]])\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(self.rules)))\n",
    "\n",
    "    def set_parameters(self, a1=100,b1=10,a2=100,b2=10,al=None,bl=None):\n",
    "        # input al and bl are lists\n",
    "        # a1/(a1 + b1) and a2/(a2 + b2) should be a  bigger than 0.5. They correspond to alpha_+, beta_+ and alpha_- and beta_- from the paper. In fact, these four values play an important role and it needs to be tuned while following the a1/(a1 + b1) >0.5 and a2/(a2 + b2) >0.5 principle\n",
    "        self.alpha_1 = a1\n",
    "        self.beta_1 = b1\n",
    "        self.alpha_2 = a2\n",
    "        self.beta_2 = b2\n",
    "        if al ==None or bl==None or len(al)!=self.maxlen or len(bl)!=self.maxlen:\n",
    "            print('No or wrong input for alpha_l and beta_l. The model will use default parameters!')\n",
    "            self.C = [1.0/self.maxlen for i in range(self.maxlen)]\n",
    "            self.C.insert(0,-1)\n",
    "            self.alpha_l = [10 for i in range(self.maxlen+1)]\n",
    "            self.beta_l= [10*self.patternSpace[i]/self.C[i] for i in range(self.maxlen+1)]\n",
    "        else:\n",
    "            self.alpha_l=[1] + list(al)\n",
    "            self.beta_l = [1] + list(bl)\n",
    "\n",
    "    def fit(self, Niteration = 300, Nchain = 1, q = 0.1, init = [], keep_most_accurate_model = True,print_message=True):\n",
    "        # print('Searching for an optimal solution...')\n",
    "        start_time = time.time()\n",
    "        nRules = len(self.rules)\n",
    "        self.rules_len = [len(rule) for rule in self.rules]\n",
    "        maps = defaultdict(list)\n",
    "        T0 = 1000\n",
    "        split = 0.7*Niteration\n",
    "        acc = {}\n",
    "        most_accurate_model = defaultdict(list)\n",
    "        for chain in range(Nchain):\n",
    "            # initialize with a random pattern set\n",
    "            if init !=[]:\n",
    "                rules_curr = init.copy()\n",
    "            else:\n",
    "                N = sample(range(1,min(8,nRules),1),1)[0]\n",
    "                rules_curr = sample(range(nRules),N)\n",
    "            rules_curr_norm = self.normalize(rules_curr)\n",
    "            pt_curr = -100000000000\n",
    "            maps[chain].append([-1,[pt_curr/3,pt_curr/3,pt_curr/3],rules_curr,[self.rules[i] for i in rules_curr]])\n",
    "            acc[chain] = 0\n",
    "            for iter in range(Niteration):\n",
    "                if iter>=split:\n",
    "                    p = np.array(range(1+len(maps[chain])))\n",
    "                    p = np.array(list(accumulate(p)))\n",
    "                    p = p/p[-1]\n",
    "                    index = find_lt(p,random())\n",
    "                    rules_curr = maps[chain][index][2].copy()\n",
    "                    rules_curr_norm = maps[chain][index][2].copy()\n",
    "                rules_new, rules_norm = self.propose(rules_curr.copy(), rules_curr_norm.copy(),q)\n",
    "                cfmatrix,prob =  self.compute_prob(rules_new)\n",
    "                T = T0**(1 - iter/Niteration)\n",
    "                pt_new = sum(prob)\n",
    "                alpha = np.exp(float(pt_new -pt_curr)/T)\n",
    "                \n",
    "                if (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)> acc[chain]:\n",
    "                    most_accurate_model[chain] = rules_new[:]\n",
    "                    acc[chain] = (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)\n",
    "                    if print_message:\n",
    "                        print('found a more accurate model: accuracy = {}'.format(acc[chain]))\n",
    "                        \n",
    "                if pt_new > sum(maps[chain][-1][1]):\n",
    "                    maps[chain].append([iter,prob,rules_new,[self.rules[i] for i in rules_new]])\n",
    "                    if print_message:\n",
    "                        print('\\n** chain = {}, max at iter = {} ** \\n accuracy = {}, TP = {},FP = {}, TN = {}, FN = {}\\n pt_new is {}, prior_ChsRules={}, likelihood_1 = {}, likelihood_2 = {}\\n '.format(chain, iter,(cfmatrix[0]+cfmatrix[2]+0.0)/len(self.Y),cfmatrix[0],cfmatrix[1],cfmatrix[2],cfmatrix[3],sum(prob), prob[0], prob[1], prob[2]))\n",
    "                        # print '\\n** chain = {}, max at iter = {} ** \\n obj = {}, prior = {}, llh = {} '.format(chain, iter,prior+llh,prior,llh)\n",
    "                        self.print_rules(rules_new)\n",
    "                        print(rules_new)\n",
    "                if random() <= alpha:\n",
    "                    rules_curr_norm,rules_curr,pt_curr = rules_norm.copy(),rules_new.copy(),pt_new\n",
    "        # print('\\tTook %0.3fs to generate an optimal rule set' % (time.time() - start_time))\n",
    "        if keep_most_accurate_model:\n",
    "            acc_list = [acc[chain] for chain in range(Nchain)]\n",
    "            index = acc_list.index(max(acc_list))\n",
    "            return [self.rules[i] for i in most_accurate_model[index]] \n",
    "        else:\n",
    "            pt_max = [sum(maps[chain][-1][1]) for chain in range(Nchain)]\n",
    "            index = pt_max.index(max(pt_max))\n",
    "            return maps[index][-1][3]\n",
    "\n",
    "    def propose(self, rules_curr,rules_norm,q):\n",
    "        nRules = len(self.rules)\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules_curr],axis = 1)>0).astype(int)\n",
    "        incorr = np.where(self.Y!=Yhat)[0]\n",
    "        N = len(rules_curr)\n",
    "        if len(incorr)==0:\n",
    "            clean = True\n",
    "            move = ['clean']\n",
    "            # it means the BOA correctly classified all points but there could be redundant patterns, so cleaning is needed\n",
    "        else:\n",
    "            clean = False\n",
    "            ex = sample(incorr.tolist(),1)[0]\n",
    "            t = random.random()\n",
    "            if self.Y[ex]==1 or N==1:\n",
    "                if t<1.0/2 or N==1:\n",
    "                    move = ['add']       # action: add\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "            else:\n",
    "                if t<1.0/2:\n",
    "                    move = ['cut']       # action: cut\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "        if move[0]=='cut':\n",
    "            \"\"\" cut \"\"\"\n",
    "            if random()<q:\n",
    "                candidate = list(set(np.where(self.RMatrix[ex,:]==1)[0]).intersection(rules_curr))\n",
    "                if len(candidate)==0:\n",
    "                    candidate = rules_curr\n",
    "                cut_rule = sample(candidate,1)[0]\n",
    "            else:\n",
    "                p = []\n",
    "                all_sum = np.sum(self.RMatrix[:,rules_curr],axis = 1)\n",
    "                for index,rule in enumerate(rules_curr):\n",
    "                    Yhat= ((all_sum - np.array(self.RMatrix[:,rule]))>0).astype(int)\n",
    "                    TP,FP,TN,FN  = getConfusion(Yhat,self.Y)\n",
    "                    p.append(TP.astype(float)/(TP+FP+1))\n",
    "                    # p.append(log_betabin(TP,TP+FP,self.alpha_1,self.beta_1) + log_betabin(FN,FN+TN,self.alpha_2,self.beta_2))\n",
    "                p = [x - min(p) for x in p]\n",
    "                p = np.exp(p)\n",
    "                p = np.insert(p,0,0)\n",
    "                p = np.array(list(accumulate(p)))\n",
    "                if p[-1]==0:\n",
    "                    index = sample(range(len(rules_curr)),1)[0]\n",
    "                else:\n",
    "                    p = p/p[-1]\n",
    "                index = find_lt(p,random())\n",
    "                cut_rule = rules_curr[index]\n",
    "            rules_curr.remove(cut_rule)\n",
    "            rules_norm = self.normalize(rules_curr)\n",
    "            move.remove('cut')\n",
    "            \n",
    "        if len(move)>0 and move[0]=='add':\n",
    "            \"\"\" add \"\"\"\n",
    "            if random()<q:\n",
    "                add_rule = sample(range(nRules),1)[0]\n",
    "            else: \n",
    "                Yhat_neg_index = list(np.where(np.sum(self.RMatrix[:,rules_curr],axis = 1)<1)[0])\n",
    "                mat = np.multiply(self.RMatrix[Yhat_neg_index,:].transpose(),self.Y[Yhat_neg_index])\n",
    "                # TP = np.array(np.sum(mat,axis = 0).tolist()[0])\n",
    "                TP = np.sum(mat,axis = 1)\n",
    "                FP = np.array((np.sum(self.RMatrix[Yhat_neg_index,:],axis = 0) - TP))\n",
    "                TN = np.sum(self.Y[Yhat_neg_index]==0)-FP\n",
    "                FN = sum(self.Y[Yhat_neg_index]) - TP\n",
    "                p = (TP.astype(float)/(TP+FP+1))\n",
    "                p[rules_curr]=0\n",
    "                add_rule = sample(np.where(p==max(p))[0].tolist(),1)[0]\n",
    "            if add_rule not in rules_curr:\n",
    "                rules_curr.append(add_rule)\n",
    "                rules_norm = self.normalize(rules_curr)\n",
    "\n",
    "        if len(move)>0 and move[0]=='clean':\n",
    "            remove = []\n",
    "            for i,rule in enumerate(rules_norm):\n",
    "                Yhat = (np.sum(self.RMatrix[:,[rule for j,rule in enumerate(rules_norm) if (j!=i and j not in remove)]],axis = 1)>0).astype(int)\n",
    "                TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "                if TP+FP==0:\n",
    "                    remove.append(i)\n",
    "            for x in remove:\n",
    "                rules_norm.remove(x)\n",
    "            return rules_curr, rules_norm\n",
    "        return rules_curr, rules_norm\n",
    "\n",
    "    def compute_prob(self,rules):\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules],axis = 1)>0).astype(int)\n",
    "        TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "        Kn_count = list(np.bincount([self.rules_len[x] for x in rules], minlength = self.maxlen+1))\n",
    "        prior_ChsRules= sum([log_betabin(Kn_count[i],self.patternSpace[i],self.alpha_l[i],self.beta_l[i]) for i in range(1,len(Kn_count),1)])            \n",
    "        likelihood_1 =  log_betabin(TP,TP+FP,self.alpha_1,self.beta_1)\n",
    "        likelihood_2 = log_betabin(TN,FN+TN,self.alpha_2,self.beta_2)\n",
    "        return [TP,FP,TN,FN],[prior_ChsRules,likelihood_1,likelihood_2]\n",
    "\n",
    "    def normalize_add(self, rules_new, rule_index):\n",
    "        rules = rules_new.copy()\n",
    "        for rule in rules_new:\n",
    "            if set(self.rules[rule]).issubset(self.rules[rule_index]):\n",
    "                return rules_new.copy()\n",
    "            if set(self.rules[rule_index]).issubset(self.rules[rule]):\n",
    "                rules.remove(rule)\n",
    "        rules.append(rule_index)\n",
    "        return rules\n",
    "\n",
    "    def normalize(self, rules_new):\n",
    "        try:\n",
    "            rules_len = [len(self.rules[index]) for index in rules_new]\n",
    "            rules = [rules_new[i] for i in np.argsort(rules_len)[::-1][:len(rules_len)]]\n",
    "            p1 = 0\n",
    "            while p1<len(rules):\n",
    "                for p2 in range(p1+1,len(rules),1):\n",
    "                    if set(self.rules[rules[p2]]).issubset(set(self.rules[rules[p1]])):\n",
    "                        rules.remove(rules[p1])\n",
    "                        p1 -= 1\n",
    "                        break\n",
    "                p1 += 1\n",
    "            return rules\n",
    "        except:\n",
    "            return rules_new.copy()\n",
    "\n",
    "\n",
    "    def print_rules(self, rules_max):\n",
    "        for rule_index in rules_max:\n",
    "            print(self.rules[rule_index])\n",
    "            \n",
    "\n",
    "\"\"\" parameters \"\"\"\n",
    "# The following parameters are recommended to change depending on the size and complexity of the data\n",
    "N = 2000      # number of rules to be used in SA_patternbased and also the output of generate_rules\n",
    "Niteration = 500  # number of iterations in each chain\n",
    "Nchain = 2         # number of chains in the simulated annealing search algorithm\n",
    "\n",
    "supp = 5           # 5% is a generally good number. The higher this supp, the 'larger' a pattern is\n",
    "maxlen = 3         # maxmum length of a pattern\n",
    "\n",
    "# \\rho = alpha/(alpha+beta). Make sure \\rho is close to one when choosing alpha and beta. \n",
    "alpha_1 = 500       # alpha_+\n",
    "beta_1 = 1          # beta_+\n",
    "alpha_2 = 500         # alpha_-\n",
    "beta_2 = 1       # beta_-\n",
    "\n",
    "\"\"\" input file \"\"\"\n",
    "# notice that in the example, X is already binary coded. \n",
    "# Data has to be binary coded and the column name shd have the form: attributename_attributevalue\n",
    "filepathX = 'tictactoe_X.txt' # input file X\n",
    "filepathY = 'tictactoe_Y.txt' # input file Y\n",
    "df = pd.read_csv(filepathX,header=0,sep=\" \")\n",
    "Y = np.loadtxt(open(filepathY,\"rb\"),delimiter=\" \")\n",
    "\n",
    "import random\n",
    "lenY = len(Y)\n",
    "train_index = random.sample(range(lenY), int(0.70 * lenY))\n",
    "test_index = [i for i in range(lenY) if i not in train_index]\n",
    "\n",
    "model = BOA(df.iloc[train_index],Y[train_index])\n",
    "model.generate_rules(supp,maxlen,N)\n",
    "model.set_parameters(alpha_1,beta_1,alpha_2,beta_2,None,None)\n",
    "rules = model.fit(Niteration,Nchain,print_message=True)\n",
    "\n",
    "# test\n",
    "Yhat = predict(rules,df.iloc[test_index])\n",
    "TP,FP,TN,FN = getConfusion(Yhat,Y[test_index])\n",
    "tpr = float(TP)/(TP+FN)\n",
    "fpr = float(FP)/(FP+TN)\n",
    "print('TP = {}, FP = {}, TN = {}, FN = {} \\n accuracy = {}, tpr = {}, fpr = {}'.format(TP,FP,TN,FN, float(TP+TN)/(TP+TN+FP+FN),tpr,fpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTook 3.616s to generate 18804 rules\n",
      "Screening rules using information gain\n",
      "\tTook 0.484s to generate 2000 rules\n",
      "Computing sizes for pattern space ...\n",
      "\tTook 0.000s to compute patternspace\n",
      "No or wrong input for alpha_l and beta_l. The model will use default parameters!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-1e475c4824ea>:107: RuntimeWarning: divide by zero encountered in log\n",
      "  cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
      "<ipython-input-9-1e475c4824ea>:107: RuntimeWarning: invalid value encountered in multiply\n",
      "  cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
      "<ipython-input-9-1e475c4824ea>:109: RuntimeWarning: divide by zero encountered in log\n",
      "  cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
      "<ipython-input-9-1e475c4824ea>:109: RuntimeWarning: invalid value encountered in multiply\n",
      "  cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable. Did you mean: 'random.random(...)'?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/x/Desktop/Repositories/BOA_model.py\u001b[0m in \u001b[0;36mline 348\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=345'>346</a>\u001b[0m model\u001b[39m.\u001b[39mgenerate_rules(supp,maxlen,N)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=346'>347</a>\u001b[0m model\u001b[39m.\u001b[39mset_parameters(alpha_1,beta_1,alpha_2,beta_2,\u001b[39mNone\u001b[39;00m,\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=347'>348</a>\u001b[0m rules \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(Niteration,Nchain,print_message\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=349'>350</a>\u001b[0m \u001b[39m# test\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=350'>351</a>\u001b[0m Yhat \u001b[39m=\u001b[39m predict(rules,df\u001b[39m.\u001b[39miloc[test_index])\n",
      "\u001b[1;32m/Users/x/Desktop/Repositories/BOA_model.py\u001b[0m in \u001b[0;36mline 162\u001b[0m, in \u001b[0;36mBOA.fit\u001b[0;34m(self, Niteration, Nchain, q, init, keep_most_accurate_model, print_message)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=159'>160</a>\u001b[0m     rules_curr \u001b[39m=\u001b[39m maps[chain][index][\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=160'>161</a>\u001b[0m     rules_curr_norm \u001b[39m=\u001b[39m maps[chain][index][\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=161'>162</a>\u001b[0m rules_new, rules_norm \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropose(rules_curr\u001b[39m.\u001b[39;49mcopy(), rules_curr_norm\u001b[39m.\u001b[39;49mcopy(),q)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=162'>163</a>\u001b[0m cfmatrix,prob \u001b[39m=\u001b[39m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_prob(rules_new)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=163'>164</a>\u001b[0m T \u001b[39m=\u001b[39m T0\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39miter\u001b[39m\u001b[39m/\u001b[39mNiteration)\n",
      "\u001b[1;32m/Users/x/Desktop/Repositories/BOA_model.py\u001b[0m in \u001b[0;36mline 247\u001b[0m, in \u001b[0;36mBOA.propose\u001b[0;34m(self, rules_curr, rules_norm, q)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=244'>245</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(move)\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m move[\u001b[39m0\u001b[39m]\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39madd\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=245'>246</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" add \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=246'>247</a>\u001b[0m     \u001b[39mif\u001b[39;00m random()\u001b[39m<\u001b[39mq:\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=247'>248</a>\u001b[0m         add_rule \u001b[39m=\u001b[39m sample(\u001b[39mrange\u001b[39m(nRules),\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=248'>249</a>\u001b[0m     \u001b[39melse\u001b[39;00m: \n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable. Did you mean: 'random.random(...)'?"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# from fim import fpgrowth,fim --> this package is difficult to install. So the rule miner can be replaced by a random forest\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import chain, combinations\n",
    "import itertools\n",
    "from numpy.random import random\n",
    "from random import sample\n",
    "import time\n",
    "import operator\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csc_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "class BOA(object):\n",
    "    def __init__(self, binary_data,Y):\n",
    "        self.df = binary_data  \n",
    "        self.Y = Y\n",
    "        self.attributeLevelNum = defaultdict(int) \n",
    "        self.attributeNames = []\n",
    "        for i,name in enumerate(binary_data.columns):\n",
    "          attribute = name.split('_')[0]\n",
    "          self.attributeLevelNum[attribute] += 1\n",
    "          self.attributeNames.append(attribute)\n",
    "        self.attributeNames = list(set(self.attributeNames))\n",
    "        \n",
    "\n",
    "    def getPatternSpace(self):\n",
    "        print('Computing sizes for pattern space ...')\n",
    "        start_time = time.time()\n",
    "        \"\"\" compute the rule space from the levels in each attribute \"\"\"\n",
    "        for item in self.attributeNames:\n",
    "            self.attributeLevelNum[item+'_neg'] = self.attributeLevelNum[item]\n",
    "        self.patternSpace = np.zeros(self.maxlen+1)\n",
    "        tmp = [ item + '_neg' for item in self.attributeNames]\n",
    "        self.attributeNames.extend(tmp)\n",
    "        for k in range(1,self.maxlen+1,1):\n",
    "            for subset in combinations(self.attributeNames,k):\n",
    "                tmp = 1\n",
    "                for i in subset:\n",
    "                    tmp = tmp * self.attributeLevelNum[i]\n",
    "                self.patternSpace[k] = self.patternSpace[k] + tmp\n",
    "        print('\\tTook %0.3fs to compute patternspace' % (time.time() - start_time))               \n",
    "\n",
    "# This function generates rules that satisfy supp and maxlen using fpgrowth, then it selects the top N rules that make data have the biggest decrease in entropy\n",
    "# there are two ways to generate rules. fpgrowth can handle cases where the maxlen is small. If maxlen<=3, fpgrowth can generates rules much faster than randomforest. \n",
    "# If maxlen is big, fpgrowh tends to generate too many rules that overflow the memories. \n",
    "    def generate_rules(self,supp,maxlen,N, method = 'randomforest'):\n",
    "        self.maxlen = maxlen\n",
    "        self.supp = supp\n",
    "        df = 1-self.df #df has negative associations\n",
    "        df.columns = [name.strip() + '_neg' for name in self.df.columns]\n",
    "        df = pd.concat([self.df,df],axis = 1)\n",
    "#         if method =='fpgrowth' and maxlen<=3:\n",
    "#             itemMatrix = [[item for item in df.columns if row[item] ==1] for i,row in df.iterrows() ]  \n",
    "#             pindex = np.where(self.Y==1)[0]\n",
    "#             nindex = np.where(self.Y!=1)[0]\n",
    "#             print('Generating rules using fpgrowth')\n",
    "#             start_time = time.time()\n",
    "#             rules= fpgrowth([itemMatrix[i] for i in pindex],supp = supp,zmin = 1,zmax = maxlen)\n",
    "#             rules = [tuple(np.sort(rule[0])) for rule in rules]\n",
    "#             rules = list(set(rules))\n",
    "#             start_time = time.time()\n",
    "#             print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "#         else:\n",
    "        rules = []\n",
    "        start_time = time.time()\n",
    "        for length in range(1,maxlen+1,1):\n",
    "            n_estimators = min(pow(df.shape[1],length),4000)\n",
    "            clf = RandomForestClassifier(n_estimators = n_estimators,max_depth = length)\n",
    "            clf.fit(self.df,self.Y)\n",
    "            for n in range(n_estimators):\n",
    "                rules.extend(extract_rules(clf.estimators_[n],df.columns))\n",
    "        rules = [list(x) for x in set(tuple(x) for x in rules)]\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "        self.screen_rules(rules,df,N) # select the top N rules using secondary criteria, information gain\n",
    "        self.getPatternSpace()\n",
    "\n",
    "    def screen_rules(self,rules,df,N):\n",
    "        print('Screening rules using information gain')\n",
    "        start_time = time.time()\n",
    "        itemInd = {}\n",
    "        for i,name in enumerate(df.columns):\n",
    "            itemInd[name] = i\n",
    "        indices = np.array(list(itertools.chain.from_iterable([[itemInd[x] for x in rule] for rule in rules])))\n",
    "        len_rules = [len(rule) for rule in rules]\n",
    "        indptr =list(accumulate(len_rules))\n",
    "        indptr.insert(0,0)\n",
    "        indptr = np.array(indptr)\n",
    "        data = np.ones(len(indices))\n",
    "        ruleMatrix = csc_matrix((data,indices,indptr),shape = (len(df.columns),len(rules)))\n",
    "        mat = np.matrix(df) * ruleMatrix\n",
    "        lenMatrix = np.matrix([len_rules for i in range(df.shape[0])])\n",
    "        Z =  (mat ==lenMatrix).astype(int)\n",
    "        Zpos = [Z[i] for i in np.where(self.Y>0)][0]\n",
    "        TP = np.array(np.sum(Zpos,axis=0).tolist()[0])\n",
    "        supp_select = np.where(TP>=self.supp*sum(self.Y)/100)[0]\n",
    "        FP = np.array(np.sum(Z,axis = 0))[0] - TP\n",
    "        TN = len(self.Y) - np.sum(self.Y) - FP\n",
    "        FN = np.sum(self.Y) - TP\n",
    "        p1 = TP.astype(float)/(TP+FP)\n",
    "        p2 = FN.astype(float)/(FN+TN)\n",
    "        pp = (TP+FP).astype(float)/(TP+FP+TN+FN)\n",
    "        tpr = TP.astype(float)/(TP+FN)\n",
    "        fpr = FP.astype(float)/(FP+TN)\n",
    "        cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
    "        cond_entropy[p1*(1-p1)==0] = -((1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2)))[p1*(1-p1)==0]\n",
    "        cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
    "        cond_entropy[p1*(1-p1)*p2*(1-p2)==0] = 0\n",
    "        select = np.argsort(cond_entropy[supp_select])[::-1][-N:]\n",
    "        self.rules = [rules[i] for i in supp_select[select]]\n",
    "        self.RMatrix = np.array(Z[:,supp_select[select]])\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(self.rules)))\n",
    "\n",
    "    def set_parameters(self, a1=100,b1=10,a2=100,b2=10,al=None,bl=None):\n",
    "        # input al and bl are lists\n",
    "        # a1/(a1 + b1) and a2/(a2 + b2) should be a  bigger than 0.5. They correspond to alpha_+, beta_+ and alpha_- and beta_- from the paper. In fact, these four values play an important role and it needs to be tuned while following the a1/(a1 + b1) >0.5 and a2/(a2 + b2) >0.5 principle\n",
    "        self.alpha_1 = a1\n",
    "        self.beta_1 = b1\n",
    "        self.alpha_2 = a2\n",
    "        self.beta_2 = b2\n",
    "        if al ==None or bl==None or len(al)!=self.maxlen or len(bl)!=self.maxlen:\n",
    "            print('No or wrong input for alpha_l and beta_l. The model will use default parameters!')\n",
    "            self.C = [1.0/self.maxlen for i in range(self.maxlen)]\n",
    "            self.C.insert(0,-1)\n",
    "            self.alpha_l = [10 for i in range(self.maxlen+1)]\n",
    "            self.beta_l= [10*self.patternSpace[i]/self.C[i] for i in range(self.maxlen+1)]\n",
    "        else:\n",
    "            self.alpha_l=[1] + list(al)\n",
    "            self.beta_l = [1] + list(bl)\n",
    "\n",
    "    def fit(self, Niteration = 300, Nchain = 1, q = 0.1, init = [], keep_most_accurate_model = True,print_message=True):\n",
    "        # print('Searching for an optimal solution...')\n",
    "        start_time = time.time()\n",
    "        nRules = len(self.rules)\n",
    "        self.rules_len = [len(rule) for rule in self.rules]\n",
    "        maps = defaultdict(list)\n",
    "        T0 = 1000\n",
    "        split = 0.7*Niteration\n",
    "        acc = {}\n",
    "        most_accurate_model = defaultdict(list)\n",
    "        for chain in range(Nchain):\n",
    "            # initialize with a random pattern set\n",
    "            if init !=[]:\n",
    "                rules_curr = init.copy()\n",
    "            else:\n",
    "                N = sample(range(1,min(8,nRules),1),1)[0]\n",
    "                rules_curr = sample(range(nRules),N)\n",
    "            rules_curr_norm = self.normalize(rules_curr)\n",
    "            pt_curr = -100000000000\n",
    "            maps[chain].append([-1,[pt_curr/3,pt_curr/3,pt_curr/3],rules_curr,[self.rules[i] for i in rules_curr]])\n",
    "            acc[chain] = 0\n",
    "            for iter in range(Niteration):\n",
    "                if iter>=split:\n",
    "                    p = np.array(range(1+len(maps[chain])))\n",
    "                    p = np.array(list(accumulate(p)))\n",
    "                    p = p/p[-1]\n",
    "                    index = find_lt(p,random())\n",
    "                    rules_curr = maps[chain][index][2].copy()\n",
    "                    rules_curr_norm = maps[chain][index][2].copy()\n",
    "                rules_new, rules_norm = self.propose(rules_curr.copy(), rules_curr_norm.copy(),q)\n",
    "                cfmatrix,prob =  self.compute_prob(rules_new)\n",
    "                T = T0**(1 - iter/Niteration)\n",
    "                pt_new = sum(prob)\n",
    "                alpha = np.exp(float(pt_new -pt_curr)/T)\n",
    "                \n",
    "                if (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)> acc[chain]:\n",
    "                    most_accurate_model[chain] = rules_new[:]\n",
    "                    acc[chain] = (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)\n",
    "                    if print_message:\n",
    "                        print('found a more accurate model: accuracy = {}'.format(acc[chain]))\n",
    "                        \n",
    "                if pt_new > sum(maps[chain][-1][1]):\n",
    "                    maps[chain].append([iter,prob,rules_new,[self.rules[i] for i in rules_new]])\n",
    "                    if print_message:\n",
    "                        print('\\n** chain = {}, max at iter = {} ** \\n accuracy = {}, TP = {},FP = {}, TN = {}, FN = {}\\n pt_new is {}, prior_ChsRules={}, likelihood_1 = {}, likelihood_2 = {}\\n '.format(chain, iter,(cfmatrix[0]+cfmatrix[2]+0.0)/len(self.Y),cfmatrix[0],cfmatrix[1],cfmatrix[2],cfmatrix[3],sum(prob), prob[0], prob[1], prob[2]))\n",
    "                        # print '\\n** chain = {}, max at iter = {} ** \\n obj = {}, prior = {}, llh = {} '.format(chain, iter,prior+llh,prior,llh)\n",
    "                        self.print_rules(rules_new)\n",
    "                        print(rules_new)\n",
    "                if random() <= alpha:\n",
    "                    rules_curr_norm,rules_curr,pt_curr = rules_norm.copy(),rules_new.copy(),pt_new\n",
    "        # print('\\tTook %0.3fs to generate an optimal rule set' % (time.time() - start_time))\n",
    "        if keep_most_accurate_model:\n",
    "            acc_list = [acc[chain] for chain in range(Nchain)]\n",
    "            index = acc_list.index(max(acc_list))\n",
    "            return [self.rules[i] for i in most_accurate_model[index]] \n",
    "        else:\n",
    "            pt_max = [sum(maps[chain][-1][1]) for chain in range(Nchain)]\n",
    "            index = pt_max.index(max(pt_max))\n",
    "            return maps[index][-1][3]\n",
    "\n",
    "    def propose(self, rules_curr,rules_norm,q):\n",
    "        nRules = len(self.rules)\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules_curr],axis = 1)>0).astype(int)\n",
    "        incorr = np.where(self.Y!=Yhat)[0]\n",
    "        N = len(rules_curr)\n",
    "        if len(incorr)==0:\n",
    "            clean = True\n",
    "            move = ['clean']\n",
    "            # it means the BOA correctly classified all points but there could be redundant patterns, so cleaning is needed\n",
    "        else:\n",
    "            clean = False\n",
    "            ex = sample(incorr.tolist(),1)[0]\n",
    "            t = random.random()\n",
    "            if self.Y[ex]==1 or N==1:\n",
    "                if t<1.0/2 or N==1:\n",
    "                    move = ['add']       # action: add\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "            else:\n",
    "                if t<1.0/2:\n",
    "                    move = ['cut']       # action: cut\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "        if move[0]=='cut':\n",
    "            \"\"\" cut \"\"\"\n",
    "            if random.random()<q:\n",
    "                candidate = list(set(np.where(self.RMatrix[ex,:]==1)[0]).intersection(rules_curr))\n",
    "                if len(candidate)==0:\n",
    "                    candidate = rules_curr\n",
    "                cut_rule = sample(candidate,1)[0]\n",
    "            else:\n",
    "                p = []\n",
    "                all_sum = np.sum(self.RMatrix[:,rules_curr],axis = 1)\n",
    "                for index,rule in enumerate(rules_curr):\n",
    "                    Yhat= ((all_sum - np.array(self.RMatrix[:,rule]))>0).astype(int)\n",
    "                    TP,FP,TN,FN  = getConfusion(Yhat,self.Y)\n",
    "                    p.append(TP.astype(float)/(TP+FP+1))\n",
    "                    # p.append(log_betabin(TP,TP+FP,self.alpha_1,self.beta_1) + log_betabin(FN,FN+TN,self.alpha_2,self.beta_2))\n",
    "                p = [x - min(p) for x in p]\n",
    "                p = np.exp(p)\n",
    "                p = np.insert(p,0,0)\n",
    "                p = np.array(list(accumulate(p)))\n",
    "                if p[-1]==0:\n",
    "                    index = sample(range(len(rules_curr)),1)[0]\n",
    "                else:\n",
    "                    p = p/p[-1]\n",
    "                index = find_lt(p,random())\n",
    "                cut_rule = rules_curr[index]\n",
    "            rules_curr.remove(cut_rule)\n",
    "            rules_norm = self.normalize(rules_curr)\n",
    "            move.remove('cut')\n",
    "            \n",
    "        if len(move)>0 and move[0]=='add':\n",
    "            \"\"\" add \"\"\"\n",
    "            if random()<q:\n",
    "                add_rule = sample(range(nRules),1)[0]\n",
    "            else: \n",
    "                Yhat_neg_index = list(np.where(np.sum(self.RMatrix[:,rules_curr],axis = 1)<1)[0])\n",
    "                mat = np.multiply(self.RMatrix[Yhat_neg_index,:].transpose(),self.Y[Yhat_neg_index])\n",
    "                # TP = np.array(np.sum(mat,axis = 0).tolist()[0])\n",
    "                TP = np.sum(mat,axis = 1)\n",
    "                FP = np.array((np.sum(self.RMatrix[Yhat_neg_index,:],axis = 0) - TP))\n",
    "                TN = np.sum(self.Y[Yhat_neg_index]==0)-FP\n",
    "                FN = sum(self.Y[Yhat_neg_index]) - TP\n",
    "                p = (TP.astype(float)/(TP+FP+1))\n",
    "                p[rules_curr]=0\n",
    "                add_rule = sample(np.where(p==max(p))[0].tolist(),1)[0]\n",
    "            if add_rule not in rules_curr:\n",
    "                rules_curr.append(add_rule)\n",
    "                rules_norm = self.normalize(rules_curr)\n",
    "\n",
    "        if len(move)>0 and move[0]=='clean':\n",
    "            remove = []\n",
    "            for i,rule in enumerate(rules_norm):\n",
    "                Yhat = (np.sum(self.RMatrix[:,[rule for j,rule in enumerate(rules_norm) if (j!=i and j not in remove)]],axis = 1)>0).astype(int)\n",
    "                TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "                if TP+FP==0:\n",
    "                    remove.append(i)\n",
    "            for x in remove:\n",
    "                rules_norm.remove(x)\n",
    "            return rules_curr, rules_norm\n",
    "        return rules_curr, rules_norm\n",
    "\n",
    "    def compute_prob(self,rules):\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules],axis = 1)>0).astype(int)\n",
    "        TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "        Kn_count = list(np.bincount([self.rules_len[x] for x in rules], minlength = self.maxlen+1))\n",
    "        prior_ChsRules= sum([log_betabin(Kn_count[i],self.patternSpace[i],self.alpha_l[i],self.beta_l[i]) for i in range(1,len(Kn_count),1)])            \n",
    "        likelihood_1 =  log_betabin(TP,TP+FP,self.alpha_1,self.beta_1)\n",
    "        likelihood_2 = log_betabin(TN,FN+TN,self.alpha_2,self.beta_2)\n",
    "        return [TP,FP,TN,FN],[prior_ChsRules,likelihood_1,likelihood_2]\n",
    "\n",
    "    def normalize_add(self, rules_new, rule_index):\n",
    "        rules = rules_new.copy()\n",
    "        for rule in rules_new:\n",
    "            if set(self.rules[rule]).issubset(self.rules[rule_index]):\n",
    "                return rules_new.copy()\n",
    "            if set(self.rules[rule_index]).issubset(self.rules[rule]):\n",
    "                rules.remove(rule)\n",
    "        rules.append(rule_index)\n",
    "        return rules\n",
    "\n",
    "    def normalize(self, rules_new):\n",
    "        try:\n",
    "            rules_len = [len(self.rules[index]) for index in rules_new]\n",
    "            rules = [rules_new[i] for i in np.argsort(rules_len)[::-1][:len(rules_len)]]\n",
    "            p1 = 0\n",
    "            while p1<len(rules):\n",
    "                for p2 in range(p1+1,len(rules),1):\n",
    "                    if set(self.rules[rules[p2]]).issubset(set(self.rules[rules[p1]])):\n",
    "                        rules.remove(rules[p1])\n",
    "                        p1 -= 1\n",
    "                        break\n",
    "                p1 += 1\n",
    "            return rules\n",
    "        except:\n",
    "            return rules_new.copy()\n",
    "\n",
    "\n",
    "    def print_rules(self, rules_max):\n",
    "        for rule_index in rules_max:\n",
    "            print(self.rules[rule_index])\n",
    "            \n",
    "\n",
    "\"\"\" parameters \"\"\"\n",
    "# The following parameters are recommended to change depending on the size and complexity of the data\n",
    "N = 2000      # number of rules to be used in SA_patternbased and also the output of generate_rules\n",
    "Niteration = 500  # number of iterations in each chain\n",
    "Nchain = 2         # number of chains in the simulated annealing search algorithm\n",
    "\n",
    "supp = 5           # 5% is a generally good number. The higher this supp, the 'larger' a pattern is\n",
    "maxlen = 3         # maxmum length of a pattern\n",
    "\n",
    "# \\rho = alpha/(alpha+beta). Make sure \\rho is close to one when choosing alpha and beta. \n",
    "alpha_1 = 500       # alpha_+\n",
    "beta_1 = 1          # beta_+\n",
    "alpha_2 = 500         # alpha_-\n",
    "beta_2 = 1       # beta_-\n",
    "\n",
    "\"\"\" input file \"\"\"\n",
    "# notice that in the example, X is already binary coded. \n",
    "# Data has to be binary coded and the column name shd have the form: attributename_attributevalue\n",
    "filepathX = 'tictactoe_X.txt' # input file X\n",
    "filepathY = 'tictactoe_Y.txt' # input file Y\n",
    "df = pd.read_csv(filepathX,header=0,sep=\" \")\n",
    "Y = np.loadtxt(open(filepathY,\"rb\"),delimiter=\" \")\n",
    "\n",
    "import random\n",
    "lenY = len(Y)\n",
    "train_index = random.sample(range(lenY), int(0.70 * lenY))\n",
    "test_index = [i for i in range(lenY) if i not in train_index]\n",
    "\n",
    "model = BOA(df.iloc[train_index],Y[train_index])\n",
    "model.generate_rules(supp,maxlen,N)\n",
    "model.set_parameters(alpha_1,beta_1,alpha_2,beta_2,None,None)\n",
    "rules = model.fit(Niteration,Nchain,print_message=True)\n",
    "\n",
    "# test\n",
    "Yhat = predict(rules,df.iloc[test_index])\n",
    "TP,FP,TN,FN = getConfusion(Yhat,Y[test_index])\n",
    "tpr = float(TP)/(TP+FN)\n",
    "fpr = float(FP)/(FP+TN)\n",
    "print('TP = {}, FP = {}, TN = {}, FN = {} \\n accuracy = {}, tpr = {}, fpr = {}'.format(TP,FP,TN,FN, float(TP+TN)/(TP+TN+FP+FN),tpr,fpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTook 3.736s to generate 18980 rules\n",
      "Screening rules using information gain\n",
      "\tTook 0.521s to generate 2000 rules\n",
      "Computing sizes for pattern space ...\n",
      "\tTook 0.000s to compute patternspace\n",
      "No or wrong input for alpha_l and beta_l. The model will use default parameters!\n",
      "found a more accurate model: accuracy = 0.6388059701492538\n",
      "\n",
      "** chain = 0, max at iter = 0 ** \n",
      " accuracy = 0.6388059701492538, TP = 332.0,FP = 130.0, TN = 96.0, FN = 112.0\n",
      " pt_new is -726.8092999631244, prior_ChsRules=-42.45912538780067, likelihood_1 = -378.2122872472819, likelihood_2 = -306.13788732804187\n",
      " \n",
      "['4_X_neg', '9_O_neg', '5_O_neg']\n",
      "['5_O', '6_O_neg', '7_X_neg']\n",
      "['5_O', '8_O_neg']\n",
      "['3_O_neg', '7_O_neg', '5_O_neg']\n",
      "[1635, 427, 1048, 1873]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-73853bcf6a16>:107: RuntimeWarning: divide by zero encountered in log\n",
      "  cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
      "<ipython-input-10-73853bcf6a16>:107: RuntimeWarning: invalid value encountered in multiply\n",
      "  cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
      "<ipython-input-10-73853bcf6a16>:109: RuntimeWarning: divide by zero encountered in log\n",
      "  cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
      "<ipython-input-10-73853bcf6a16>:109: RuntimeWarning: invalid value encountered in multiply\n",
      "  cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
      "<ipython-input-10-73853bcf6a16>:166: RuntimeWarning: overflow encountered in exp\n",
      "  alpha = np.exp(float(pt_new -pt_curr)/T)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable. Did you mean: 'random.random(...)'?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/x/Desktop/Repositories/BOA_model.py\u001b[0m in \u001b[0;36mline 348\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=345'>346</a>\u001b[0m model\u001b[39m.\u001b[39mgenerate_rules(supp,maxlen,N)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=346'>347</a>\u001b[0m model\u001b[39m.\u001b[39mset_parameters(alpha_1,beta_1,alpha_2,beta_2,\u001b[39mNone\u001b[39;00m,\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=347'>348</a>\u001b[0m rules \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(Niteration,Nchain,print_message\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=349'>350</a>\u001b[0m \u001b[39m# test\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=350'>351</a>\u001b[0m Yhat \u001b[39m=\u001b[39m predict(rules,df\u001b[39m.\u001b[39miloc[test_index])\n",
      "\u001b[1;32m/Users/x/Desktop/Repositories/BOA_model.py\u001b[0m in \u001b[0;36mline 181\u001b[0m, in \u001b[0;36mBOA.fit\u001b[0;34m(self, Niteration, Nchain, q, init, keep_most_accurate_model, print_message)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=178'>179</a>\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_rules(rules_new)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=179'>180</a>\u001b[0m                 \u001b[39mprint\u001b[39m(rules_new)\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=180'>181</a>\u001b[0m         \u001b[39mif\u001b[39;00m random() \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m alpha:\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=181'>182</a>\u001b[0m             rules_curr_norm,rules_curr,pt_curr \u001b[39m=\u001b[39m rules_norm\u001b[39m.\u001b[39mcopy(),rules_new\u001b[39m.\u001b[39mcopy(),pt_new\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=182'>183</a>\u001b[0m \u001b[39m# print('\\tTook %0.3fs to generate an optimal rule set' % (time.time() - start_time))\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable. Did you mean: 'random.random(...)'?"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# from fim import fpgrowth,fim --> this package is difficult to install. So the rule miner can be replaced by a random forest\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import chain, combinations\n",
    "import itertools\n",
    "from numpy.random import random\n",
    "from random import sample\n",
    "import time\n",
    "import operator\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csc_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "class BOA(object):\n",
    "    def __init__(self, binary_data,Y):\n",
    "        self.df = binary_data  \n",
    "        self.Y = Y\n",
    "        self.attributeLevelNum = defaultdict(int) \n",
    "        self.attributeNames = []\n",
    "        for i,name in enumerate(binary_data.columns):\n",
    "          attribute = name.split('_')[0]\n",
    "          self.attributeLevelNum[attribute] += 1\n",
    "          self.attributeNames.append(attribute)\n",
    "        self.attributeNames = list(set(self.attributeNames))\n",
    "        \n",
    "\n",
    "    def getPatternSpace(self):\n",
    "        print('Computing sizes for pattern space ...')\n",
    "        start_time = time.time()\n",
    "        \"\"\" compute the rule space from the levels in each attribute \"\"\"\n",
    "        for item in self.attributeNames:\n",
    "            self.attributeLevelNum[item+'_neg'] = self.attributeLevelNum[item]\n",
    "        self.patternSpace = np.zeros(self.maxlen+1)\n",
    "        tmp = [ item + '_neg' for item in self.attributeNames]\n",
    "        self.attributeNames.extend(tmp)\n",
    "        for k in range(1,self.maxlen+1,1):\n",
    "            for subset in combinations(self.attributeNames,k):\n",
    "                tmp = 1\n",
    "                for i in subset:\n",
    "                    tmp = tmp * self.attributeLevelNum[i]\n",
    "                self.patternSpace[k] = self.patternSpace[k] + tmp\n",
    "        print('\\tTook %0.3fs to compute patternspace' % (time.time() - start_time))               \n",
    "\n",
    "# This function generates rules that satisfy supp and maxlen using fpgrowth, then it selects the top N rules that make data have the biggest decrease in entropy\n",
    "# there are two ways to generate rules. fpgrowth can handle cases where the maxlen is small. If maxlen<=3, fpgrowth can generates rules much faster than randomforest. \n",
    "# If maxlen is big, fpgrowh tends to generate too many rules that overflow the memories. \n",
    "    def generate_rules(self,supp,maxlen,N, method = 'randomforest'):\n",
    "        self.maxlen = maxlen\n",
    "        self.supp = supp\n",
    "        df = 1-self.df #df has negative associations\n",
    "        df.columns = [name.strip() + '_neg' for name in self.df.columns]\n",
    "        df = pd.concat([self.df,df],axis = 1)\n",
    "#         if method =='fpgrowth' and maxlen<=3:\n",
    "#             itemMatrix = [[item for item in df.columns if row[item] ==1] for i,row in df.iterrows() ]  \n",
    "#             pindex = np.where(self.Y==1)[0]\n",
    "#             nindex = np.where(self.Y!=1)[0]\n",
    "#             print('Generating rules using fpgrowth')\n",
    "#             start_time = time.time()\n",
    "#             rules= fpgrowth([itemMatrix[i] for i in pindex],supp = supp,zmin = 1,zmax = maxlen)\n",
    "#             rules = [tuple(np.sort(rule[0])) for rule in rules]\n",
    "#             rules = list(set(rules))\n",
    "#             start_time = time.time()\n",
    "#             print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "#         else:\n",
    "        rules = []\n",
    "        start_time = time.time()\n",
    "        for length in range(1,maxlen+1,1):\n",
    "            n_estimators = min(pow(df.shape[1],length),4000)\n",
    "            clf = RandomForestClassifier(n_estimators = n_estimators,max_depth = length)\n",
    "            clf.fit(self.df,self.Y)\n",
    "            for n in range(n_estimators):\n",
    "                rules.extend(extract_rules(clf.estimators_[n],df.columns))\n",
    "        rules = [list(x) for x in set(tuple(x) for x in rules)]\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "        self.screen_rules(rules,df,N) # select the top N rules using secondary criteria, information gain\n",
    "        self.getPatternSpace()\n",
    "\n",
    "    def screen_rules(self,rules,df,N):\n",
    "        print('Screening rules using information gain')\n",
    "        start_time = time.time()\n",
    "        itemInd = {}\n",
    "        for i,name in enumerate(df.columns):\n",
    "            itemInd[name] = i\n",
    "        indices = np.array(list(itertools.chain.from_iterable([[itemInd[x] for x in rule] for rule in rules])))\n",
    "        len_rules = [len(rule) for rule in rules]\n",
    "        indptr =list(accumulate(len_rules))\n",
    "        indptr.insert(0,0)\n",
    "        indptr = np.array(indptr)\n",
    "        data = np.ones(len(indices))\n",
    "        ruleMatrix = csc_matrix((data,indices,indptr),shape = (len(df.columns),len(rules)))\n",
    "        mat = np.matrix(df) * ruleMatrix\n",
    "        lenMatrix = np.matrix([len_rules for i in range(df.shape[0])])\n",
    "        Z =  (mat ==lenMatrix).astype(int)\n",
    "        Zpos = [Z[i] for i in np.where(self.Y>0)][0]\n",
    "        TP = np.array(np.sum(Zpos,axis=0).tolist()[0])\n",
    "        supp_select = np.where(TP>=self.supp*sum(self.Y)/100)[0]\n",
    "        FP = np.array(np.sum(Z,axis = 0))[0] - TP\n",
    "        TN = len(self.Y) - np.sum(self.Y) - FP\n",
    "        FN = np.sum(self.Y) - TP\n",
    "        p1 = TP.astype(float)/(TP+FP)\n",
    "        p2 = FN.astype(float)/(FN+TN)\n",
    "        pp = (TP+FP).astype(float)/(TP+FP+TN+FN)\n",
    "        tpr = TP.astype(float)/(TP+FN)\n",
    "        fpr = FP.astype(float)/(FP+TN)\n",
    "        cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
    "        cond_entropy[p1*(1-p1)==0] = -((1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2)))[p1*(1-p1)==0]\n",
    "        cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
    "        cond_entropy[p1*(1-p1)*p2*(1-p2)==0] = 0\n",
    "        select = np.argsort(cond_entropy[supp_select])[::-1][-N:]\n",
    "        self.rules = [rules[i] for i in supp_select[select]]\n",
    "        self.RMatrix = np.array(Z[:,supp_select[select]])\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(self.rules)))\n",
    "\n",
    "    def set_parameters(self, a1=100,b1=10,a2=100,b2=10,al=None,bl=None):\n",
    "        # input al and bl are lists\n",
    "        # a1/(a1 + b1) and a2/(a2 + b2) should be a  bigger than 0.5. They correspond to alpha_+, beta_+ and alpha_- and beta_- from the paper. In fact, these four values play an important role and it needs to be tuned while following the a1/(a1 + b1) >0.5 and a2/(a2 + b2) >0.5 principle\n",
    "        self.alpha_1 = a1\n",
    "        self.beta_1 = b1\n",
    "        self.alpha_2 = a2\n",
    "        self.beta_2 = b2\n",
    "        if al ==None or bl==None or len(al)!=self.maxlen or len(bl)!=self.maxlen:\n",
    "            print('No or wrong input for alpha_l and beta_l. The model will use default parameters!')\n",
    "            self.C = [1.0/self.maxlen for i in range(self.maxlen)]\n",
    "            self.C.insert(0,-1)\n",
    "            self.alpha_l = [10 for i in range(self.maxlen+1)]\n",
    "            self.beta_l= [10*self.patternSpace[i]/self.C[i] for i in range(self.maxlen+1)]\n",
    "        else:\n",
    "            self.alpha_l=[1] + list(al)\n",
    "            self.beta_l = [1] + list(bl)\n",
    "\n",
    "    def fit(self, Niteration = 300, Nchain = 1, q = 0.1, init = [], keep_most_accurate_model = True,print_message=True):\n",
    "        # print('Searching for an optimal solution...')\n",
    "        start_time = time.time()\n",
    "        nRules = len(self.rules)\n",
    "        self.rules_len = [len(rule) for rule in self.rules]\n",
    "        maps = defaultdict(list)\n",
    "        T0 = 1000\n",
    "        split = 0.7*Niteration\n",
    "        acc = {}\n",
    "        most_accurate_model = defaultdict(list)\n",
    "        for chain in range(Nchain):\n",
    "            # initialize with a random pattern set\n",
    "            if init !=[]:\n",
    "                rules_curr = init.copy()\n",
    "            else:\n",
    "                N = sample(range(1,min(8,nRules),1),1)[0]\n",
    "                rules_curr = sample(range(nRules),N)\n",
    "            rules_curr_norm = self.normalize(rules_curr)\n",
    "            pt_curr = -100000000000\n",
    "            maps[chain].append([-1,[pt_curr/3,pt_curr/3,pt_curr/3],rules_curr,[self.rules[i] for i in rules_curr]])\n",
    "            acc[chain] = 0\n",
    "            for iter in range(Niteration):\n",
    "                if iter>=split:\n",
    "                    p = np.array(range(1+len(maps[chain])))\n",
    "                    p = np.array(list(accumulate(p)))\n",
    "                    p = p/p[-1]\n",
    "                    index = find_lt(p,random())\n",
    "                    rules_curr = maps[chain][index][2].copy()\n",
    "                    rules_curr_norm = maps[chain][index][2].copy()\n",
    "                rules_new, rules_norm = self.propose(rules_curr.copy(), rules_curr_norm.copy(),q)\n",
    "                cfmatrix,prob =  self.compute_prob(rules_new)\n",
    "                T = T0**(1 - iter/Niteration)\n",
    "                pt_new = sum(prob)\n",
    "                alpha = np.exp(float(pt_new -pt_curr)/T)\n",
    "                \n",
    "                if (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)> acc[chain]:\n",
    "                    most_accurate_model[chain] = rules_new[:]\n",
    "                    acc[chain] = (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)\n",
    "                    if print_message:\n",
    "                        print('found a more accurate model: accuracy = {}'.format(acc[chain]))\n",
    "                        \n",
    "                if pt_new > sum(maps[chain][-1][1]):\n",
    "                    maps[chain].append([iter,prob,rules_new,[self.rules[i] for i in rules_new]])\n",
    "                    if print_message:\n",
    "                        print('\\n** chain = {}, max at iter = {} ** \\n accuracy = {}, TP = {},FP = {}, TN = {}, FN = {}\\n pt_new is {}, prior_ChsRules={}, likelihood_1 = {}, likelihood_2 = {}\\n '.format(chain, iter,(cfmatrix[0]+cfmatrix[2]+0.0)/len(self.Y),cfmatrix[0],cfmatrix[1],cfmatrix[2],cfmatrix[3],sum(prob), prob[0], prob[1], prob[2]))\n",
    "                        # print '\\n** chain = {}, max at iter = {} ** \\n obj = {}, prior = {}, llh = {} '.format(chain, iter,prior+llh,prior,llh)\n",
    "                        self.print_rules(rules_new)\n",
    "                        print(rules_new)\n",
    "                if random() <= alpha:\n",
    "                    rules_curr_norm,rules_curr,pt_curr = rules_norm.copy(),rules_new.copy(),pt_new\n",
    "        # print('\\tTook %0.3fs to generate an optimal rule set' % (time.time() - start_time))\n",
    "        if keep_most_accurate_model:\n",
    "            acc_list = [acc[chain] for chain in range(Nchain)]\n",
    "            index = acc_list.index(max(acc_list))\n",
    "            return [self.rules[i] for i in most_accurate_model[index]] \n",
    "        else:\n",
    "            pt_max = [sum(maps[chain][-1][1]) for chain in range(Nchain)]\n",
    "            index = pt_max.index(max(pt_max))\n",
    "            return maps[index][-1][3]\n",
    "\n",
    "    def propose(self, rules_curr,rules_norm,q):\n",
    "        nRules = len(self.rules)\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules_curr],axis = 1)>0).astype(int)\n",
    "        incorr = np.where(self.Y!=Yhat)[0]\n",
    "        N = len(rules_curr)\n",
    "        if len(incorr)==0:\n",
    "            clean = True\n",
    "            move = ['clean']\n",
    "            # it means the BOA correctly classified all points but there could be redundant patterns, so cleaning is needed\n",
    "        else:\n",
    "            clean = False\n",
    "            ex = sample(incorr.tolist(),1)[0]\n",
    "            t = random.random()\n",
    "            if self.Y[ex]==1 or N==1:\n",
    "                if t<1.0/2 or N==1:\n",
    "                    move = ['add']       # action: add\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "            else:\n",
    "                if t<1.0/2:\n",
    "                    move = ['cut']       # action: cut\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "        if move[0]=='cut':\n",
    "            \"\"\" cut \"\"\"\n",
    "            if random.random()<q:\n",
    "                candidate = list(set(np.where(self.RMatrix[ex,:]==1)[0]).intersection(rules_curr))\n",
    "                if len(candidate)==0:\n",
    "                    candidate = rules_curr\n",
    "                cut_rule = sample(candidate,1)[0]\n",
    "            else:\n",
    "                p = []\n",
    "                all_sum = np.sum(self.RMatrix[:,rules_curr],axis = 1)\n",
    "                for index,rule in enumerate(rules_curr):\n",
    "                    Yhat= ((all_sum - np.array(self.RMatrix[:,rule]))>0).astype(int)\n",
    "                    TP,FP,TN,FN  = getConfusion(Yhat,self.Y)\n",
    "                    p.append(TP.astype(float)/(TP+FP+1))\n",
    "                    # p.append(log_betabin(TP,TP+FP,self.alpha_1,self.beta_1) + log_betabin(FN,FN+TN,self.alpha_2,self.beta_2))\n",
    "                p = [x - min(p) for x in p]\n",
    "                p = np.exp(p)\n",
    "                p = np.insert(p,0,0)\n",
    "                p = np.array(list(accumulate(p)))\n",
    "                if p[-1]==0:\n",
    "                    index = sample(range(len(rules_curr)),1)[0]\n",
    "                else:\n",
    "                    p = p/p[-1]\n",
    "                index = find_lt(p,random())\n",
    "                cut_rule = rules_curr[index]\n",
    "            rules_curr.remove(cut_rule)\n",
    "            rules_norm = self.normalize(rules_curr)\n",
    "            move.remove('cut')\n",
    "            \n",
    "        if len(move)>0 and move[0]=='add':\n",
    "            \"\"\" add \"\"\"\n",
    "            if random.random()<q:\n",
    "                add_rule = sample(range(nRules),1)[0]\n",
    "            else: \n",
    "                Yhat_neg_index = list(np.where(np.sum(self.RMatrix[:,rules_curr],axis = 1)<1)[0])\n",
    "                mat = np.multiply(self.RMatrix[Yhat_neg_index,:].transpose(),self.Y[Yhat_neg_index])\n",
    "                # TP = np.array(np.sum(mat,axis = 0).tolist()[0])\n",
    "                TP = np.sum(mat,axis = 1)\n",
    "                FP = np.array((np.sum(self.RMatrix[Yhat_neg_index,:],axis = 0) - TP))\n",
    "                TN = np.sum(self.Y[Yhat_neg_index]==0)-FP\n",
    "                FN = sum(self.Y[Yhat_neg_index]) - TP\n",
    "                p = (TP.astype(float)/(TP+FP+1))\n",
    "                p[rules_curr]=0\n",
    "                add_rule = sample(np.where(p==max(p))[0].tolist(),1)[0]\n",
    "            if add_rule not in rules_curr:\n",
    "                rules_curr.append(add_rule)\n",
    "                rules_norm = self.normalize(rules_curr)\n",
    "\n",
    "        if len(move)>0 and move[0]=='clean':\n",
    "            remove = []\n",
    "            for i,rule in enumerate(rules_norm):\n",
    "                Yhat = (np.sum(self.RMatrix[:,[rule for j,rule in enumerate(rules_norm) if (j!=i and j not in remove)]],axis = 1)>0).astype(int)\n",
    "                TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "                if TP+FP==0:\n",
    "                    remove.append(i)\n",
    "            for x in remove:\n",
    "                rules_norm.remove(x)\n",
    "            return rules_curr, rules_norm\n",
    "        return rules_curr, rules_norm\n",
    "\n",
    "    def compute_prob(self,rules):\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules],axis = 1)>0).astype(int)\n",
    "        TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "        Kn_count = list(np.bincount([self.rules_len[x] for x in rules], minlength = self.maxlen+1))\n",
    "        prior_ChsRules= sum([log_betabin(Kn_count[i],self.patternSpace[i],self.alpha_l[i],self.beta_l[i]) for i in range(1,len(Kn_count),1)])            \n",
    "        likelihood_1 =  log_betabin(TP,TP+FP,self.alpha_1,self.beta_1)\n",
    "        likelihood_2 = log_betabin(TN,FN+TN,self.alpha_2,self.beta_2)\n",
    "        return [TP,FP,TN,FN],[prior_ChsRules,likelihood_1,likelihood_2]\n",
    "\n",
    "    def normalize_add(self, rules_new, rule_index):\n",
    "        rules = rules_new.copy()\n",
    "        for rule in rules_new:\n",
    "            if set(self.rules[rule]).issubset(self.rules[rule_index]):\n",
    "                return rules_new.copy()\n",
    "            if set(self.rules[rule_index]).issubset(self.rules[rule]):\n",
    "                rules.remove(rule)\n",
    "        rules.append(rule_index)\n",
    "        return rules\n",
    "\n",
    "    def normalize(self, rules_new):\n",
    "        try:\n",
    "            rules_len = [len(self.rules[index]) for index in rules_new]\n",
    "            rules = [rules_new[i] for i in np.argsort(rules_len)[::-1][:len(rules_len)]]\n",
    "            p1 = 0\n",
    "            while p1<len(rules):\n",
    "                for p2 in range(p1+1,len(rules),1):\n",
    "                    if set(self.rules[rules[p2]]).issubset(set(self.rules[rules[p1]])):\n",
    "                        rules.remove(rules[p1])\n",
    "                        p1 -= 1\n",
    "                        break\n",
    "                p1 += 1\n",
    "            return rules\n",
    "        except:\n",
    "            return rules_new.copy()\n",
    "\n",
    "\n",
    "    def print_rules(self, rules_max):\n",
    "        for rule_index in rules_max:\n",
    "            print(self.rules[rule_index])\n",
    "            \n",
    "\n",
    "\"\"\" parameters \"\"\"\n",
    "# The following parameters are recommended to change depending on the size and complexity of the data\n",
    "N = 2000      # number of rules to be used in SA_patternbased and also the output of generate_rules\n",
    "Niteration = 500  # number of iterations in each chain\n",
    "Nchain = 2         # number of chains in the simulated annealing search algorithm\n",
    "\n",
    "supp = 5           # 5% is a generally good number. The higher this supp, the 'larger' a pattern is\n",
    "maxlen = 3         # maxmum length of a pattern\n",
    "\n",
    "# \\rho = alpha/(alpha+beta). Make sure \\rho is close to one when choosing alpha and beta. \n",
    "alpha_1 = 500       # alpha_+\n",
    "beta_1 = 1          # beta_+\n",
    "alpha_2 = 500         # alpha_-\n",
    "beta_2 = 1       # beta_-\n",
    "\n",
    "\"\"\" input file \"\"\"\n",
    "# notice that in the example, X is already binary coded. \n",
    "# Data has to be binary coded and the column name shd have the form: attributename_attributevalue\n",
    "filepathX = 'tictactoe_X.txt' # input file X\n",
    "filepathY = 'tictactoe_Y.txt' # input file Y\n",
    "df = pd.read_csv(filepathX,header=0,sep=\" \")\n",
    "Y = np.loadtxt(open(filepathY,\"rb\"),delimiter=\" \")\n",
    "\n",
    "import random\n",
    "lenY = len(Y)\n",
    "train_index = random.sample(range(lenY), int(0.70 * lenY))\n",
    "test_index = [i for i in range(lenY) if i not in train_index]\n",
    "\n",
    "model = BOA(df.iloc[train_index],Y[train_index])\n",
    "model.generate_rules(supp,maxlen,N)\n",
    "model.set_parameters(alpha_1,beta_1,alpha_2,beta_2,None,None)\n",
    "rules = model.fit(Niteration,Nchain,print_message=True)\n",
    "\n",
    "# test\n",
    "Yhat = predict(rules,df.iloc[test_index])\n",
    "TP,FP,TN,FN = getConfusion(Yhat,Y[test_index])\n",
    "tpr = float(TP)/(TP+FN)\n",
    "fpr = float(FP)/(FP+TN)\n",
    "print('TP = {}, FP = {}, TN = {}, FN = {} \\n accuracy = {}, tpr = {}, fpr = {}'.format(TP,FP,TN,FN, float(TP+TN)/(TP+TN+FP+FN),tpr,fpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTook 3.670s to generate 18766 rules\n",
      "Screening rules using information gain\n",
      "\tTook 0.477s to generate 2000 rules\n",
      "Computing sizes for pattern space ...\n",
      "\tTook 0.000s to compute patternspace\n",
      "No or wrong input for alpha_l and beta_l. The model will use default parameters!\n",
      "found a more accurate model: accuracy = 0.6925373134328359\n",
      "\n",
      "** chain = 0, max at iter = 0 ** \n",
      " accuracy = 0.6925373134328359, TP = 339.0,FP = 113.0, TN = 125.0, FN = 93.0\n",
      " pt_new is -682.1402548686824, prior_ChsRules=-64.12358237369153, likelihood_1 = -344.13170495976647, likelihood_2 = -273.8849675352244\n",
      " \n",
      "['5_X', '9_B', '1_O_neg']\n",
      "['8_B_neg', '1_O_neg', '5_O_neg']\n",
      "['8_O', '7_O_neg', '5_X']\n",
      "['5_O_neg', '7_B_neg', '9_O_neg']\n",
      "['9_O_neg', '3_O_neg', '8_X_neg']\n",
      "['5_O_neg', '1_O_neg']\n",
      "[1842, 729, 1052, 1585, 345, 1801]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-73853bcf6a16>:107: RuntimeWarning: divide by zero encountered in log\n",
      "  cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
      "<ipython-input-11-73853bcf6a16>:107: RuntimeWarning: invalid value encountered in multiply\n",
      "  cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
      "<ipython-input-11-73853bcf6a16>:109: RuntimeWarning: divide by zero encountered in log\n",
      "  cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
      "<ipython-input-11-73853bcf6a16>:109: RuntimeWarning: invalid value encountered in multiply\n",
      "  cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
      "<ipython-input-11-73853bcf6a16>:166: RuntimeWarning: overflow encountered in exp\n",
      "  alpha = np.exp(float(pt_new -pt_curr)/T)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable. Did you mean: 'random.random(...)'?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/x/Desktop/Repositories/BOA_model.py\u001b[0m in \u001b[0;36mline 348\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=345'>346</a>\u001b[0m model\u001b[39m.\u001b[39mgenerate_rules(supp,maxlen,N)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=346'>347</a>\u001b[0m model\u001b[39m.\u001b[39mset_parameters(alpha_1,beta_1,alpha_2,beta_2,\u001b[39mNone\u001b[39;00m,\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=347'>348</a>\u001b[0m rules \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(Niteration,Nchain,print_message\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=349'>350</a>\u001b[0m \u001b[39m# test\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=350'>351</a>\u001b[0m Yhat \u001b[39m=\u001b[39m predict(rules,df\u001b[39m.\u001b[39miloc[test_index])\n",
      "\u001b[1;32m/Users/x/Desktop/Repositories/BOA_model.py\u001b[0m in \u001b[0;36mline 181\u001b[0m, in \u001b[0;36mBOA.fit\u001b[0;34m(self, Niteration, Nchain, q, init, keep_most_accurate_model, print_message)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=178'>179</a>\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_rules(rules_new)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=179'>180</a>\u001b[0m                 \u001b[39mprint\u001b[39m(rules_new)\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=180'>181</a>\u001b[0m         \u001b[39mif\u001b[39;00m random() \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m alpha:\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=181'>182</a>\u001b[0m             rules_curr_norm,rules_curr,pt_curr \u001b[39m=\u001b[39m rules_norm\u001b[39m.\u001b[39mcopy(),rules_new\u001b[39m.\u001b[39mcopy(),pt_new\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=182'>183</a>\u001b[0m \u001b[39m# print('\\tTook %0.3fs to generate an optimal rule set' % (time.time() - start_time))\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable. Did you mean: 'random.random(...)'?"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# from fim import fpgrowth,fim --> this package is difficult to install. So the rule miner can be replaced by a random forest\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import chain, combinations\n",
    "import itertools\n",
    "from numpy.random import random\n",
    "from random import sample\n",
    "import time\n",
    "import operator\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csc_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "class BOA(object):\n",
    "    def __init__(self, binary_data,Y):\n",
    "        self.df = binary_data  \n",
    "        self.Y = Y\n",
    "        self.attributeLevelNum = defaultdict(int) \n",
    "        self.attributeNames = []\n",
    "        for i,name in enumerate(binary_data.columns):\n",
    "          attribute = name.split('_')[0]\n",
    "          self.attributeLevelNum[attribute] += 1\n",
    "          self.attributeNames.append(attribute)\n",
    "        self.attributeNames = list(set(self.attributeNames))\n",
    "        \n",
    "\n",
    "    def getPatternSpace(self):\n",
    "        print('Computing sizes for pattern space ...')\n",
    "        start_time = time.time()\n",
    "        \"\"\" compute the rule space from the levels in each attribute \"\"\"\n",
    "        for item in self.attributeNames:\n",
    "            self.attributeLevelNum[item+'_neg'] = self.attributeLevelNum[item]\n",
    "        self.patternSpace = np.zeros(self.maxlen+1)\n",
    "        tmp = [ item + '_neg' for item in self.attributeNames]\n",
    "        self.attributeNames.extend(tmp)\n",
    "        for k in range(1,self.maxlen+1,1):\n",
    "            for subset in combinations(self.attributeNames,k):\n",
    "                tmp = 1\n",
    "                for i in subset:\n",
    "                    tmp = tmp * self.attributeLevelNum[i]\n",
    "                self.patternSpace[k] = self.patternSpace[k] + tmp\n",
    "        print('\\tTook %0.3fs to compute patternspace' % (time.time() - start_time))               \n",
    "\n",
    "# This function generates rules that satisfy supp and maxlen using fpgrowth, then it selects the top N rules that make data have the biggest decrease in entropy\n",
    "# there are two ways to generate rules. fpgrowth can handle cases where the maxlen is small. If maxlen<=3, fpgrowth can generates rules much faster than randomforest. \n",
    "# If maxlen is big, fpgrowh tends to generate too many rules that overflow the memories. \n",
    "    def generate_rules(self,supp,maxlen,N, method = 'randomforest'):\n",
    "        self.maxlen = maxlen\n",
    "        self.supp = supp\n",
    "        df = 1-self.df #df has negative associations\n",
    "        df.columns = [name.strip() + '_neg' for name in self.df.columns]\n",
    "        df = pd.concat([self.df,df],axis = 1)\n",
    "#         if method =='fpgrowth' and maxlen<=3:\n",
    "#             itemMatrix = [[item for item in df.columns if row[item] ==1] for i,row in df.iterrows() ]  \n",
    "#             pindex = np.where(self.Y==1)[0]\n",
    "#             nindex = np.where(self.Y!=1)[0]\n",
    "#             print('Generating rules using fpgrowth')\n",
    "#             start_time = time.time()\n",
    "#             rules= fpgrowth([itemMatrix[i] for i in pindex],supp = supp,zmin = 1,zmax = maxlen)\n",
    "#             rules = [tuple(np.sort(rule[0])) for rule in rules]\n",
    "#             rules = list(set(rules))\n",
    "#             start_time = time.time()\n",
    "#             print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "#         else:\n",
    "        rules = []\n",
    "        start_time = time.time()\n",
    "        for length in range(1,maxlen+1,1):\n",
    "            n_estimators = min(pow(df.shape[1],length),4000)\n",
    "            clf = RandomForestClassifier(n_estimators = n_estimators,max_depth = length)\n",
    "            clf.fit(self.df,self.Y)\n",
    "            for n in range(n_estimators):\n",
    "                rules.extend(extract_rules(clf.estimators_[n],df.columns))\n",
    "        rules = [list(x) for x in set(tuple(x) for x in rules)]\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "        self.screen_rules(rules,df,N) # select the top N rules using secondary criteria, information gain\n",
    "        self.getPatternSpace()\n",
    "\n",
    "    def screen_rules(self,rules,df,N):\n",
    "        print('Screening rules using information gain')\n",
    "        start_time = time.time()\n",
    "        itemInd = {}\n",
    "        for i,name in enumerate(df.columns):\n",
    "            itemInd[name] = i\n",
    "        indices = np.array(list(itertools.chain.from_iterable([[itemInd[x] for x in rule] for rule in rules])))\n",
    "        len_rules = [len(rule) for rule in rules]\n",
    "        indptr =list(accumulate(len_rules))\n",
    "        indptr.insert(0,0)\n",
    "        indptr = np.array(indptr)\n",
    "        data = np.ones(len(indices))\n",
    "        ruleMatrix = csc_matrix((data,indices,indptr),shape = (len(df.columns),len(rules)))\n",
    "        mat = np.matrix(df) * ruleMatrix\n",
    "        lenMatrix = np.matrix([len_rules for i in range(df.shape[0])])\n",
    "        Z =  (mat ==lenMatrix).astype(int)\n",
    "        Zpos = [Z[i] for i in np.where(self.Y>0)][0]\n",
    "        TP = np.array(np.sum(Zpos,axis=0).tolist()[0])\n",
    "        supp_select = np.where(TP>=self.supp*sum(self.Y)/100)[0]\n",
    "        FP = np.array(np.sum(Z,axis = 0))[0] - TP\n",
    "        TN = len(self.Y) - np.sum(self.Y) - FP\n",
    "        FN = np.sum(self.Y) - TP\n",
    "        p1 = TP.astype(float)/(TP+FP)\n",
    "        p2 = FN.astype(float)/(FN+TN)\n",
    "        pp = (TP+FP).astype(float)/(TP+FP+TN+FN)\n",
    "        tpr = TP.astype(float)/(TP+FN)\n",
    "        fpr = FP.astype(float)/(FP+TN)\n",
    "        cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
    "        cond_entropy[p1*(1-p1)==0] = -((1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2)))[p1*(1-p1)==0]\n",
    "        cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
    "        cond_entropy[p1*(1-p1)*p2*(1-p2)==0] = 0\n",
    "        select = np.argsort(cond_entropy[supp_select])[::-1][-N:]\n",
    "        self.rules = [rules[i] for i in supp_select[select]]\n",
    "        self.RMatrix = np.array(Z[:,supp_select[select]])\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(self.rules)))\n",
    "\n",
    "    def set_parameters(self, a1=100,b1=10,a2=100,b2=10,al=None,bl=None):\n",
    "        # input al and bl are lists\n",
    "        # a1/(a1 + b1) and a2/(a2 + b2) should be a  bigger than 0.5. They correspond to alpha_+, beta_+ and alpha_- and beta_- from the paper. In fact, these four values play an important role and it needs to be tuned while following the a1/(a1 + b1) >0.5 and a2/(a2 + b2) >0.5 principle\n",
    "        self.alpha_1 = a1\n",
    "        self.beta_1 = b1\n",
    "        self.alpha_2 = a2\n",
    "        self.beta_2 = b2\n",
    "        if al ==None or bl==None or len(al)!=self.maxlen or len(bl)!=self.maxlen:\n",
    "            print('No or wrong input for alpha_l and beta_l. The model will use default parameters!')\n",
    "            self.C = [1.0/self.maxlen for i in range(self.maxlen)]\n",
    "            self.C.insert(0,-1)\n",
    "            self.alpha_l = [10 for i in range(self.maxlen+1)]\n",
    "            self.beta_l= [10*self.patternSpace[i]/self.C[i] for i in range(self.maxlen+1)]\n",
    "        else:\n",
    "            self.alpha_l=[1] + list(al)\n",
    "            self.beta_l = [1] + list(bl)\n",
    "\n",
    "    def fit(self, Niteration = 300, Nchain = 1, q = 0.1, init = [], keep_most_accurate_model = True,print_message=True):\n",
    "        # print('Searching for an optimal solution...')\n",
    "        start_time = time.time()\n",
    "        nRules = len(self.rules)\n",
    "        self.rules_len = [len(rule) for rule in self.rules]\n",
    "        maps = defaultdict(list)\n",
    "        T0 = 1000\n",
    "        split = 0.7*Niteration\n",
    "        acc = {}\n",
    "        most_accurate_model = defaultdict(list)\n",
    "        for chain in range(Nchain):\n",
    "            # initialize with a random pattern set\n",
    "            if init !=[]:\n",
    "                rules_curr = init.copy()\n",
    "            else:\n",
    "                N = sample(range(1,min(8,nRules),1),1)[0]\n",
    "                rules_curr = sample(range(nRules),N)\n",
    "            rules_curr_norm = self.normalize(rules_curr)\n",
    "            pt_curr = -100000000000\n",
    "            maps[chain].append([-1,[pt_curr/3,pt_curr/3,pt_curr/3],rules_curr,[self.rules[i] for i in rules_curr]])\n",
    "            acc[chain] = 0\n",
    "            for iter in range(Niteration):\n",
    "                if iter>=split:\n",
    "                    p = np.array(range(1+len(maps[chain])))\n",
    "                    p = np.array(list(accumulate(p)))\n",
    "                    p = p/p[-1]\n",
    "                    index = find_lt(p,random())\n",
    "                    rules_curr = maps[chain][index][2].copy()\n",
    "                    rules_curr_norm = maps[chain][index][2].copy()\n",
    "                rules_new, rules_norm = self.propose(rules_curr.copy(), rules_curr_norm.copy(),q)\n",
    "                cfmatrix,prob =  self.compute_prob(rules_new)\n",
    "                T = T0**(1 - iter/Niteration)\n",
    "                pt_new = sum(prob)\n",
    "                alpha = np.exp(float(pt_new -pt_curr)/T)\n",
    "                \n",
    "                if (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)> acc[chain]:\n",
    "                    most_accurate_model[chain] = rules_new[:]\n",
    "                    acc[chain] = (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)\n",
    "                    if print_message:\n",
    "                        print('found a more accurate model: accuracy = {}'.format(acc[chain]))\n",
    "                        \n",
    "                if pt_new > sum(maps[chain][-1][1]):\n",
    "                    maps[chain].append([iter,prob,rules_new,[self.rules[i] for i in rules_new]])\n",
    "                    if print_message:\n",
    "                        print('\\n** chain = {}, max at iter = {} ** \\n accuracy = {}, TP = {},FP = {}, TN = {}, FN = {}\\n pt_new is {}, prior_ChsRules={}, likelihood_1 = {}, likelihood_2 = {}\\n '.format(chain, iter,(cfmatrix[0]+cfmatrix[2]+0.0)/len(self.Y),cfmatrix[0],cfmatrix[1],cfmatrix[2],cfmatrix[3],sum(prob), prob[0], prob[1], prob[2]))\n",
    "                        # print '\\n** chain = {}, max at iter = {} ** \\n obj = {}, prior = {}, llh = {} '.format(chain, iter,prior+llh,prior,llh)\n",
    "                        self.print_rules(rules_new)\n",
    "                        print(rules_new)\n",
    "                if random() <= alpha:\n",
    "                    rules_curr_norm,rules_curr,pt_curr = rules_norm.copy(),rules_new.copy(),pt_new\n",
    "        # print('\\tTook %0.3fs to generate an optimal rule set' % (time.time() - start_time))\n",
    "        if keep_most_accurate_model:\n",
    "            acc_list = [acc[chain] for chain in range(Nchain)]\n",
    "            index = acc_list.index(max(acc_list))\n",
    "            return [self.rules[i] for i in most_accurate_model[index]] \n",
    "        else:\n",
    "            pt_max = [sum(maps[chain][-1][1]) for chain in range(Nchain)]\n",
    "            index = pt_max.index(max(pt_max))\n",
    "            return maps[index][-1][3]\n",
    "\n",
    "    def propose(self, rules_curr,rules_norm,q):\n",
    "        nRules = len(self.rules)\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules_curr],axis = 1)>0).astype(int)\n",
    "        incorr = np.where(self.Y!=Yhat)[0]\n",
    "        N = len(rules_curr)\n",
    "        if len(incorr)==0:\n",
    "            clean = True\n",
    "            move = ['clean']\n",
    "            # it means the BOA correctly classified all points but there could be redundant patterns, so cleaning is needed\n",
    "        else:\n",
    "            clean = False\n",
    "            ex = sample(incorr.tolist(),1)[0]\n",
    "            t = random.random()\n",
    "            if self.Y[ex]==1 or N==1:\n",
    "                if t<1.0/2 or N==1:\n",
    "                    move = ['add']       # action: add\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "            else:\n",
    "                if t<1.0/2:\n",
    "                    move = ['cut']       # action: cut\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "        if move[0]=='cut':\n",
    "            \"\"\" cut \"\"\"\n",
    "            if random.random()<q:\n",
    "                candidate = list(set(np.where(self.RMatrix[ex,:]==1)[0]).intersection(rules_curr))\n",
    "                if len(candidate)==0:\n",
    "                    candidate = rules_curr\n",
    "                cut_rule = sample(candidate,1)[0]\n",
    "            else:\n",
    "                p = []\n",
    "                all_sum = np.sum(self.RMatrix[:,rules_curr],axis = 1)\n",
    "                for index,rule in enumerate(rules_curr):\n",
    "                    Yhat= ((all_sum - np.array(self.RMatrix[:,rule]))>0).astype(int)\n",
    "                    TP,FP,TN,FN  = getConfusion(Yhat,self.Y)\n",
    "                    p.append(TP.astype(float)/(TP+FP+1))\n",
    "                    # p.append(log_betabin(TP,TP+FP,self.alpha_1,self.beta_1) + log_betabin(FN,FN+TN,self.alpha_2,self.beta_2))\n",
    "                p = [x - min(p) for x in p]\n",
    "                p = np.exp(p)\n",
    "                p = np.insert(p,0,0)\n",
    "                p = np.array(list(accumulate(p)))\n",
    "                if p[-1]==0:\n",
    "                    index = sample(range(len(rules_curr)),1)[0]\n",
    "                else:\n",
    "                    p = p/p[-1]\n",
    "                index = find_lt(p,random())\n",
    "                cut_rule = rules_curr[index]\n",
    "            rules_curr.remove(cut_rule)\n",
    "            rules_norm = self.normalize(rules_curr)\n",
    "            move.remove('cut')\n",
    "            \n",
    "        if len(move)>0 and move[0]=='add':\n",
    "            \"\"\" add \"\"\"\n",
    "            if random.random()<q:\n",
    "                add_rule = sample(range(nRules),1)[0]\n",
    "            else: \n",
    "                Yhat_neg_index = list(np.where(np.sum(self.RMatrix[:,rules_curr],axis = 1)<1)[0])\n",
    "                mat = np.multiply(self.RMatrix[Yhat_neg_index,:].transpose(),self.Y[Yhat_neg_index])\n",
    "                # TP = np.array(np.sum(mat,axis = 0).tolist()[0])\n",
    "                TP = np.sum(mat,axis = 1)\n",
    "                FP = np.array((np.sum(self.RMatrix[Yhat_neg_index,:],axis = 0) - TP))\n",
    "                TN = np.sum(self.Y[Yhat_neg_index]==0)-FP\n",
    "                FN = sum(self.Y[Yhat_neg_index]) - TP\n",
    "                p = (TP.astype(float)/(TP+FP+1))\n",
    "                p[rules_curr]=0\n",
    "                add_rule = sample(np.where(p==max(p))[0].tolist(),1)[0]\n",
    "            if add_rule not in rules_curr:\n",
    "                rules_curr.append(add_rule)\n",
    "                rules_norm = self.normalize(rules_curr)\n",
    "\n",
    "        if len(move)>0 and move[0]=='clean':\n",
    "            remove = []\n",
    "            for i,rule in enumerate(rules_norm):\n",
    "                Yhat = (np.sum(self.RMatrix[:,[rule for j,rule in enumerate(rules_norm) if (j!=i and j not in remove)]],axis = 1)>0).astype(int)\n",
    "                TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "                if TP+FP==0:\n",
    "                    remove.append(i)\n",
    "            for x in remove:\n",
    "                rules_norm.remove(x)\n",
    "            return rules_curr, rules_norm\n",
    "        return rules_curr, rules_norm\n",
    "\n",
    "    def compute_prob(self,rules):\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules],axis = 1)>0).astype(int)\n",
    "        TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "        Kn_count = list(np.bincount([self.rules_len[x] for x in rules], minlength = self.maxlen+1))\n",
    "        prior_ChsRules= sum([log_betabin(Kn_count[i],self.patternSpace[i],self.alpha_l[i],self.beta_l[i]) for i in range(1,len(Kn_count),1)])            \n",
    "        likelihood_1 =  log_betabin(TP,TP+FP,self.alpha_1,self.beta_1)\n",
    "        likelihood_2 = log_betabin(TN,FN+TN,self.alpha_2,self.beta_2)\n",
    "        return [TP,FP,TN,FN],[prior_ChsRules,likelihood_1,likelihood_2]\n",
    "\n",
    "    def normalize_add(self, rules_new, rule_index):\n",
    "        rules = rules_new.copy()\n",
    "        for rule in rules_new:\n",
    "            if set(self.rules[rule]).issubset(self.rules[rule_index]):\n",
    "                return rules_new.copy()\n",
    "            if set(self.rules[rule_index]).issubset(self.rules[rule]):\n",
    "                rules.remove(rule)\n",
    "        rules.append(rule_index)\n",
    "        return rules\n",
    "\n",
    "    def normalize(self, rules_new):\n",
    "        try:\n",
    "            rules_len = [len(self.rules[index]) for index in rules_new]\n",
    "            rules = [rules_new[i] for i in np.argsort(rules_len)[::-1][:len(rules_len)]]\n",
    "            p1 = 0\n",
    "            while p1<len(rules):\n",
    "                for p2 in range(p1+1,len(rules),1):\n",
    "                    if set(self.rules[rules[p2]]).issubset(set(self.rules[rules[p1]])):\n",
    "                        rules.remove(rules[p1])\n",
    "                        p1 -= 1\n",
    "                        break\n",
    "                p1 += 1\n",
    "            return rules\n",
    "        except:\n",
    "            return rules_new.copy()\n",
    "\n",
    "\n",
    "    def print_rules(self, rules_max):\n",
    "        for rule_index in rules_max:\n",
    "            print(self.rules[rule_index])\n",
    "            \n",
    "\n",
    "\"\"\" parameters \"\"\"\n",
    "# The following parameters are recommended to change depending on the size and complexity of the data\n",
    "N = 2000      # number of rules to be used in SA_patternbased and also the output of generate_rules\n",
    "Niteration = 500  # number of iterations in each chain\n",
    "Nchain = 2         # number of chains in the simulated annealing search algorithm\n",
    "\n",
    "supp = 5           # 5% is a generally good number. The higher this supp, the 'larger' a pattern is\n",
    "maxlen = 3         # maxmum length of a pattern\n",
    "\n",
    "# \\rho = alpha/(alpha+beta). Make sure \\rho is close to one when choosing alpha and beta. \n",
    "alpha_1 = 500       # alpha_+\n",
    "beta_1 = 1          # beta_+\n",
    "alpha_2 = 500         # alpha_-\n",
    "beta_2 = 1       # beta_-\n",
    "\n",
    "\"\"\" input file \"\"\"\n",
    "# notice that in the example, X is already binary coded. \n",
    "# Data has to be binary coded and the column name shd have the form: attributename_attributevalue\n",
    "filepathX = 'tictactoe_X.txt' # input file X\n",
    "filepathY = 'tictactoe_Y.txt' # input file Y\n",
    "df = pd.read_csv(filepathX,header=0,sep=\" \")\n",
    "Y = np.loadtxt(open(filepathY,\"rb\"),delimiter=\" \")\n",
    "\n",
    "import random\n",
    "lenY = len(Y)\n",
    "train_index = random.sample(range(lenY), int(0.70 * lenY))\n",
    "test_index = [i for i in range(lenY) if i not in train_index]\n",
    "\n",
    "model = BOA(df.iloc[train_index],Y[train_index])\n",
    "model.generate_rules(supp,maxlen,N)\n",
    "model.set_parameters(alpha_1,beta_1,alpha_2,beta_2,None,None)\n",
    "rules = model.fit(Niteration,Nchain,print_message=True)\n",
    "\n",
    "# test\n",
    "Yhat = predict(rules,df.iloc[test_index])\n",
    "TP,FP,TN,FN = getConfusion(Yhat,Y[test_index])\n",
    "tpr = float(TP)/(TP+FN)\n",
    "fpr = float(FP)/(FP+TN)\n",
    "print('TP = {}, FP = {}, TN = {}, FN = {} \\n accuracy = {}, tpr = {}, fpr = {}'.format(TP,FP,TN,FN, float(TP+TN)/(TP+TN+FP+FN),tpr,fpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTook 3.692s to generate 19244 rules\n",
      "Screening rules using information gain\n",
      "\tTook 0.516s to generate 2000 rules\n",
      "Computing sizes for pattern space ...\n",
      "\tTook 0.000s to compute patternspace\n",
      "No or wrong input for alpha_l and beta_l. The model will use default parameters!\n",
      "found a more accurate model: accuracy = 0.5880597014925373\n",
      "\n",
      "** chain = 0, max at iter = 0 ** \n",
      " accuracy = 0.5880597014925373, TP = 304.0,FP = 148.0, TN = 90.0, FN = 128.0\n",
      " pt_new is -797.7256136755959, prior_ChsRules=-55.76454119002847, likelihood_1 = -408.4700343922632, likelihood_2 = -333.49103809330427\n",
      " \n",
      "['4_B_neg', '1_X', '5_O_neg']\n",
      "['5_X_neg', '5_B_neg', '8_X']\n",
      "['5_O', '2_B_neg', '8_X_neg']\n",
      "['9_B_neg', '5_O', '2_B_neg']\n",
      "['7_O_neg', '3_O_neg', '5_O_neg']\n",
      "[944, 1029, 369, 1599, 1851]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-c05f40bfe04e>:107: RuntimeWarning: divide by zero encountered in log\n",
      "  cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
      "<ipython-input-12-c05f40bfe04e>:107: RuntimeWarning: invalid value encountered in multiply\n",
      "  cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
      "<ipython-input-12-c05f40bfe04e>:109: RuntimeWarning: divide by zero encountered in log\n",
      "  cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
      "<ipython-input-12-c05f40bfe04e>:109: RuntimeWarning: invalid value encountered in multiply\n",
      "  cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
      "<ipython-input-12-c05f40bfe04e>:166: RuntimeWarning: overflow encountered in exp\n",
      "  alpha = np.exp(float(pt_new -pt_curr)/T)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable. Did you mean: 'random.random(...)'?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/x/Desktop/Repositories/BOA_model.py\u001b[0m in \u001b[0;36mline 348\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=345'>346</a>\u001b[0m model\u001b[39m.\u001b[39mgenerate_rules(supp,maxlen,N)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=346'>347</a>\u001b[0m model\u001b[39m.\u001b[39mset_parameters(alpha_1,beta_1,alpha_2,beta_2,\u001b[39mNone\u001b[39;00m,\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=347'>348</a>\u001b[0m rules \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(Niteration,Nchain,print_message\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=349'>350</a>\u001b[0m \u001b[39m# test\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=350'>351</a>\u001b[0m Yhat \u001b[39m=\u001b[39m predict(rules,df\u001b[39m.\u001b[39miloc[test_index])\n",
      "\u001b[1;32m/Users/x/Desktop/Repositories/BOA_model.py\u001b[0m in \u001b[0;36mline 162\u001b[0m, in \u001b[0;36mBOA.fit\u001b[0;34m(self, Niteration, Nchain, q, init, keep_most_accurate_model, print_message)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=159'>160</a>\u001b[0m     rules_curr \u001b[39m=\u001b[39m maps[chain][index][\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=160'>161</a>\u001b[0m     rules_curr_norm \u001b[39m=\u001b[39m maps[chain][index][\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=161'>162</a>\u001b[0m rules_new, rules_norm \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropose(rules_curr\u001b[39m.\u001b[39;49mcopy(), rules_curr_norm\u001b[39m.\u001b[39;49mcopy(),q)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=162'>163</a>\u001b[0m cfmatrix,prob \u001b[39m=\u001b[39m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_prob(rules_new)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=163'>164</a>\u001b[0m T \u001b[39m=\u001b[39m T0\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39miter\u001b[39m\u001b[39m/\u001b[39mNiteration)\n",
      "\u001b[1;32m/Users/x/Desktop/Repositories/BOA_model.py\u001b[0m in \u001b[0;36mline 239\u001b[0m, in \u001b[0;36mBOA.propose\u001b[0;34m(self, rules_curr, rules_norm, q)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=236'>237</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=237'>238</a>\u001b[0m         p \u001b[39m=\u001b[39m p\u001b[39m/\u001b[39mp[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=238'>239</a>\u001b[0m     index \u001b[39m=\u001b[39m find_lt(p,random())\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=239'>240</a>\u001b[0m     cut_rule \u001b[39m=\u001b[39m rules_curr[index]\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=240'>241</a>\u001b[0m rules_curr\u001b[39m.\u001b[39mremove(cut_rule)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable. Did you mean: 'random.random(...)'?"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# from fim import fpgrowth,fim --> this package is difficult to install. So the rule miner can be replaced by a random forest\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import chain, combinations\n",
    "import itertools\n",
    "from numpy.random import random\n",
    "from random import sample\n",
    "import time\n",
    "import operator\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csc_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "class BOA(object):\n",
    "    def __init__(self, binary_data,Y):\n",
    "        self.df = binary_data  \n",
    "        self.Y = Y\n",
    "        self.attributeLevelNum = defaultdict(int) \n",
    "        self.attributeNames = []\n",
    "        for i,name in enumerate(binary_data.columns):\n",
    "          attribute = name.split('_')[0]\n",
    "          self.attributeLevelNum[attribute] += 1\n",
    "          self.attributeNames.append(attribute)\n",
    "        self.attributeNames = list(set(self.attributeNames))\n",
    "        \n",
    "\n",
    "    def getPatternSpace(self):\n",
    "        print('Computing sizes for pattern space ...')\n",
    "        start_time = time.time()\n",
    "        \"\"\" compute the rule space from the levels in each attribute \"\"\"\n",
    "        for item in self.attributeNames:\n",
    "            self.attributeLevelNum[item+'_neg'] = self.attributeLevelNum[item]\n",
    "        self.patternSpace = np.zeros(self.maxlen+1)\n",
    "        tmp = [ item + '_neg' for item in self.attributeNames]\n",
    "        self.attributeNames.extend(tmp)\n",
    "        for k in range(1,self.maxlen+1,1):\n",
    "            for subset in combinations(self.attributeNames,k):\n",
    "                tmp = 1\n",
    "                for i in subset:\n",
    "                    tmp = tmp * self.attributeLevelNum[i]\n",
    "                self.patternSpace[k] = self.patternSpace[k] + tmp\n",
    "        print('\\tTook %0.3fs to compute patternspace' % (time.time() - start_time))               \n",
    "\n",
    "# This function generates rules that satisfy supp and maxlen using fpgrowth, then it selects the top N rules that make data have the biggest decrease in entropy\n",
    "# there are two ways to generate rules. fpgrowth can handle cases where the maxlen is small. If maxlen<=3, fpgrowth can generates rules much faster than randomforest. \n",
    "# If maxlen is big, fpgrowh tends to generate too many rules that overflow the memories. \n",
    "    def generate_rules(self,supp,maxlen,N, method = 'randomforest'):\n",
    "        self.maxlen = maxlen\n",
    "        self.supp = supp\n",
    "        df = 1-self.df #df has negative associations\n",
    "        df.columns = [name.strip() + '_neg' for name in self.df.columns]\n",
    "        df = pd.concat([self.df,df],axis = 1)\n",
    "#         if method =='fpgrowth' and maxlen<=3:\n",
    "#             itemMatrix = [[item for item in df.columns if row[item] ==1] for i,row in df.iterrows() ]  \n",
    "#             pindex = np.where(self.Y==1)[0]\n",
    "#             nindex = np.where(self.Y!=1)[0]\n",
    "#             print('Generating rules using fpgrowth')\n",
    "#             start_time = time.time()\n",
    "#             rules= fpgrowth([itemMatrix[i] for i in pindex],supp = supp,zmin = 1,zmax = maxlen)\n",
    "#             rules = [tuple(np.sort(rule[0])) for rule in rules]\n",
    "#             rules = list(set(rules))\n",
    "#             start_time = time.time()\n",
    "#             print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "#         else:\n",
    "        rules = []\n",
    "        start_time = time.time()\n",
    "        for length in range(1,maxlen+1,1):\n",
    "            n_estimators = min(pow(df.shape[1],length),4000)\n",
    "            clf = RandomForestClassifier(n_estimators = n_estimators,max_depth = length)\n",
    "            clf.fit(self.df,self.Y)\n",
    "            for n in range(n_estimators):\n",
    "                rules.extend(extract_rules(clf.estimators_[n],df.columns))\n",
    "        rules = [list(x) for x in set(tuple(x) for x in rules)]\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "        self.screen_rules(rules,df,N) # select the top N rules using secondary criteria, information gain\n",
    "        self.getPatternSpace()\n",
    "\n",
    "    def screen_rules(self,rules,df,N):\n",
    "        print('Screening rules using information gain')\n",
    "        start_time = time.time()\n",
    "        itemInd = {}\n",
    "        for i,name in enumerate(df.columns):\n",
    "            itemInd[name] = i\n",
    "        indices = np.array(list(itertools.chain.from_iterable([[itemInd[x] for x in rule] for rule in rules])))\n",
    "        len_rules = [len(rule) for rule in rules]\n",
    "        indptr =list(accumulate(len_rules))\n",
    "        indptr.insert(0,0)\n",
    "        indptr = np.array(indptr)\n",
    "        data = np.ones(len(indices))\n",
    "        ruleMatrix = csc_matrix((data,indices,indptr),shape = (len(df.columns),len(rules)))\n",
    "        mat = np.matrix(df) * ruleMatrix\n",
    "        lenMatrix = np.matrix([len_rules for i in range(df.shape[0])])\n",
    "        Z =  (mat ==lenMatrix).astype(int)\n",
    "        Zpos = [Z[i] for i in np.where(self.Y>0)][0]\n",
    "        TP = np.array(np.sum(Zpos,axis=0).tolist()[0])\n",
    "        supp_select = np.where(TP>=self.supp*sum(self.Y)/100)[0]\n",
    "        FP = np.array(np.sum(Z,axis = 0))[0] - TP\n",
    "        TN = len(self.Y) - np.sum(self.Y) - FP\n",
    "        FN = np.sum(self.Y) - TP\n",
    "        p1 = TP.astype(float)/(TP+FP)\n",
    "        p2 = FN.astype(float)/(FN+TN)\n",
    "        pp = (TP+FP).astype(float)/(TP+FP+TN+FN)\n",
    "        tpr = TP.astype(float)/(TP+FN)\n",
    "        fpr = FP.astype(float)/(FP+TN)\n",
    "        cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
    "        cond_entropy[p1*(1-p1)==0] = -((1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2)))[p1*(1-p1)==0]\n",
    "        cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
    "        cond_entropy[p1*(1-p1)*p2*(1-p2)==0] = 0\n",
    "        select = np.argsort(cond_entropy[supp_select])[::-1][-N:]\n",
    "        self.rules = [rules[i] for i in supp_select[select]]\n",
    "        self.RMatrix = np.array(Z[:,supp_select[select]])\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(self.rules)))\n",
    "\n",
    "    def set_parameters(self, a1=100,b1=10,a2=100,b2=10,al=None,bl=None):\n",
    "        # input al and bl are lists\n",
    "        # a1/(a1 + b1) and a2/(a2 + b2) should be a  bigger than 0.5. They correspond to alpha_+, beta_+ and alpha_- and beta_- from the paper. In fact, these four values play an important role and it needs to be tuned while following the a1/(a1 + b1) >0.5 and a2/(a2 + b2) >0.5 principle\n",
    "        self.alpha_1 = a1\n",
    "        self.beta_1 = b1\n",
    "        self.alpha_2 = a2\n",
    "        self.beta_2 = b2\n",
    "        if al ==None or bl==None or len(al)!=self.maxlen or len(bl)!=self.maxlen:\n",
    "            print('No or wrong input for alpha_l and beta_l. The model will use default parameters!')\n",
    "            self.C = [1.0/self.maxlen for i in range(self.maxlen)]\n",
    "            self.C.insert(0,-1)\n",
    "            self.alpha_l = [10 for i in range(self.maxlen+1)]\n",
    "            self.beta_l= [10*self.patternSpace[i]/self.C[i] for i in range(self.maxlen+1)]\n",
    "        else:\n",
    "            self.alpha_l=[1] + list(al)\n",
    "            self.beta_l = [1] + list(bl)\n",
    "\n",
    "    def fit(self, Niteration = 300, Nchain = 1, q = 0.1, init = [], keep_most_accurate_model = True,print_message=True):\n",
    "        # print('Searching for an optimal solution...')\n",
    "        start_time = time.time()\n",
    "        nRules = len(self.rules)\n",
    "        self.rules_len = [len(rule) for rule in self.rules]\n",
    "        maps = defaultdict(list)\n",
    "        T0 = 1000\n",
    "        split = 0.7*Niteration\n",
    "        acc = {}\n",
    "        most_accurate_model = defaultdict(list)\n",
    "        for chain in range(Nchain):\n",
    "            # initialize with a random pattern set\n",
    "            if init !=[]:\n",
    "                rules_curr = init.copy()\n",
    "            else:\n",
    "                N = sample(range(1,min(8,nRules),1),1)[0]\n",
    "                rules_curr = sample(range(nRules),N)\n",
    "            rules_curr_norm = self.normalize(rules_curr)\n",
    "            pt_curr = -100000000000\n",
    "            maps[chain].append([-1,[pt_curr/3,pt_curr/3,pt_curr/3],rules_curr,[self.rules[i] for i in rules_curr]])\n",
    "            acc[chain] = 0\n",
    "            for iter in range(Niteration):\n",
    "                if iter>=split:\n",
    "                    p = np.array(range(1+len(maps[chain])))\n",
    "                    p = np.array(list(accumulate(p)))\n",
    "                    p = p/p[-1]\n",
    "                    index = find_lt(p,random())\n",
    "                    rules_curr = maps[chain][index][2].copy()\n",
    "                    rules_curr_norm = maps[chain][index][2].copy()\n",
    "                rules_new, rules_norm = self.propose(rules_curr.copy(), rules_curr_norm.copy(),q)\n",
    "                cfmatrix,prob =  self.compute_prob(rules_new)\n",
    "                T = T0**(1 - iter/Niteration)\n",
    "                pt_new = sum(prob)\n",
    "                alpha = np.exp(float(pt_new -pt_curr)/T)\n",
    "                \n",
    "                if (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)> acc[chain]:\n",
    "                    most_accurate_model[chain] = rules_new[:]\n",
    "                    acc[chain] = (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)\n",
    "                    if print_message:\n",
    "                        print('found a more accurate model: accuracy = {}'.format(acc[chain]))\n",
    "                        \n",
    "                if pt_new > sum(maps[chain][-1][1]):\n",
    "                    maps[chain].append([iter,prob,rules_new,[self.rules[i] for i in rules_new]])\n",
    "                    if print_message:\n",
    "                        print('\\n** chain = {}, max at iter = {} ** \\n accuracy = {}, TP = {},FP = {}, TN = {}, FN = {}\\n pt_new is {}, prior_ChsRules={}, likelihood_1 = {}, likelihood_2 = {}\\n '.format(chain, iter,(cfmatrix[0]+cfmatrix[2]+0.0)/len(self.Y),cfmatrix[0],cfmatrix[1],cfmatrix[2],cfmatrix[3],sum(prob), prob[0], prob[1], prob[2]))\n",
    "                        # print '\\n** chain = {}, max at iter = {} ** \\n obj = {}, prior = {}, llh = {} '.format(chain, iter,prior+llh,prior,llh)\n",
    "                        self.print_rules(rules_new)\n",
    "                        print(rules_new)\n",
    "                if random.random() <= alpha:\n",
    "                    rules_curr_norm,rules_curr,pt_curr = rules_norm.copy(),rules_new.copy(),pt_new\n",
    "        # print('\\tTook %0.3fs to generate an optimal rule set' % (time.time() - start_time))\n",
    "        if keep_most_accurate_model:\n",
    "            acc_list = [acc[chain] for chain in range(Nchain)]\n",
    "            index = acc_list.index(max(acc_list))\n",
    "            return [self.rules[i] for i in most_accurate_model[index]] \n",
    "        else:\n",
    "            pt_max = [sum(maps[chain][-1][1]) for chain in range(Nchain)]\n",
    "            index = pt_max.index(max(pt_max))\n",
    "            return maps[index][-1][3]\n",
    "\n",
    "    def propose(self, rules_curr,rules_norm,q):\n",
    "        nRules = len(self.rules)\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules_curr],axis = 1)>0).astype(int)\n",
    "        incorr = np.where(self.Y!=Yhat)[0]\n",
    "        N = len(rules_curr)\n",
    "        if len(incorr)==0:\n",
    "            clean = True\n",
    "            move = ['clean']\n",
    "            # it means the BOA correctly classified all points but there could be redundant patterns, so cleaning is needed\n",
    "        else:\n",
    "            clean = False\n",
    "            ex = sample(incorr.tolist(),1)[0]\n",
    "            t = random.random()\n",
    "            if self.Y[ex]==1 or N==1:\n",
    "                if t<1.0/2 or N==1:\n",
    "                    move = ['add']       # action: add\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "            else:\n",
    "                if t<1.0/2:\n",
    "                    move = ['cut']       # action: cut\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "        if move[0]=='cut':\n",
    "            \"\"\" cut \"\"\"\n",
    "            if random.random()<q:\n",
    "                candidate = list(set(np.where(self.RMatrix[ex,:]==1)[0]).intersection(rules_curr))\n",
    "                if len(candidate)==0:\n",
    "                    candidate = rules_curr\n",
    "                cut_rule = sample(candidate,1)[0]\n",
    "            else:\n",
    "                p = []\n",
    "                all_sum = np.sum(self.RMatrix[:,rules_curr],axis = 1)\n",
    "                for index,rule in enumerate(rules_curr):\n",
    "                    Yhat= ((all_sum - np.array(self.RMatrix[:,rule]))>0).astype(int)\n",
    "                    TP,FP,TN,FN  = getConfusion(Yhat,self.Y)\n",
    "                    p.append(TP.astype(float)/(TP+FP+1))\n",
    "                    # p.append(log_betabin(TP,TP+FP,self.alpha_1,self.beta_1) + log_betabin(FN,FN+TN,self.alpha_2,self.beta_2))\n",
    "                p = [x - min(p) for x in p]\n",
    "                p = np.exp(p)\n",
    "                p = np.insert(p,0,0)\n",
    "                p = np.array(list(accumulate(p)))\n",
    "                if p[-1]==0:\n",
    "                    index = sample(range(len(rules_curr)),1)[0]\n",
    "                else:\n",
    "                    p = p/p[-1]\n",
    "                index = find_lt(p,random())\n",
    "                cut_rule = rules_curr[index]\n",
    "            rules_curr.remove(cut_rule)\n",
    "            rules_norm = self.normalize(rules_curr)\n",
    "            move.remove('cut')\n",
    "            \n",
    "        if len(move)>0 and move[0]=='add':\n",
    "            \"\"\" add \"\"\"\n",
    "            if random.random()<q:\n",
    "                add_rule = sample(range(nRules),1)[0]\n",
    "            else: \n",
    "                Yhat_neg_index = list(np.where(np.sum(self.RMatrix[:,rules_curr],axis = 1)<1)[0])\n",
    "                mat = np.multiply(self.RMatrix[Yhat_neg_index,:].transpose(),self.Y[Yhat_neg_index])\n",
    "                # TP = np.array(np.sum(mat,axis = 0).tolist()[0])\n",
    "                TP = np.sum(mat,axis = 1)\n",
    "                FP = np.array((np.sum(self.RMatrix[Yhat_neg_index,:],axis = 0) - TP))\n",
    "                TN = np.sum(self.Y[Yhat_neg_index]==0)-FP\n",
    "                FN = sum(self.Y[Yhat_neg_index]) - TP\n",
    "                p = (TP.astype(float)/(TP+FP+1))\n",
    "                p[rules_curr]=0\n",
    "                add_rule = sample(np.where(p==max(p))[0].tolist(),1)[0]\n",
    "            if add_rule not in rules_curr:\n",
    "                rules_curr.append(add_rule)\n",
    "                rules_norm = self.normalize(rules_curr)\n",
    "\n",
    "        if len(move)>0 and move[0]=='clean':\n",
    "            remove = []\n",
    "            for i,rule in enumerate(rules_norm):\n",
    "                Yhat = (np.sum(self.RMatrix[:,[rule for j,rule in enumerate(rules_norm) if (j!=i and j not in remove)]],axis = 1)>0).astype(int)\n",
    "                TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "                if TP+FP==0:\n",
    "                    remove.append(i)\n",
    "            for x in remove:\n",
    "                rules_norm.remove(x)\n",
    "            return rules_curr, rules_norm\n",
    "        return rules_curr, rules_norm\n",
    "\n",
    "    def compute_prob(self,rules):\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules],axis = 1)>0).astype(int)\n",
    "        TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "        Kn_count = list(np.bincount([self.rules_len[x] for x in rules], minlength = self.maxlen+1))\n",
    "        prior_ChsRules= sum([log_betabin(Kn_count[i],self.patternSpace[i],self.alpha_l[i],self.beta_l[i]) for i in range(1,len(Kn_count),1)])            \n",
    "        likelihood_1 =  log_betabin(TP,TP+FP,self.alpha_1,self.beta_1)\n",
    "        likelihood_2 = log_betabin(TN,FN+TN,self.alpha_2,self.beta_2)\n",
    "        return [TP,FP,TN,FN],[prior_ChsRules,likelihood_1,likelihood_2]\n",
    "\n",
    "    def normalize_add(self, rules_new, rule_index):\n",
    "        rules = rules_new.copy()\n",
    "        for rule in rules_new:\n",
    "            if set(self.rules[rule]).issubset(self.rules[rule_index]):\n",
    "                return rules_new.copy()\n",
    "            if set(self.rules[rule_index]).issubset(self.rules[rule]):\n",
    "                rules.remove(rule)\n",
    "        rules.append(rule_index)\n",
    "        return rules\n",
    "\n",
    "    def normalize(self, rules_new):\n",
    "        try:\n",
    "            rules_len = [len(self.rules[index]) for index in rules_new]\n",
    "            rules = [rules_new[i] for i in np.argsort(rules_len)[::-1][:len(rules_len)]]\n",
    "            p1 = 0\n",
    "            while p1<len(rules):\n",
    "                for p2 in range(p1+1,len(rules),1):\n",
    "                    if set(self.rules[rules[p2]]).issubset(set(self.rules[rules[p1]])):\n",
    "                        rules.remove(rules[p1])\n",
    "                        p1 -= 1\n",
    "                        break\n",
    "                p1 += 1\n",
    "            return rules\n",
    "        except:\n",
    "            return rules_new.copy()\n",
    "\n",
    "\n",
    "    def print_rules(self, rules_max):\n",
    "        for rule_index in rules_max:\n",
    "            print(self.rules[rule_index])\n",
    "            \n",
    "\n",
    "\"\"\" parameters \"\"\"\n",
    "# The following parameters are recommended to change depending on the size and complexity of the data\n",
    "N = 2000      # number of rules to be used in SA_patternbased and also the output of generate_rules\n",
    "Niteration = 500  # number of iterations in each chain\n",
    "Nchain = 2         # number of chains in the simulated annealing search algorithm\n",
    "\n",
    "supp = 5           # 5% is a generally good number. The higher this supp, the 'larger' a pattern is\n",
    "maxlen = 3         # maxmum length of a pattern\n",
    "\n",
    "# \\rho = alpha/(alpha+beta). Make sure \\rho is close to one when choosing alpha and beta. \n",
    "alpha_1 = 500       # alpha_+\n",
    "beta_1 = 1          # beta_+\n",
    "alpha_2 = 500         # alpha_-\n",
    "beta_2 = 1       # beta_-\n",
    "\n",
    "\"\"\" input file \"\"\"\n",
    "# notice that in the example, X is already binary coded. \n",
    "# Data has to be binary coded and the column name shd have the form: attributename_attributevalue\n",
    "filepathX = 'tictactoe_X.txt' # input file X\n",
    "filepathY = 'tictactoe_Y.txt' # input file Y\n",
    "df = pd.read_csv(filepathX,header=0,sep=\" \")\n",
    "Y = np.loadtxt(open(filepathY,\"rb\"),delimiter=\" \")\n",
    "\n",
    "import random\n",
    "lenY = len(Y)\n",
    "train_index = random.sample(range(lenY), int(0.70 * lenY))\n",
    "test_index = [i for i in range(lenY) if i not in train_index]\n",
    "\n",
    "model = BOA(df.iloc[train_index],Y[train_index])\n",
    "model.generate_rules(supp,maxlen,N)\n",
    "model.set_parameters(alpha_1,beta_1,alpha_2,beta_2,None,None)\n",
    "rules = model.fit(Niteration,Nchain,print_message=True)\n",
    "\n",
    "# test\n",
    "Yhat = predict(rules,df.iloc[test_index])\n",
    "TP,FP,TN,FN = getConfusion(Yhat,Y[test_index])\n",
    "tpr = float(TP)/(TP+FN)\n",
    "fpr = float(FP)/(FP+TN)\n",
    "print('TP = {}, FP = {}, TN = {}, FN = {} \\n accuracy = {}, tpr = {}, fpr = {}'.format(TP,FP,TN,FN, float(TP+TN)/(TP+TN+FP+FN),tpr,fpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTook 3.695s to generate 19320 rules\n",
      "Screening rules using information gain\n",
      "\tTook 0.483s to generate 2000 rules\n",
      "Computing sizes for pattern space ...\n",
      "\tTook 0.000s to compute patternspace\n",
      "No or wrong input for alpha_l and beta_l. The model will use default parameters!\n",
      "found a more accurate model: accuracy = 0.5044776119402985\n",
      "\n",
      "** chain = 0, max at iter = 0 ** \n",
      " accuracy = 0.5044776119402985, TP = 231.0,FP = 124.0, TN = 107.0, FN = 208.0\n",
      " pt_new is -844.8555910506775, prior_ChsRules=-34.1000842041376, likelihood_1 = -351.08898420397963, likelihood_2 = -459.66652264256027\n",
      " \n",
      "['5_B_neg', '5_X_neg', '2_B_neg']\n",
      "['5_X_neg', '2_B_neg', '3_X_neg']\n",
      "['7_O_neg', '5_O_neg', '3_O_neg']\n",
      "[1789, 1707, 1833]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-bc04e0ae21e6>:107: RuntimeWarning: divide by zero encountered in log\n",
      "  cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
      "<ipython-input-13-bc04e0ae21e6>:107: RuntimeWarning: invalid value encountered in multiply\n",
      "  cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
      "<ipython-input-13-bc04e0ae21e6>:109: RuntimeWarning: divide by zero encountered in log\n",
      "  cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
      "<ipython-input-13-bc04e0ae21e6>:109: RuntimeWarning: invalid value encountered in multiply\n",
      "  cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
      "<ipython-input-13-bc04e0ae21e6>:166: RuntimeWarning: overflow encountered in exp\n",
      "  alpha = np.exp(float(pt_new -pt_curr)/T)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bisect_left' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/x/Desktop/Repositories/BOA_model.py\u001b[0m in \u001b[0;36mline 348\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=345'>346</a>\u001b[0m model\u001b[39m.\u001b[39mgenerate_rules(supp,maxlen,N)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=346'>347</a>\u001b[0m model\u001b[39m.\u001b[39mset_parameters(alpha_1,beta_1,alpha_2,beta_2,\u001b[39mNone\u001b[39;00m,\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=347'>348</a>\u001b[0m rules \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(Niteration,Nchain,print_message\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=349'>350</a>\u001b[0m \u001b[39m# test\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=350'>351</a>\u001b[0m Yhat \u001b[39m=\u001b[39m predict(rules,df\u001b[39m.\u001b[39miloc[test_index])\n",
      "\u001b[1;32m/Users/x/Desktop/Repositories/BOA_model.py\u001b[0m in \u001b[0;36mline 162\u001b[0m, in \u001b[0;36mBOA.fit\u001b[0;34m(self, Niteration, Nchain, q, init, keep_most_accurate_model, print_message)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=159'>160</a>\u001b[0m     rules_curr \u001b[39m=\u001b[39m maps[chain][index][\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=160'>161</a>\u001b[0m     rules_curr_norm \u001b[39m=\u001b[39m maps[chain][index][\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=161'>162</a>\u001b[0m rules_new, rules_norm \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropose(rules_curr\u001b[39m.\u001b[39;49mcopy(), rules_curr_norm\u001b[39m.\u001b[39;49mcopy(),q)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=162'>163</a>\u001b[0m cfmatrix,prob \u001b[39m=\u001b[39m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_prob(rules_new)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=163'>164</a>\u001b[0m T \u001b[39m=\u001b[39m T0\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39miter\u001b[39m\u001b[39m/\u001b[39mNiteration)\n",
      "\u001b[1;32m/Users/x/Desktop/Repositories/BOA_model.py\u001b[0m in \u001b[0;36mline 239\u001b[0m, in \u001b[0;36mBOA.propose\u001b[0;34m(self, rules_curr, rules_norm, q)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=236'>237</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=237'>238</a>\u001b[0m         p \u001b[39m=\u001b[39m p\u001b[39m/\u001b[39mp[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=238'>239</a>\u001b[0m     index \u001b[39m=\u001b[39m find_lt(p,random\u001b[39m.\u001b[39;49mrandom())\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=239'>240</a>\u001b[0m     cut_rule \u001b[39m=\u001b[39m rules_curr[index]\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=240'>241</a>\u001b[0m rules_curr\u001b[39m.\u001b[39mremove(cut_rule)\n",
      "\u001b[1;32m/Users/x/Desktop/Repositories/boa_m/BOA/example.py\u001b[0m in \u001b[0;36mline 29\u001b[0m, in \u001b[0;36mfind_lt\u001b[0;34m(a, x)\n\u001b[1;32m     <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=26'>27</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_lt\u001b[39m(a, x):\n\u001b[1;32m     <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=27'>28</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Find rightmost value less than x\"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=28'>29</a>\u001b[0m     i \u001b[39m=\u001b[39m bisect_left(a, x)\n\u001b[1;32m     <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=29'>30</a>\u001b[0m     \u001b[39mif\u001b[39;00m i:\n\u001b[1;32m     <a href='file:///Users/x/Desktop/Repositories/boa_m/BOA/example.py?line=30'>31</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bisect_left' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# from fim import fpgrowth,fim --> this package is difficult to install. So the rule miner can be replaced by a random forest\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import chain, combinations\n",
    "import itertools\n",
    "from numpy.random import random\n",
    "from random import sample\n",
    "import time\n",
    "import operator\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csc_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "class BOA(object):\n",
    "    def __init__(self, binary_data,Y):\n",
    "        self.df = binary_data  \n",
    "        self.Y = Y\n",
    "        self.attributeLevelNum = defaultdict(int) \n",
    "        self.attributeNames = []\n",
    "        for i,name in enumerate(binary_data.columns):\n",
    "          attribute = name.split('_')[0]\n",
    "          self.attributeLevelNum[attribute] += 1\n",
    "          self.attributeNames.append(attribute)\n",
    "        self.attributeNames = list(set(self.attributeNames))\n",
    "        \n",
    "\n",
    "    def getPatternSpace(self):\n",
    "        print('Computing sizes for pattern space ...')\n",
    "        start_time = time.time()\n",
    "        \"\"\" compute the rule space from the levels in each attribute \"\"\"\n",
    "        for item in self.attributeNames:\n",
    "            self.attributeLevelNum[item+'_neg'] = self.attributeLevelNum[item]\n",
    "        self.patternSpace = np.zeros(self.maxlen+1)\n",
    "        tmp = [ item + '_neg' for item in self.attributeNames]\n",
    "        self.attributeNames.extend(tmp)\n",
    "        for k in range(1,self.maxlen+1,1):\n",
    "            for subset in combinations(self.attributeNames,k):\n",
    "                tmp = 1\n",
    "                for i in subset:\n",
    "                    tmp = tmp * self.attributeLevelNum[i]\n",
    "                self.patternSpace[k] = self.patternSpace[k] + tmp\n",
    "        print('\\tTook %0.3fs to compute patternspace' % (time.time() - start_time))               \n",
    "\n",
    "# This function generates rules that satisfy supp and maxlen using fpgrowth, then it selects the top N rules that make data have the biggest decrease in entropy\n",
    "# there are two ways to generate rules. fpgrowth can handle cases where the maxlen is small. If maxlen<=3, fpgrowth can generates rules much faster than randomforest. \n",
    "# If maxlen is big, fpgrowh tends to generate too many rules that overflow the memories. \n",
    "    def generate_rules(self,supp,maxlen,N, method = 'randomforest'):\n",
    "        self.maxlen = maxlen\n",
    "        self.supp = supp\n",
    "        df = 1-self.df #df has negative associations\n",
    "        df.columns = [name.strip() + '_neg' for name in self.df.columns]\n",
    "        df = pd.concat([self.df,df],axis = 1)\n",
    "#         if method =='fpgrowth' and maxlen<=3:\n",
    "#             itemMatrix = [[item for item in df.columns if row[item] ==1] for i,row in df.iterrows() ]  \n",
    "#             pindex = np.where(self.Y==1)[0]\n",
    "#             nindex = np.where(self.Y!=1)[0]\n",
    "#             print('Generating rules using fpgrowth')\n",
    "#             start_time = time.time()\n",
    "#             rules= fpgrowth([itemMatrix[i] for i in pindex],supp = supp,zmin = 1,zmax = maxlen)\n",
    "#             rules = [tuple(np.sort(rule[0])) for rule in rules]\n",
    "#             rules = list(set(rules))\n",
    "#             start_time = time.time()\n",
    "#             print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "#         else:\n",
    "        rules = []\n",
    "        start_time = time.time()\n",
    "        for length in range(1,maxlen+1,1):\n",
    "            n_estimators = min(pow(df.shape[1],length),4000)\n",
    "            clf = RandomForestClassifier(n_estimators = n_estimators,max_depth = length)\n",
    "            clf.fit(self.df,self.Y)\n",
    "            for n in range(n_estimators):\n",
    "                rules.extend(extract_rules(clf.estimators_[n],df.columns))\n",
    "        rules = [list(x) for x in set(tuple(x) for x in rules)]\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "        self.screen_rules(rules,df,N) # select the top N rules using secondary criteria, information gain\n",
    "        self.getPatternSpace()\n",
    "\n",
    "    def screen_rules(self,rules,df,N):\n",
    "        print('Screening rules using information gain')\n",
    "        start_time = time.time()\n",
    "        itemInd = {}\n",
    "        for i,name in enumerate(df.columns):\n",
    "            itemInd[name] = i\n",
    "        indices = np.array(list(itertools.chain.from_iterable([[itemInd[x] for x in rule] for rule in rules])))\n",
    "        len_rules = [len(rule) for rule in rules]\n",
    "        indptr =list(accumulate(len_rules))\n",
    "        indptr.insert(0,0)\n",
    "        indptr = np.array(indptr)\n",
    "        data = np.ones(len(indices))\n",
    "        ruleMatrix = csc_matrix((data,indices,indptr),shape = (len(df.columns),len(rules)))\n",
    "        mat = np.matrix(df) * ruleMatrix\n",
    "        lenMatrix = np.matrix([len_rules for i in range(df.shape[0])])\n",
    "        Z =  (mat ==lenMatrix).astype(int)\n",
    "        Zpos = [Z[i] for i in np.where(self.Y>0)][0]\n",
    "        TP = np.array(np.sum(Zpos,axis=0).tolist()[0])\n",
    "        supp_select = np.where(TP>=self.supp*sum(self.Y)/100)[0]\n",
    "        FP = np.array(np.sum(Z,axis = 0))[0] - TP\n",
    "        TN = len(self.Y) - np.sum(self.Y) - FP\n",
    "        FN = np.sum(self.Y) - TP\n",
    "        p1 = TP.astype(float)/(TP+FP)\n",
    "        p2 = FN.astype(float)/(FN+TN)\n",
    "        pp = (TP+FP).astype(float)/(TP+FP+TN+FN)\n",
    "        tpr = TP.astype(float)/(TP+FN)\n",
    "        fpr = FP.astype(float)/(FP+TN)\n",
    "        cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
    "        cond_entropy[p1*(1-p1)==0] = -((1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2)))[p1*(1-p1)==0]\n",
    "        cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
    "        cond_entropy[p1*(1-p1)*p2*(1-p2)==0] = 0\n",
    "        select = np.argsort(cond_entropy[supp_select])[::-1][-N:]\n",
    "        self.rules = [rules[i] for i in supp_select[select]]\n",
    "        self.RMatrix = np.array(Z[:,supp_select[select]])\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(self.rules)))\n",
    "\n",
    "    def set_parameters(self, a1=100,b1=10,a2=100,b2=10,al=None,bl=None):\n",
    "        # input al and bl are lists\n",
    "        # a1/(a1 + b1) and a2/(a2 + b2) should be a  bigger than 0.5. They correspond to alpha_+, beta_+ and alpha_- and beta_- from the paper. In fact, these four values play an important role and it needs to be tuned while following the a1/(a1 + b1) >0.5 and a2/(a2 + b2) >0.5 principle\n",
    "        self.alpha_1 = a1\n",
    "        self.beta_1 = b1\n",
    "        self.alpha_2 = a2\n",
    "        self.beta_2 = b2\n",
    "        if al ==None or bl==None or len(al)!=self.maxlen or len(bl)!=self.maxlen:\n",
    "            print('No or wrong input for alpha_l and beta_l. The model will use default parameters!')\n",
    "            self.C = [1.0/self.maxlen for i in range(self.maxlen)]\n",
    "            self.C.insert(0,-1)\n",
    "            self.alpha_l = [10 for i in range(self.maxlen+1)]\n",
    "            self.beta_l= [10*self.patternSpace[i]/self.C[i] for i in range(self.maxlen+1)]\n",
    "        else:\n",
    "            self.alpha_l=[1] + list(al)\n",
    "            self.beta_l = [1] + list(bl)\n",
    "\n",
    "    def fit(self, Niteration = 300, Nchain = 1, q = 0.1, init = [], keep_most_accurate_model = True,print_message=True):\n",
    "        # print('Searching for an optimal solution...')\n",
    "        start_time = time.time()\n",
    "        nRules = len(self.rules)\n",
    "        self.rules_len = [len(rule) for rule in self.rules]\n",
    "        maps = defaultdict(list)\n",
    "        T0 = 1000\n",
    "        split = 0.7*Niteration\n",
    "        acc = {}\n",
    "        most_accurate_model = defaultdict(list)\n",
    "        for chain in range(Nchain):\n",
    "            # initialize with a random pattern set\n",
    "            if init !=[]:\n",
    "                rules_curr = init.copy()\n",
    "            else:\n",
    "                N = sample(range(1,min(8,nRules),1),1)[0]\n",
    "                rules_curr = sample(range(nRules),N)\n",
    "            rules_curr_norm = self.normalize(rules_curr)\n",
    "            pt_curr = -100000000000\n",
    "            maps[chain].append([-1,[pt_curr/3,pt_curr/3,pt_curr/3],rules_curr,[self.rules[i] for i in rules_curr]])\n",
    "            acc[chain] = 0\n",
    "            for iter in range(Niteration):\n",
    "                if iter>=split:\n",
    "                    p = np.array(range(1+len(maps[chain])))\n",
    "                    p = np.array(list(accumulate(p)))\n",
    "                    p = p/p[-1]\n",
    "                    index = find_lt(p,random())\n",
    "                    rules_curr = maps[chain][index][2].copy()\n",
    "                    rules_curr_norm = maps[chain][index][2].copy()\n",
    "                rules_new, rules_norm = self.propose(rules_curr.copy(), rules_curr_norm.copy(),q)\n",
    "                cfmatrix,prob =  self.compute_prob(rules_new)\n",
    "                T = T0**(1 - iter/Niteration)\n",
    "                pt_new = sum(prob)\n",
    "                alpha = np.exp(float(pt_new -pt_curr)/T)\n",
    "                \n",
    "                if (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)> acc[chain]:\n",
    "                    most_accurate_model[chain] = rules_new[:]\n",
    "                    acc[chain] = (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)\n",
    "                    if print_message:\n",
    "                        print('found a more accurate model: accuracy = {}'.format(acc[chain]))\n",
    "                        \n",
    "                if pt_new > sum(maps[chain][-1][1]):\n",
    "                    maps[chain].append([iter,prob,rules_new,[self.rules[i] for i in rules_new]])\n",
    "                    if print_message:\n",
    "                        print('\\n** chain = {}, max at iter = {} ** \\n accuracy = {}, TP = {},FP = {}, TN = {}, FN = {}\\n pt_new is {}, prior_ChsRules={}, likelihood_1 = {}, likelihood_2 = {}\\n '.format(chain, iter,(cfmatrix[0]+cfmatrix[2]+0.0)/len(self.Y),cfmatrix[0],cfmatrix[1],cfmatrix[2],cfmatrix[3],sum(prob), prob[0], prob[1], prob[2]))\n",
    "                        # print '\\n** chain = {}, max at iter = {} ** \\n obj = {}, prior = {}, llh = {} '.format(chain, iter,prior+llh,prior,llh)\n",
    "                        self.print_rules(rules_new)\n",
    "                        print(rules_new)\n",
    "                if random.random() <= alpha:\n",
    "                    rules_curr_norm,rules_curr,pt_curr = rules_norm.copy(),rules_new.copy(),pt_new\n",
    "        # print('\\tTook %0.3fs to generate an optimal rule set' % (time.time() - start_time))\n",
    "        if keep_most_accurate_model:\n",
    "            acc_list = [acc[chain] for chain in range(Nchain)]\n",
    "            index = acc_list.index(max(acc_list))\n",
    "            return [self.rules[i] for i in most_accurate_model[index]] \n",
    "        else:\n",
    "            pt_max = [sum(maps[chain][-1][1]) for chain in range(Nchain)]\n",
    "            index = pt_max.index(max(pt_max))\n",
    "            return maps[index][-1][3]\n",
    "\n",
    "    def propose(self, rules_curr,rules_norm,q):\n",
    "        nRules = len(self.rules)\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules_curr],axis = 1)>0).astype(int)\n",
    "        incorr = np.where(self.Y!=Yhat)[0]\n",
    "        N = len(rules_curr)\n",
    "        if len(incorr)==0:\n",
    "            clean = True\n",
    "            move = ['clean']\n",
    "            # it means the BOA correctly classified all points but there could be redundant patterns, so cleaning is needed\n",
    "        else:\n",
    "            clean = False\n",
    "            ex = sample(incorr.tolist(),1)[0]\n",
    "            t = random.random()\n",
    "            if self.Y[ex]==1 or N==1:\n",
    "                if t<1.0/2 or N==1:\n",
    "                    move = ['add']       # action: add\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "            else:\n",
    "                if t<1.0/2:\n",
    "                    move = ['cut']       # action: cut\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "        if move[0]=='cut':\n",
    "            \"\"\" cut \"\"\"\n",
    "            if random.random()<q:\n",
    "                candidate = list(set(np.where(self.RMatrix[ex,:]==1)[0]).intersection(rules_curr))\n",
    "                if len(candidate)==0:\n",
    "                    candidate = rules_curr\n",
    "                cut_rule = sample(candidate,1)[0]\n",
    "            else:\n",
    "                p = []\n",
    "                all_sum = np.sum(self.RMatrix[:,rules_curr],axis = 1)\n",
    "                for index,rule in enumerate(rules_curr):\n",
    "                    Yhat= ((all_sum - np.array(self.RMatrix[:,rule]))>0).astype(int)\n",
    "                    TP,FP,TN,FN  = getConfusion(Yhat,self.Y)\n",
    "                    p.append(TP.astype(float)/(TP+FP+1))\n",
    "                    # p.append(log_betabin(TP,TP+FP,self.alpha_1,self.beta_1) + log_betabin(FN,FN+TN,self.alpha_2,self.beta_2))\n",
    "                p = [x - min(p) for x in p]\n",
    "                p = np.exp(p)\n",
    "                p = np.insert(p,0,0)\n",
    "                p = np.array(list(accumulate(p)))\n",
    "                if p[-1]==0:\n",
    "                    index = sample(range(len(rules_curr)),1)[0]\n",
    "                else:\n",
    "                    p = p/p[-1]\n",
    "                index = find_lt(p,random.random())\n",
    "                cut_rule = rules_curr[index]\n",
    "            rules_curr.remove(cut_rule)\n",
    "            rules_norm = self.normalize(rules_curr)\n",
    "            move.remove('cut')\n",
    "            \n",
    "        if len(move)>0 and move[0]=='add':\n",
    "            \"\"\" add \"\"\"\n",
    "            if random.random()<q:\n",
    "                add_rule = sample(range(nRules),1)[0]\n",
    "            else: \n",
    "                Yhat_neg_index = list(np.where(np.sum(self.RMatrix[:,rules_curr],axis = 1)<1)[0])\n",
    "                mat = np.multiply(self.RMatrix[Yhat_neg_index,:].transpose(),self.Y[Yhat_neg_index])\n",
    "                # TP = np.array(np.sum(mat,axis = 0).tolist()[0])\n",
    "                TP = np.sum(mat,axis = 1)\n",
    "                FP = np.array((np.sum(self.RMatrix[Yhat_neg_index,:],axis = 0) - TP))\n",
    "                TN = np.sum(self.Y[Yhat_neg_index]==0)-FP\n",
    "                FN = sum(self.Y[Yhat_neg_index]) - TP\n",
    "                p = (TP.astype(float)/(TP+FP+1))\n",
    "                p[rules_curr]=0\n",
    "                add_rule = sample(np.where(p==max(p))[0].tolist(),1)[0]\n",
    "            if add_rule not in rules_curr:\n",
    "                rules_curr.append(add_rule)\n",
    "                rules_norm = self.normalize(rules_curr)\n",
    "\n",
    "        if len(move)>0 and move[0]=='clean':\n",
    "            remove = []\n",
    "            for i,rule in enumerate(rules_norm):\n",
    "                Yhat = (np.sum(self.RMatrix[:,[rule for j,rule in enumerate(rules_norm) if (j!=i and j not in remove)]],axis = 1)>0).astype(int)\n",
    "                TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "                if TP+FP==0:\n",
    "                    remove.append(i)\n",
    "            for x in remove:\n",
    "                rules_norm.remove(x)\n",
    "            return rules_curr, rules_norm\n",
    "        return rules_curr, rules_norm\n",
    "\n",
    "    def compute_prob(self,rules):\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules],axis = 1)>0).astype(int)\n",
    "        TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "        Kn_count = list(np.bincount([self.rules_len[x] for x in rules], minlength = self.maxlen+1))\n",
    "        prior_ChsRules= sum([log_betabin(Kn_count[i],self.patternSpace[i],self.alpha_l[i],self.beta_l[i]) for i in range(1,len(Kn_count),1)])            \n",
    "        likelihood_1 =  log_betabin(TP,TP+FP,self.alpha_1,self.beta_1)\n",
    "        likelihood_2 = log_betabin(TN,FN+TN,self.alpha_2,self.beta_2)\n",
    "        return [TP,FP,TN,FN],[prior_ChsRules,likelihood_1,likelihood_2]\n",
    "\n",
    "    def normalize_add(self, rules_new, rule_index):\n",
    "        rules = rules_new.copy()\n",
    "        for rule in rules_new:\n",
    "            if set(self.rules[rule]).issubset(self.rules[rule_index]):\n",
    "                return rules_new.copy()\n",
    "            if set(self.rules[rule_index]).issubset(self.rules[rule]):\n",
    "                rules.remove(rule)\n",
    "        rules.append(rule_index)\n",
    "        return rules\n",
    "\n",
    "    def normalize(self, rules_new):\n",
    "        try:\n",
    "            rules_len = [len(self.rules[index]) for index in rules_new]\n",
    "            rules = [rules_new[i] for i in np.argsort(rules_len)[::-1][:len(rules_len)]]\n",
    "            p1 = 0\n",
    "            while p1<len(rules):\n",
    "                for p2 in range(p1+1,len(rules),1):\n",
    "                    if set(self.rules[rules[p2]]).issubset(set(self.rules[rules[p1]])):\n",
    "                        rules.remove(rules[p1])\n",
    "                        p1 -= 1\n",
    "                        break\n",
    "                p1 += 1\n",
    "            return rules\n",
    "        except:\n",
    "            return rules_new.copy()\n",
    "\n",
    "\n",
    "    def print_rules(self, rules_max):\n",
    "        for rule_index in rules_max:\n",
    "            print(self.rules[rule_index])\n",
    "            \n",
    "\n",
    "\"\"\" parameters \"\"\"\n",
    "# The following parameters are recommended to change depending on the size and complexity of the data\n",
    "N = 2000      # number of rules to be used in SA_patternbased and also the output of generate_rules\n",
    "Niteration = 500  # number of iterations in each chain\n",
    "Nchain = 2         # number of chains in the simulated annealing search algorithm\n",
    "\n",
    "supp = 5           # 5% is a generally good number. The higher this supp, the 'larger' a pattern is\n",
    "maxlen = 3         # maxmum length of a pattern\n",
    "\n",
    "# \\rho = alpha/(alpha+beta). Make sure \\rho is close to one when choosing alpha and beta. \n",
    "alpha_1 = 500       # alpha_+\n",
    "beta_1 = 1          # beta_+\n",
    "alpha_2 = 500         # alpha_-\n",
    "beta_2 = 1       # beta_-\n",
    "\n",
    "\"\"\" input file \"\"\"\n",
    "# notice that in the example, X is already binary coded. \n",
    "# Data has to be binary coded and the column name shd have the form: attributename_attributevalue\n",
    "filepathX = 'tictactoe_X.txt' # input file X\n",
    "filepathY = 'tictactoe_Y.txt' # input file Y\n",
    "df = pd.read_csv(filepathX,header=0,sep=\" \")\n",
    "Y = np.loadtxt(open(filepathY,\"rb\"),delimiter=\" \")\n",
    "\n",
    "import random\n",
    "lenY = len(Y)\n",
    "train_index = random.sample(range(lenY), int(0.70 * lenY))\n",
    "test_index = [i for i in range(lenY) if i not in train_index]\n",
    "\n",
    "model = BOA(df.iloc[train_index],Y[train_index])\n",
    "model.generate_rules(supp,maxlen,N)\n",
    "model.set_parameters(alpha_1,beta_1,alpha_2,beta_2,None,None)\n",
    "rules = model.fit(Niteration,Nchain,print_message=True)\n",
    "\n",
    "# test\n",
    "Yhat = predict(rules,df.iloc[test_index])\n",
    "TP,FP,TN,FN = getConfusion(Yhat,Y[test_index])\n",
    "tpr = float(TP)/(TP+FN)\n",
    "fpr = float(FP)/(FP+TN)\n",
    "print('TP = {}, FP = {}, TN = {}, FN = {} \\n accuracy = {}, tpr = {}, fpr = {}'.format(TP,FP,TN,FN, float(TP+TN)/(TP+TN+FP+FN),tpr,fpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTook 3.652s to generate 19180 rules\n",
      "Screening rules using information gain\n",
      "\tTook 0.547s to generate 2000 rules\n",
      "Computing sizes for pattern space ...\n",
      "\tTook 0.000s to compute patternspace\n",
      "No or wrong input for alpha_l and beta_l. The model will use default parameters!\n",
      "found a more accurate model: accuracy = 0.6373134328358209\n",
      "\n",
      "** chain = 0, max at iter = 0 ** \n",
      " accuracy = 0.6373134328358209, TP = 222.0,FP = 26.0, TN = 205.0, FN = 217.0\n",
      " pt_new is -633.8454180328308, prior_ChsRules=-23.150756822809853, likelihood_1 = -110.71867743136954, likelihood_2 = -499.9759837786514\n",
      " \n",
      "['6_X_neg', '8_B_neg', '5_X']\n",
      "['5_O_neg', '1_O_neg', '9_O_neg']\n",
      "[286, 1960]\n",
      "found a more accurate model: accuracy = 0.7373134328358208\n",
      "\n",
      "** chain = 0, max at iter = 1 ** \n",
      " accuracy = 0.7373134328358208, TP = 289.0,FP = 26.0, TN = 205.0, FN = 150.0\n",
      " pt_new is -541.2566532451183, prior_ChsRules=-34.1000842041376, likelihood_1 = -113.07434913208999, likelihood_2 = -394.0822199088907\n",
      " \n",
      "['6_X_neg', '8_B_neg', '5_X']\n",
      "['5_O_neg', '1_O_neg', '9_O_neg']\n",
      "['3_O_neg', '5_O_neg', '7_O_neg']\n",
      "[286, 1960, 1804]\n",
      "found a more accurate model: accuracy = 0.7835820895522388\n",
      "\n",
      "** chain = 0, max at iter = 3 ** \n",
      " accuracy = 0.7835820895522388, TP = 294.0,FP = 0.0, TN = 231.0, FN = 145.0\n",
      " pt_new is -424.7030061902351, prior_ChsRules=-34.1000842041376, likelihood_1 = -0.46247536282498913, likelihood_2 = -390.1404466232725\n",
      " \n",
      "['5_O_neg', '1_O_neg', '9_O_neg']\n",
      "['3_O_neg', '5_O_neg', '7_O_neg']\n",
      "['5_X', '8_X', '2_X']\n",
      "[1960, 1804, 1932]\n",
      "found a more accurate model: accuracy = 0.8313432835820895\n",
      "\n",
      "** chain = 0, max at iter = 5 ** \n",
      " accuracy = 0.8313432835820895, TP = 326.0,FP = 0.0, TN = 231.0, FN = 113.0\n",
      " pt_new is -374.92928665106956, prior_ChsRules=-44.96936741648642, likelihood_1 = -0.5019866750990332, likelihood_2 = -329.4579325594841\n",
      " \n",
      "['5_O_neg', '1_O_neg', '9_O_neg']\n",
      "['3_O_neg', '5_O_neg', '7_O_neg']\n",
      "['5_X', '8_X', '2_X']\n",
      "['6_X', '9_X', '3_X']\n",
      "[1960, 1804, 1932, 1839]\n",
      "found a more accurate model: accuracy = 0.8791044776119403\n",
      "\n",
      "** chain = 0, max at iter = 6 ** \n",
      " accuracy = 0.8791044776119403, TP = 358.0,FP = 0.0, TN = 231.0, FN = 81.0\n",
      " pt_new is -317.14733831462763, prior_ChsRules=-55.76454119002847, likelihood_1 = -0.5399960010654468, likelihood_2 = -260.8428011235337\n",
      " \n",
      "['5_O_neg', '1_O_neg', '9_O_neg']\n",
      "['3_O_neg', '5_O_neg', '7_O_neg']\n",
      "['5_X', '8_X', '2_X']\n",
      "['6_X', '9_X', '3_X']\n",
      "['7_X', '1_X', '4_X']\n",
      "[1960, 1804, 1932, 1839, 1837]\n",
      "found a more accurate model: accuracy = 0.9208955223880597\n",
      "\n",
      "** chain = 0, max at iter = 7 ** \n",
      " accuracy = 0.9208955223880597, TP = 386.0,FP = 0.0, TN = 231.0, FN = 53.0\n",
      " pt_new is -258.52692957970794, prior_ChsRules=-66.49072063146559, likelihood_1 = -0.5721088521822821, likelihood_2 = -191.46410009606006\n",
      " \n",
      "['5_O_neg', '1_O_neg', '9_O_neg']\n",
      "['3_O_neg', '5_O_neg', '7_O_neg']\n",
      "['5_X', '8_X', '2_X']\n",
      "['6_X', '9_X', '3_X']\n",
      "['7_X', '1_X', '4_X']\n",
      "['9_X', '7_X', '8_X']\n",
      "[1960, 1804, 1932, 1839, 1837, 1973]\n",
      "found a more accurate model: accuracy = 0.9611940298507463\n",
      "\n",
      "** chain = 0, max at iter = 8 ** \n",
      " accuracy = 0.9611940298507463, TP = 413.0,FP = 0.0, TN = 231.0, FN = 26.0\n",
      " pt_new is -188.80180622428816, prior_ChsRules=-77.15236008530883, likelihood_1 = -0.602127782172829, likelihood_2 = -111.0473183568065\n",
      " \n",
      "['5_O_neg', '1_O_neg', '9_O_neg']\n",
      "['3_O_neg', '5_O_neg', '7_O_neg']\n",
      "['5_X', '8_X', '2_X']\n",
      "['6_X', '9_X', '3_X']\n",
      "['7_X', '1_X', '4_X']\n",
      "['9_X', '7_X', '8_X']\n",
      "['2_X', '1_X', '3_X']\n",
      "[1960, 1804, 1932, 1839, 1837, 1973, 1892]\n",
      "found a more accurate model: accuracy = 1.0\n",
      "\n",
      "** chain = 0, max at iter = 9 ** \n",
      " accuracy = 1.0, TP = 439.0,FP = 0.0, TN = 231.0, FN = 0.0\n",
      " pt_new is -88.76338619541866, prior_ChsRules=-87.75337345330445, likelihood_1 = -0.6302073807864872, likelihood_2 = -0.37980536132772613\n",
      " \n",
      "['5_O_neg', '1_O_neg', '9_O_neg']\n",
      "['3_O_neg', '5_O_neg', '7_O_neg']\n",
      "['5_X', '8_X', '2_X']\n",
      "['6_X', '9_X', '3_X']\n",
      "['7_X', '1_X', '4_X']\n",
      "['9_X', '7_X', '8_X']\n",
      "['2_X', '1_X', '3_X']\n",
      "['5_X', '4_X', '6_X']\n",
      "[1960, 1804, 1932, 1839, 1837, 1973, 1892, 1915]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-e541f5b7b1d1>:109: RuntimeWarning: divide by zero encountered in log\n",
      "  cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
      "<ipython-input-14-e541f5b7b1d1>:109: RuntimeWarning: invalid value encountered in multiply\n",
      "  cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
      "<ipython-input-14-e541f5b7b1d1>:111: RuntimeWarning: divide by zero encountered in log\n",
      "  cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
      "<ipython-input-14-e541f5b7b1d1>:111: RuntimeWarning: invalid value encountered in multiply\n",
      "  cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
      "<ipython-input-14-e541f5b7b1d1>:168: RuntimeWarning: overflow encountered in exp\n",
      "  alpha = np.exp(float(pt_new -pt_curr)/T)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable. Did you mean: 'random.random(...)'?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/x/Desktop/Repositories/BOA_model.py\u001b[0m in \u001b[0;36mline 350\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=347'>348</a>\u001b[0m model\u001b[39m.\u001b[39mgenerate_rules(supp,maxlen,N)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=348'>349</a>\u001b[0m model\u001b[39m.\u001b[39mset_parameters(alpha_1,beta_1,alpha_2,beta_2,\u001b[39mNone\u001b[39;00m,\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=349'>350</a>\u001b[0m rules \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(Niteration,Nchain,print_message\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=351'>352</a>\u001b[0m \u001b[39m# test\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=352'>353</a>\u001b[0m Yhat \u001b[39m=\u001b[39m predict(rules,df\u001b[39m.\u001b[39miloc[test_index])\n",
      "\u001b[1;32m/Users/x/Desktop/Repositories/BOA_model.py\u001b[0m in \u001b[0;36mline 161\u001b[0m, in \u001b[0;36mBOA.fit\u001b[0;34m(self, Niteration, Nchain, q, init, keep_most_accurate_model, print_message)\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=158'>159</a>\u001b[0m p \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mlist\u001b[39m(accumulate(p)))\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=159'>160</a>\u001b[0m p \u001b[39m=\u001b[39m p\u001b[39m/\u001b[39mp[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=160'>161</a>\u001b[0m index \u001b[39m=\u001b[39m find_lt(p,random())\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=161'>162</a>\u001b[0m rules_curr \u001b[39m=\u001b[39m maps[chain][index][\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    <a href='file:///Users/x/Desktop/Repositories/BOA_model.py?line=162'>163</a>\u001b[0m rules_curr_norm \u001b[39m=\u001b[39m maps[chain][index][\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mcopy()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable. Did you mean: 'random.random(...)'?"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# from fim import fpgrowth,fim --> this package is difficult to install. So the rule miner can be replaced by a random forest\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import chain, combinations\n",
    "import itertools\n",
    "from numpy.random import random\n",
    "from random import sample\n",
    "import time\n",
    "import operator\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csc_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from bisect import bisect_left\n",
    "\n",
    "\n",
    "\n",
    "class BOA(object):\n",
    "    def __init__(self, binary_data,Y):\n",
    "        self.df = binary_data  \n",
    "        self.Y = Y\n",
    "        self.attributeLevelNum = defaultdict(int) \n",
    "        self.attributeNames = []\n",
    "        for i,name in enumerate(binary_data.columns):\n",
    "          attribute = name.split('_')[0]\n",
    "          self.attributeLevelNum[attribute] += 1\n",
    "          self.attributeNames.append(attribute)\n",
    "        self.attributeNames = list(set(self.attributeNames))\n",
    "        \n",
    "\n",
    "    def getPatternSpace(self):\n",
    "        print('Computing sizes for pattern space ...')\n",
    "        start_time = time.time()\n",
    "        \"\"\" compute the rule space from the levels in each attribute \"\"\"\n",
    "        for item in self.attributeNames:\n",
    "            self.attributeLevelNum[item+'_neg'] = self.attributeLevelNum[item]\n",
    "        self.patternSpace = np.zeros(self.maxlen+1)\n",
    "        tmp = [ item + '_neg' for item in self.attributeNames]\n",
    "        self.attributeNames.extend(tmp)\n",
    "        for k in range(1,self.maxlen+1,1):\n",
    "            for subset in combinations(self.attributeNames,k):\n",
    "                tmp = 1\n",
    "                for i in subset:\n",
    "                    tmp = tmp * self.attributeLevelNum[i]\n",
    "                self.patternSpace[k] = self.patternSpace[k] + tmp\n",
    "        print('\\tTook %0.3fs to compute patternspace' % (time.time() - start_time))               \n",
    "\n",
    "# This function generates rules that satisfy supp and maxlen using fpgrowth, then it selects the top N rules that make data have the biggest decrease in entropy\n",
    "# there are two ways to generate rules. fpgrowth can handle cases where the maxlen is small. If maxlen<=3, fpgrowth can generates rules much faster than randomforest. \n",
    "# If maxlen is big, fpgrowh tends to generate too many rules that overflow the memories. \n",
    "    def generate_rules(self,supp,maxlen,N, method = 'randomforest'):\n",
    "        self.maxlen = maxlen\n",
    "        self.supp = supp\n",
    "        df = 1-self.df #df has negative associations\n",
    "        df.columns = [name.strip() + '_neg' for name in self.df.columns]\n",
    "        df = pd.concat([self.df,df],axis = 1)\n",
    "#         if method =='fpgrowth' and maxlen<=3:\n",
    "#             itemMatrix = [[item for item in df.columns if row[item] ==1] for i,row in df.iterrows() ]  \n",
    "#             pindex = np.where(self.Y==1)[0]\n",
    "#             nindex = np.where(self.Y!=1)[0]\n",
    "#             print('Generating rules using fpgrowth')\n",
    "#             start_time = time.time()\n",
    "#             rules= fpgrowth([itemMatrix[i] for i in pindex],supp = supp,zmin = 1,zmax = maxlen)\n",
    "#             rules = [tuple(np.sort(rule[0])) for rule in rules]\n",
    "#             rules = list(set(rules))\n",
    "#             start_time = time.time()\n",
    "#             print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "#         else:\n",
    "        rules = []\n",
    "        start_time = time.time()\n",
    "        for length in range(1,maxlen+1,1):\n",
    "            n_estimators = min(pow(df.shape[1],length),4000)\n",
    "            clf = RandomForestClassifier(n_estimators = n_estimators,max_depth = length)\n",
    "            clf.fit(self.df,self.Y)\n",
    "            for n in range(n_estimators):\n",
    "                rules.extend(extract_rules(clf.estimators_[n],df.columns))\n",
    "        rules = [list(x) for x in set(tuple(x) for x in rules)]\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "        self.screen_rules(rules,df,N) # select the top N rules using secondary criteria, information gain\n",
    "        self.getPatternSpace()\n",
    "\n",
    "    def screen_rules(self,rules,df,N):\n",
    "        print('Screening rules using information gain')\n",
    "        start_time = time.time()\n",
    "        itemInd = {}\n",
    "        for i,name in enumerate(df.columns):\n",
    "            itemInd[name] = i\n",
    "        indices = np.array(list(itertools.chain.from_iterable([[itemInd[x] for x in rule] for rule in rules])))\n",
    "        len_rules = [len(rule) for rule in rules]\n",
    "        indptr =list(accumulate(len_rules))\n",
    "        indptr.insert(0,0)\n",
    "        indptr = np.array(indptr)\n",
    "        data = np.ones(len(indices))\n",
    "        ruleMatrix = csc_matrix((data,indices,indptr),shape = (len(df.columns),len(rules)))\n",
    "        mat = np.matrix(df) * ruleMatrix\n",
    "        lenMatrix = np.matrix([len_rules for i in range(df.shape[0])])\n",
    "        Z =  (mat ==lenMatrix).astype(int)\n",
    "        Zpos = [Z[i] for i in np.where(self.Y>0)][0]\n",
    "        TP = np.array(np.sum(Zpos,axis=0).tolist()[0])\n",
    "        supp_select = np.where(TP>=self.supp*sum(self.Y)/100)[0]\n",
    "        FP = np.array(np.sum(Z,axis = 0))[0] - TP\n",
    "        TN = len(self.Y) - np.sum(self.Y) - FP\n",
    "        FN = np.sum(self.Y) - TP\n",
    "        p1 = TP.astype(float)/(TP+FP)\n",
    "        p2 = FN.astype(float)/(FN+TN)\n",
    "        pp = (TP+FP).astype(float)/(TP+FP+TN+FN)\n",
    "        tpr = TP.astype(float)/(TP+FN)\n",
    "        fpr = FP.astype(float)/(FP+TN)\n",
    "        cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
    "        cond_entropy[p1*(1-p1)==0] = -((1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2)))[p1*(1-p1)==0]\n",
    "        cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
    "        cond_entropy[p1*(1-p1)*p2*(1-p2)==0] = 0\n",
    "        select = np.argsort(cond_entropy[supp_select])[::-1][-N:]\n",
    "        self.rules = [rules[i] for i in supp_select[select]]\n",
    "        self.RMatrix = np.array(Z[:,supp_select[select]])\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(self.rules)))\n",
    "\n",
    "    def set_parameters(self, a1=100,b1=10,a2=100,b2=10,al=None,bl=None):\n",
    "        # input al and bl are lists\n",
    "        # a1/(a1 + b1) and a2/(a2 + b2) should be a  bigger than 0.5. They correspond to alpha_+, beta_+ and alpha_- and beta_- from the paper. In fact, these four values play an important role and it needs to be tuned while following the a1/(a1 + b1) >0.5 and a2/(a2 + b2) >0.5 principle\n",
    "        self.alpha_1 = a1\n",
    "        self.beta_1 = b1\n",
    "        self.alpha_2 = a2\n",
    "        self.beta_2 = b2\n",
    "        if al ==None or bl==None or len(al)!=self.maxlen or len(bl)!=self.maxlen:\n",
    "            print('No or wrong input for alpha_l and beta_l. The model will use default parameters!')\n",
    "            self.C = [1.0/self.maxlen for i in range(self.maxlen)]\n",
    "            self.C.insert(0,-1)\n",
    "            self.alpha_l = [10 for i in range(self.maxlen+1)]\n",
    "            self.beta_l= [10*self.patternSpace[i]/self.C[i] for i in range(self.maxlen+1)]\n",
    "        else:\n",
    "            self.alpha_l=[1] + list(al)\n",
    "            self.beta_l = [1] + list(bl)\n",
    "\n",
    "    def fit(self, Niteration = 300, Nchain = 1, q = 0.1, init = [], keep_most_accurate_model = True,print_message=True):\n",
    "        # print('Searching for an optimal solution...')\n",
    "        start_time = time.time()\n",
    "        nRules = len(self.rules)\n",
    "        self.rules_len = [len(rule) for rule in self.rules]\n",
    "        maps = defaultdict(list)\n",
    "        T0 = 1000\n",
    "        split = 0.7*Niteration\n",
    "        acc = {}\n",
    "        most_accurate_model = defaultdict(list)\n",
    "        for chain in range(Nchain):\n",
    "            # initialize with a random pattern set\n",
    "            if init !=[]:\n",
    "                rules_curr = init.copy()\n",
    "            else:\n",
    "                N = sample(range(1,min(8,nRules),1),1)[0]\n",
    "                rules_curr = sample(range(nRules),N)\n",
    "            rules_curr_norm = self.normalize(rules_curr)\n",
    "            pt_curr = -100000000000\n",
    "            maps[chain].append([-1,[pt_curr/3,pt_curr/3,pt_curr/3],rules_curr,[self.rules[i] for i in rules_curr]])\n",
    "            acc[chain] = 0\n",
    "            for iter in range(Niteration):\n",
    "                if iter>=split:\n",
    "                    p = np.array(range(1+len(maps[chain])))\n",
    "                    p = np.array(list(accumulate(p)))\n",
    "                    p = p/p[-1]\n",
    "                    index = find_lt(p,random())\n",
    "                    rules_curr = maps[chain][index][2].copy()\n",
    "                    rules_curr_norm = maps[chain][index][2].copy()\n",
    "                rules_new, rules_norm = self.propose(rules_curr.copy(), rules_curr_norm.copy(),q)\n",
    "                cfmatrix,prob =  self.compute_prob(rules_new)\n",
    "                T = T0**(1 - iter/Niteration)\n",
    "                pt_new = sum(prob)\n",
    "                alpha = np.exp(float(pt_new -pt_curr)/T)\n",
    "                \n",
    "                if (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)> acc[chain]:\n",
    "                    most_accurate_model[chain] = rules_new[:]\n",
    "                    acc[chain] = (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)\n",
    "                    if print_message:\n",
    "                        print('found a more accurate model: accuracy = {}'.format(acc[chain]))\n",
    "                        \n",
    "                if pt_new > sum(maps[chain][-1][1]):\n",
    "                    maps[chain].append([iter,prob,rules_new,[self.rules[i] for i in rules_new]])\n",
    "                    if print_message:\n",
    "                        print('\\n** chain = {}, max at iter = {} ** \\n accuracy = {}, TP = {},FP = {}, TN = {}, FN = {}\\n pt_new is {}, prior_ChsRules={}, likelihood_1 = {}, likelihood_2 = {}\\n '.format(chain, iter,(cfmatrix[0]+cfmatrix[2]+0.0)/len(self.Y),cfmatrix[0],cfmatrix[1],cfmatrix[2],cfmatrix[3],sum(prob), prob[0], prob[1], prob[2]))\n",
    "                        # print '\\n** chain = {}, max at iter = {} ** \\n obj = {}, prior = {}, llh = {} '.format(chain, iter,prior+llh,prior,llh)\n",
    "                        self.print_rules(rules_new)\n",
    "                        print(rules_new)\n",
    "                if random.random() <= alpha:\n",
    "                    rules_curr_norm,rules_curr,pt_curr = rules_norm.copy(),rules_new.copy(),pt_new\n",
    "        # print('\\tTook %0.3fs to generate an optimal rule set' % (time.time() - start_time))\n",
    "        if keep_most_accurate_model:\n",
    "            acc_list = [acc[chain] for chain in range(Nchain)]\n",
    "            index = acc_list.index(max(acc_list))\n",
    "            return [self.rules[i] for i in most_accurate_model[index]] \n",
    "        else:\n",
    "            pt_max = [sum(maps[chain][-1][1]) for chain in range(Nchain)]\n",
    "            index = pt_max.index(max(pt_max))\n",
    "            return maps[index][-1][3]\n",
    "\n",
    "    def propose(self, rules_curr,rules_norm,q):\n",
    "        nRules = len(self.rules)\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules_curr],axis = 1)>0).astype(int)\n",
    "        incorr = np.where(self.Y!=Yhat)[0]\n",
    "        N = len(rules_curr)\n",
    "        if len(incorr)==0:\n",
    "            clean = True\n",
    "            move = ['clean']\n",
    "            # it means the BOA correctly classified all points but there could be redundant patterns, so cleaning is needed\n",
    "        else:\n",
    "            clean = False\n",
    "            ex = sample(incorr.tolist(),1)[0]\n",
    "            t = random.random()\n",
    "            if self.Y[ex]==1 or N==1:\n",
    "                if t<1.0/2 or N==1:\n",
    "                    move = ['add']       # action: add\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "            else:\n",
    "                if t<1.0/2:\n",
    "                    move = ['cut']       # action: cut\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "        if move[0]=='cut':\n",
    "            \"\"\" cut \"\"\"\n",
    "            if random.random()<q:\n",
    "                candidate = list(set(np.where(self.RMatrix[ex,:]==1)[0]).intersection(rules_curr))\n",
    "                if len(candidate)==0:\n",
    "                    candidate = rules_curr\n",
    "                cut_rule = sample(candidate,1)[0]\n",
    "            else:\n",
    "                p = []\n",
    "                all_sum = np.sum(self.RMatrix[:,rules_curr],axis = 1)\n",
    "                for index,rule in enumerate(rules_curr):\n",
    "                    Yhat= ((all_sum - np.array(self.RMatrix[:,rule]))>0).astype(int)\n",
    "                    TP,FP,TN,FN  = getConfusion(Yhat,self.Y)\n",
    "                    p.append(TP.astype(float)/(TP+FP+1))\n",
    "                    # p.append(log_betabin(TP,TP+FP,self.alpha_1,self.beta_1) + log_betabin(FN,FN+TN,self.alpha_2,self.beta_2))\n",
    "                p = [x - min(p) for x in p]\n",
    "                p = np.exp(p)\n",
    "                p = np.insert(p,0,0)\n",
    "                p = np.array(list(accumulate(p)))\n",
    "                if p[-1]==0:\n",
    "                    index = sample(range(len(rules_curr)),1)[0]\n",
    "                else:\n",
    "                    p = p/p[-1]\n",
    "                index = find_lt(p,random.random())\n",
    "                cut_rule = rules_curr[index]\n",
    "            rules_curr.remove(cut_rule)\n",
    "            rules_norm = self.normalize(rules_curr)\n",
    "            move.remove('cut')\n",
    "            \n",
    "        if len(move)>0 and move[0]=='add':\n",
    "            \"\"\" add \"\"\"\n",
    "            if random.random()<q:\n",
    "                add_rule = sample(range(nRules),1)[0]\n",
    "            else: \n",
    "                Yhat_neg_index = list(np.where(np.sum(self.RMatrix[:,rules_curr],axis = 1)<1)[0])\n",
    "                mat = np.multiply(self.RMatrix[Yhat_neg_index,:].transpose(),self.Y[Yhat_neg_index])\n",
    "                # TP = np.array(np.sum(mat,axis = 0).tolist()[0])\n",
    "                TP = np.sum(mat,axis = 1)\n",
    "                FP = np.array((np.sum(self.RMatrix[Yhat_neg_index,:],axis = 0) - TP))\n",
    "                TN = np.sum(self.Y[Yhat_neg_index]==0)-FP\n",
    "                FN = sum(self.Y[Yhat_neg_index]) - TP\n",
    "                p = (TP.astype(float)/(TP+FP+1))\n",
    "                p[rules_curr]=0\n",
    "                add_rule = sample(np.where(p==max(p))[0].tolist(),1)[0]\n",
    "            if add_rule not in rules_curr:\n",
    "                rules_curr.append(add_rule)\n",
    "                rules_norm = self.normalize(rules_curr)\n",
    "\n",
    "        if len(move)>0 and move[0]=='clean':\n",
    "            remove = []\n",
    "            for i,rule in enumerate(rules_norm):\n",
    "                Yhat = (np.sum(self.RMatrix[:,[rule for j,rule in enumerate(rules_norm) if (j!=i and j not in remove)]],axis = 1)>0).astype(int)\n",
    "                TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "                if TP+FP==0:\n",
    "                    remove.append(i)\n",
    "            for x in remove:\n",
    "                rules_norm.remove(x)\n",
    "            return rules_curr, rules_norm\n",
    "        return rules_curr, rules_norm\n",
    "\n",
    "    def compute_prob(self,rules):\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules],axis = 1)>0).astype(int)\n",
    "        TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "        Kn_count = list(np.bincount([self.rules_len[x] for x in rules], minlength = self.maxlen+1))\n",
    "        prior_ChsRules= sum([log_betabin(Kn_count[i],self.patternSpace[i],self.alpha_l[i],self.beta_l[i]) for i in range(1,len(Kn_count),1)])            \n",
    "        likelihood_1 =  log_betabin(TP,TP+FP,self.alpha_1,self.beta_1)\n",
    "        likelihood_2 = log_betabin(TN,FN+TN,self.alpha_2,self.beta_2)\n",
    "        return [TP,FP,TN,FN],[prior_ChsRules,likelihood_1,likelihood_2]\n",
    "\n",
    "    def normalize_add(self, rules_new, rule_index):\n",
    "        rules = rules_new.copy()\n",
    "        for rule in rules_new:\n",
    "            if set(self.rules[rule]).issubset(self.rules[rule_index]):\n",
    "                return rules_new.copy()\n",
    "            if set(self.rules[rule_index]).issubset(self.rules[rule]):\n",
    "                rules.remove(rule)\n",
    "        rules.append(rule_index)\n",
    "        return rules\n",
    "\n",
    "    def normalize(self, rules_new):\n",
    "        try:\n",
    "            rules_len = [len(self.rules[index]) for index in rules_new]\n",
    "            rules = [rules_new[i] for i in np.argsort(rules_len)[::-1][:len(rules_len)]]\n",
    "            p1 = 0\n",
    "            while p1<len(rules):\n",
    "                for p2 in range(p1+1,len(rules),1):\n",
    "                    if set(self.rules[rules[p2]]).issubset(set(self.rules[rules[p1]])):\n",
    "                        rules.remove(rules[p1])\n",
    "                        p1 -= 1\n",
    "                        break\n",
    "                p1 += 1\n",
    "            return rules\n",
    "        except:\n",
    "            return rules_new.copy()\n",
    "\n",
    "\n",
    "    def print_rules(self, rules_max):\n",
    "        for rule_index in rules_max:\n",
    "            print(self.rules[rule_index])\n",
    "            \n",
    "\n",
    "\"\"\" parameters \"\"\"\n",
    "# The following parameters are recommended to change depending on the size and complexity of the data\n",
    "N = 2000      # number of rules to be used in SA_patternbased and also the output of generate_rules\n",
    "Niteration = 500  # number of iterations in each chain\n",
    "Nchain = 2         # number of chains in the simulated annealing search algorithm\n",
    "\n",
    "supp = 5           # 5% is a generally good number. The higher this supp, the 'larger' a pattern is\n",
    "maxlen = 3         # maxmum length of a pattern\n",
    "\n",
    "# \\rho = alpha/(alpha+beta). Make sure \\rho is close to one when choosing alpha and beta. \n",
    "alpha_1 = 500       # alpha_+\n",
    "beta_1 = 1          # beta_+\n",
    "alpha_2 = 500         # alpha_-\n",
    "beta_2 = 1       # beta_-\n",
    "\n",
    "\"\"\" input file \"\"\"\n",
    "# notice that in the example, X is already binary coded. \n",
    "# Data has to be binary coded and the column name shd have the form: attributename_attributevalue\n",
    "filepathX = 'tictactoe_X.txt' # input file X\n",
    "filepathY = 'tictactoe_Y.txt' # input file Y\n",
    "df = pd.read_csv(filepathX,header=0,sep=\" \")\n",
    "Y = np.loadtxt(open(filepathY,\"rb\"),delimiter=\" \")\n",
    "\n",
    "import random\n",
    "lenY = len(Y)\n",
    "train_index = random.sample(range(lenY), int(0.70 * lenY))\n",
    "test_index = [i for i in range(lenY) if i not in train_index]\n",
    "\n",
    "model = BOA(df.iloc[train_index],Y[train_index])\n",
    "model.generate_rules(supp,maxlen,N)\n",
    "model.set_parameters(alpha_1,beta_1,alpha_2,beta_2,None,None)\n",
    "rules = model.fit(Niteration,Nchain,print_message=True)\n",
    "\n",
    "# test\n",
    "Yhat = predict(rules,df.iloc[test_index])\n",
    "TP,FP,TN,FN = getConfusion(Yhat,Y[test_index])\n",
    "tpr = float(TP)/(TP+FN)\n",
    "fpr = float(FP)/(FP+TN)\n",
    "print('TP = {}, FP = {}, TN = {}, FN = {} \\n accuracy = {}, tpr = {}, fpr = {}'.format(TP,FP,TN,FN, float(TP+TN)/(TP+TN+FP+FN),tpr,fpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTook 3.792s to generate 18745 rules\n",
      "Screening rules using information gain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-4469ac129a39>:109: RuntimeWarning: divide by zero encountered in log\n",
      "  cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
      "<ipython-input-15-4469ac129a39>:109: RuntimeWarning: invalid value encountered in multiply\n",
      "  cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
      "<ipython-input-15-4469ac129a39>:111: RuntimeWarning: divide by zero encountered in log\n",
      "  cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
      "<ipython-input-15-4469ac129a39>:111: RuntimeWarning: invalid value encountered in multiply\n",
      "  cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
      "<ipython-input-15-4469ac129a39>:168: RuntimeWarning: overflow encountered in exp\n",
      "  alpha = np.exp(float(pt_new -pt_curr)/T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTook 0.481s to generate 2000 rules\n",
      "Computing sizes for pattern space ...\n",
      "\tTook 0.000s to compute patternspace\n",
      "No or wrong input for alpha_l and beta_l. The model will use default parameters!\n",
      "found a more accurate model: accuracy = 0.6537313432835821\n",
      "\n",
      "** chain = 0, max at iter = 0 ** \n",
      " accuracy = 0.6537313432835821, TP = 348.0,FP = 149.0, TN = 90.0, FN = 83.0\n",
      " pt_new is -732.6346666948652, prior_ChsRules=-66.49072063146559, likelihood_1 = -417.6734608631941, likelihood_2 = -248.47048520020553\n",
      " \n",
      "['4_X_neg', '3_X', '5_O_neg']\n",
      "['1_O', '8_X', '5_X_neg']\n",
      "['5_X', '9_O_neg', '7_B_neg']\n",
      "['6_O_neg', '5_O', '2_B_neg']\n",
      "['1_X', '7_O_neg', '5_O_neg']\n",
      "['5_X', '6_X', '4_X']\n",
      "[775, 491, 878, 975, 803, 1881]\n",
      "found a more accurate model: accuracy = 0.664179104477612\n",
      "\n",
      "** chain = 0, max at iter = 1 ** \n",
      " accuracy = 0.664179104477612, TP = 344.0,FP = 138.0, TN = 101.0, FN = 87.0\n",
      " pt_new is -720.5796264639825, prior_ChsRules=-66.49072063146559, likelihood_1 = -395.8330315016501, likelihood_2 = -258.25587433086685\n",
      " \n",
      "['1_O', '8_X', '5_X_neg']\n",
      "['5_X', '9_O_neg', '7_B_neg']\n",
      "['6_O_neg', '5_O', '2_B_neg']\n",
      "['1_X', '7_O_neg', '5_O_neg']\n",
      "['5_X', '6_X', '4_X']\n",
      "['5_O_neg', '7_O_neg', '3_O_neg']\n",
      "[491, 878, 975, 803, 1881, 1854]\n",
      "found a more accurate model: accuracy = 0.7089552238805971\n",
      "\n",
      "** chain = 0, max at iter = 2 ** \n",
      " accuracy = 0.7089552238805971, TP = 351.0,FP = 115.0, TN = 124.0, FN = 80.0\n",
      " pt_new is -662.8289569702474, prior_ChsRules=-66.49072063146559, likelihood_1 = -349.91538666368797, likelihood_2 = -246.42284967509386\n",
      " \n",
      "['1_O', '8_X', '5_X_neg']\n",
      "['6_O_neg', '5_O', '2_B_neg']\n",
      "['1_X', '7_O_neg', '5_O_neg']\n",
      "['5_X', '6_X', '4_X']\n",
      "['5_O_neg', '7_O_neg', '3_O_neg']\n",
      "['9_O_neg', '5_O_neg', '1_O_neg']\n",
      "[491, 975, 803, 1881, 1854, 1865]\n",
      "\n",
      "** chain = 0, max at iter = 5 ** \n",
      " accuracy = 0.6970149253731344, TP = 317.0,FP = 89.0, TN = 150.0, FN = 114.0\n",
      " pt_new is -652.3042353489705, prior_ChsRules=-44.96936741648642, likelihood_1 = -288.3723619066823, likelihood_2 = -318.96250602580176\n",
      " \n",
      "['6_O_neg', '5_O', '2_B_neg']\n",
      "['1_X', '7_O_neg', '5_O_neg']\n",
      "['9_O_neg', '5_O_neg', '1_O_neg']\n",
      "['7_O_neg', '5_O_neg', '3_O_neg']\n",
      "[975, 803, 1865, 1826]\n",
      "found a more accurate model: accuracy = 0.7865671641791044\n",
      "\n",
      "** chain = 0, max at iter = 6 ** \n",
      " accuracy = 0.7865671641791044, TP = 298.0,FP = 10.0, TN = 229.0, FN = 133.0\n",
      " pt_new is -465.0497210278718, prior_ChsRules=-44.96936741648642, likelihood_1 = -52.25279588416197, likelihood_2 = -367.8275577272234\n",
      " \n",
      "['1_X', '7_O_neg', '5_O_neg']\n",
      "['9_O_neg', '5_O_neg', '1_O_neg']\n",
      "['7_O_neg', '5_O_neg', '3_O_neg']\n",
      "['6_X', '3_X', '9_X']\n",
      "[803, 1865, 1826, 1874]\n",
      "found a more accurate model: accuracy = 0.8373134328358209\n",
      "\n",
      "** chain = 0, max at iter = 7 ** \n",
      " accuracy = 0.8373134328358209, TP = 322.0,FP = 0.0, TN = 239.0, FN = 109.0\n",
      " pt_new is -367.9512858081807, prior_ChsRules=-44.96936741648642, likelihood_1 = -0.49713229663484526, likelihood_2 = -322.48478609505946\n",
      " \n",
      "['9_O_neg', '5_O_neg', '1_O_neg']\n",
      "['7_O_neg', '5_O_neg', '3_O_neg']\n",
      "['6_X', '3_X', '9_X']\n",
      "['4_X', '1_X', '7_X']\n",
      "[1865, 1826, 1874, 1992]\n",
      "found a more accurate model: accuracy = 0.8820895522388059\n",
      "\n",
      "** chain = 0, max at iter = 8 ** \n",
      " accuracy = 0.8820895522388059, TP = 352.0,FP = 0.0, TN = 239.0, FN = 79.0\n",
      " pt_new is -313.3461108675883, prior_ChsRules=-55.76454119002847, likelihood_1 = -0.53297842840675, likelihood_2 = -257.0485912491531\n",
      " \n",
      "['9_O_neg', '5_O_neg', '1_O_neg']\n",
      "['7_O_neg', '5_O_neg', '3_O_neg']\n",
      "['6_X', '3_X', '9_X']\n",
      "['4_X', '1_X', '7_X']\n",
      "['8_X', '5_X', '2_X']\n",
      "[1865, 1826, 1874, 1992, 1898]\n",
      "found a more accurate model: accuracy = 0.9238805970149254\n",
      "\n",
      "** chain = 0, max at iter = 11 ** \n",
      " accuracy = 0.9238805970149254, TP = 380.0,FP = 0.0, TN = 239.0, FN = 51.0\n",
      " pt_new is -253.66135524455694, prior_ChsRules=-66.49072063146559, likelihood_1 = -0.5653138090492575, likelihood_2 = -186.6053208040421\n",
      " \n",
      "['9_O_neg', '5_O_neg', '1_O_neg']\n",
      "['6_X', '3_X', '9_X']\n",
      "['4_X', '1_X', '7_X']\n",
      "['5_X', '2_X', '8_X']\n",
      "['3_O_neg', '5_O_neg', '7_O_neg']\n",
      "['2_X', '1_X', '3_X']\n",
      "[1865, 1874, 1992, 1831, 1856, 1873]\n",
      "found a more accurate model: accuracy = 0.9626865671641791\n",
      "\n",
      "** chain = 0, max at iter = 12 ** \n",
      " accuracy = 0.9626865671641791, TP = 406.0,FP = 0.0, TN = 239.0, FN = 25.0\n",
      " pt_new is -185.70113447913172, prior_ChsRules=-77.15236008530883, likelihood_1 = -0.5944312076217102, likelihood_2 = -107.95434318620119\n",
      " \n",
      "['9_O_neg', '5_O_neg', '1_O_neg']\n",
      "['6_X', '3_X', '9_X']\n",
      "['4_X', '1_X', '7_X']\n",
      "['5_X', '2_X', '8_X']\n",
      "['3_O_neg', '5_O_neg', '7_O_neg']\n",
      "['2_X', '1_X', '3_X']\n",
      "['4_X', '5_X', '6_X']\n",
      "[1865, 1874, 1992, 1831, 1856, 1873, 1843]\n",
      "found a more accurate model: accuracy = 1.0\n",
      "\n",
      "** chain = 0, max at iter = 14 ** \n",
      " accuracy = 1.0, TP = 431.0,FP = 0.0, TN = 239.0, FN = 0.0\n",
      " pt_new is -88.76571445468562, prior_ChsRules=-87.75337345330445, likelihood_1 = -0.6216511788552452, likelihood_2 = -0.3906898225259283\n",
      " \n",
      "['9_O_neg', '5_O_neg', '1_O_neg']\n",
      "['6_X', '3_X', '9_X']\n",
      "['4_X', '1_X', '7_X']\n",
      "['5_X', '2_X', '8_X']\n",
      "['2_X', '1_X', '3_X']\n",
      "['4_X', '5_X', '6_X']\n",
      "['7_X', '5_O_neg', '3_O_neg']\n",
      "['7_X', '9_X', '8_X']\n",
      "[1865, 1874, 1992, 1831, 1873, 1843, 1994, 1921]\n",
      "found a more accurate model: accuracy = 0.6208955223880597\n",
      "\n",
      "** chain = 1, max at iter = 0 ** \n",
      " accuracy = 0.6208955223880597, TP = 253.0,FP = 76.0, TN = 163.0, FN = 178.0\n",
      " pt_new is -727.3101941307705, prior_ChsRules=-44.96936741648642, likelihood_1 = -251.37778097115824, likelihood_2 = -430.9630457431258\n",
      " \n",
      "['7_O_neg', '5_O_neg', '3_O_neg']\n",
      "['1_O_neg', '5_O_neg', '4_X_neg']\n",
      "['4_O', '5_X', '1_O_neg']\n",
      "['3_O', '5_X_neg', '6_O_neg']\n",
      "[1826, 1621, 563, 1189]\n",
      "found a more accurate model: accuracy = 0.7373134328358208\n",
      "\n",
      "** chain = 1, max at iter = 2 ** \n",
      " accuracy = 0.7373134328358208, TP = 262.0,FP = 7.0, TN = 232.0, FN = 169.0\n",
      " pt_new is -504.3791631389131, prior_ChsRules=-34.1000842041376, likelihood_1 = -38.38442842423865, likelihood_2 = -431.89465051053685\n",
      " \n",
      "['7_O_neg', '5_O_neg', '3_O_neg']\n",
      "['4_O', '5_X', '1_O_neg']\n",
      "['1_O_neg', '5_O_neg', '9_O_neg']\n",
      "[1826, 563, 1890]\n",
      "found a more accurate model: accuracy = 0.7880597014925373\n",
      "\n",
      "** chain = 1, max at iter = 3 ** \n",
      " accuracy = 0.7880597014925373, TP = 296.0,FP = 7.0, TN = 232.0, FN = 135.0\n",
      " pt_new is -455.760409636493, prior_ChsRules=-44.96936741648642, likelihood_1 = -38.73208994491324, likelihood_2 = -372.05895227509336\n",
      " \n",
      "['7_O_neg', '5_O_neg', '3_O_neg']\n",
      "['4_O', '5_X', '1_O_neg']\n",
      "['1_O_neg', '5_O_neg', '9_O_neg']\n",
      "['6_X', '9_X', '3_X']\n",
      "[1826, 563, 1890, 1926]\n",
      "found a more accurate model: accuracy = 0.8343283582089552\n",
      "\n",
      "** chain = 1, max at iter = 4 ** \n",
      " accuracy = 0.8343283582089552, TP = 327.0,FP = 7.0, TN = 232.0, FN = 104.0\n",
      " pt_new is -406.0121425649222, prior_ChsRules=-55.76454119002847, likelihood_1 = -39.036423526050385, likelihood_2 = -311.21117784884336\n",
      " \n",
      "['7_O_neg', '5_O_neg', '3_O_neg']\n",
      "['4_O', '5_X', '1_O_neg']\n",
      "['1_O_neg', '5_O_neg', '9_O_neg']\n",
      "['6_X', '9_X', '3_X']\n",
      "['4_X', '1_X', '7_X']\n",
      "[1826, 563, 1890, 1926, 1992]\n",
      "found a more accurate model: accuracy = 0.8820895522388059\n",
      "\n",
      "** chain = 1, max at iter = 5 ** \n",
      " accuracy = 0.8820895522388059, TP = 352.0,FP = 0.0, TN = 239.0, FN = 79.0\n",
      " pt_new is -313.3461108675883, prior_ChsRules=-55.76454119002847, likelihood_1 = -0.53297842840675, likelihood_2 = -257.0485912491531\n",
      " \n",
      "['7_O_neg', '5_O_neg', '3_O_neg']\n",
      "['1_O_neg', '5_O_neg', '9_O_neg']\n",
      "['6_X', '9_X', '3_X']\n",
      "['4_X', '1_X', '7_X']\n",
      "['5_X', '2_X', '8_X']\n",
      "[1826, 1890, 1926, 1992, 1831]\n",
      "found a more accurate model: accuracy = 0.9208955223880597\n",
      "\n",
      "** chain = 1, max at iter = 11 ** \n",
      " accuracy = 0.9208955223880597, TP = 378.0,FP = 0.0, TN = 239.0, FN = 53.0\n",
      " pt_new is -259.0854036581695, prior_ChsRules=-66.49072063146559, likelihood_1 = -0.563038495213732, likelihood_2 = -192.0316445314902\n",
      " \n",
      "['7_O_neg', '5_O_neg', '3_O_neg']\n",
      "['6_X', '9_X', '3_X']\n",
      "['4_X', '1_X', '7_X']\n",
      "['5_X', '2_X', '8_X']\n",
      "['5_O_neg', '1_O_neg', '9_O_neg']\n",
      "['4_X', '5_X', '6_X']\n",
      "[1826, 1926, 1992, 1831, 1835, 1843]\n",
      "found a more accurate model: accuracy = 0.9238805970149254\n",
      "\n",
      "** chain = 1, max at iter = 17 ** \n",
      " accuracy = 0.9238805970149254, TP = 380.0,FP = 0.0, TN = 239.0, FN = 51.0\n",
      " pt_new is -253.66135524455694, prior_ChsRules=-66.49072063146559, likelihood_1 = -0.5653138090492575, likelihood_2 = -186.6053208040421\n",
      " \n",
      "['5_X', '2_X', '8_X']\n",
      "['5_O_neg', '1_O_neg', '9_O_neg']\n",
      "['6_X', '3_X', '9_X']\n",
      "['7_X', '1_X', '4_X']\n",
      "['7_O_neg', '3_O_neg', '5_O_neg']\n",
      "['2_X', '1_X', '3_X']\n",
      "[1831, 1835, 1874, 1937, 1855, 1873]\n",
      "found a more accurate model: accuracy = 0.9626865671641791\n",
      "\n",
      "** chain = 1, max at iter = 21 ** \n",
      " accuracy = 0.9626865671641791, TP = 406.0,FP = 0.0, TN = 239.0, FN = 25.0\n",
      " pt_new is -185.70113447913172, prior_ChsRules=-77.15236008530883, likelihood_1 = -0.5944312076217102, likelihood_2 = -107.95434318620119\n",
      " \n",
      "['5_X', '2_X', '8_X']\n",
      "['5_O_neg', '1_O_neg', '9_O_neg']\n",
      "['7_X', '1_X', '4_X']\n",
      "['7_O_neg', '3_O_neg', '5_O_neg']\n",
      "['6_X', '9_X', '3_X']\n",
      "['2_X', '1_X', '3_X']\n",
      "['5_X', '6_X', '4_X']\n",
      "[1831, 1835, 1937, 1855, 1926, 1873, 1881]\n",
      "found a more accurate model: accuracy = 1.0\n",
      "\n",
      "** chain = 1, max at iter = 26 ** \n",
      " accuracy = 1.0, TP = 431.0,FP = 0.0, TN = 239.0, FN = 0.0\n",
      " pt_new is -88.76571445468562, prior_ChsRules=-87.75337345330445, likelihood_1 = -0.6216511788552452, likelihood_2 = -0.3906898225259283\n",
      " \n",
      "['5_X', '2_X', '8_X']\n",
      "['5_O_neg', '1_O_neg', '9_O_neg']\n",
      "['7_O_neg', '3_O_neg', '5_O_neg']\n",
      "['2_X', '1_X', '3_X']\n",
      "['5_X', '6_X', '4_X']\n",
      "['6_X', '3_X', '9_X']\n",
      "['4_X', '7_X', '1_X']\n",
      "['9_X', '7_X', '8_X']\n",
      "[1831, 1835, 1855, 1873, 1881, 1874, 1891, 1822]\n",
      "TP = 195.0, FP = 0.0, TN = 93.0, FN = 0.0 \n",
      " accuracy = 1.0, tpr = 1.0, fpr = 0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# from fim import fpgrowth,fim --> this package is difficult to install. So the rule miner can be replaced by a random forest\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import chain, combinations\n",
    "import itertools\n",
    "from numpy.random import random\n",
    "from random import sample\n",
    "import time\n",
    "import operator\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csc_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from bisect import bisect_left\n",
    "\n",
    "\n",
    "\n",
    "class BOA(object):\n",
    "    def __init__(self, binary_data,Y):\n",
    "        self.df = binary_data  \n",
    "        self.Y = Y\n",
    "        self.attributeLevelNum = defaultdict(int) \n",
    "        self.attributeNames = []\n",
    "        for i,name in enumerate(binary_data.columns):\n",
    "          attribute = name.split('_')[0]\n",
    "          self.attributeLevelNum[attribute] += 1\n",
    "          self.attributeNames.append(attribute)\n",
    "        self.attributeNames = list(set(self.attributeNames))\n",
    "        \n",
    "\n",
    "    def getPatternSpace(self):\n",
    "        print('Computing sizes for pattern space ...')\n",
    "        start_time = time.time()\n",
    "        \"\"\" compute the rule space from the levels in each attribute \"\"\"\n",
    "        for item in self.attributeNames:\n",
    "            self.attributeLevelNum[item+'_neg'] = self.attributeLevelNum[item]\n",
    "        self.patternSpace = np.zeros(self.maxlen+1)\n",
    "        tmp = [ item + '_neg' for item in self.attributeNames]\n",
    "        self.attributeNames.extend(tmp)\n",
    "        for k in range(1,self.maxlen+1,1):\n",
    "            for subset in combinations(self.attributeNames,k):\n",
    "                tmp = 1\n",
    "                for i in subset:\n",
    "                    tmp = tmp * self.attributeLevelNum[i]\n",
    "                self.patternSpace[k] = self.patternSpace[k] + tmp\n",
    "        print('\\tTook %0.3fs to compute patternspace' % (time.time() - start_time))               \n",
    "\n",
    "# This function generates rules that satisfy supp and maxlen using fpgrowth, then it selects the top N rules that make data have the biggest decrease in entropy\n",
    "# there are two ways to generate rules. fpgrowth can handle cases where the maxlen is small. If maxlen<=3, fpgrowth can generates rules much faster than randomforest. \n",
    "# If maxlen is big, fpgrowh tends to generate too many rules that overflow the memories. \n",
    "    def generate_rules(self,supp,maxlen,N, method = 'randomforest'):\n",
    "        self.maxlen = maxlen\n",
    "        self.supp = supp\n",
    "        df = 1-self.df #df has negative associations\n",
    "        df.columns = [name.strip() + '_neg' for name in self.df.columns]\n",
    "        df = pd.concat([self.df,df],axis = 1)\n",
    "#         if method =='fpgrowth' and maxlen<=3:\n",
    "#             itemMatrix = [[item for item in df.columns if row[item] ==1] for i,row in df.iterrows() ]  \n",
    "#             pindex = np.where(self.Y==1)[0]\n",
    "#             nindex = np.where(self.Y!=1)[0]\n",
    "#             print('Generating rules using fpgrowth')\n",
    "#             start_time = time.time()\n",
    "#             rules= fpgrowth([itemMatrix[i] for i in pindex],supp = supp,zmin = 1,zmax = maxlen)\n",
    "#             rules = [tuple(np.sort(rule[0])) for rule in rules]\n",
    "#             rules = list(set(rules))\n",
    "#             start_time = time.time()\n",
    "#             print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "#         else:\n",
    "        rules = []\n",
    "        start_time = time.time()\n",
    "        for length in range(1,maxlen+1,1):\n",
    "            n_estimators = min(pow(df.shape[1],length),4000)\n",
    "            clf = RandomForestClassifier(n_estimators = n_estimators,max_depth = length)\n",
    "            clf.fit(self.df,self.Y)\n",
    "            for n in range(n_estimators):\n",
    "                rules.extend(extract_rules(clf.estimators_[n],df.columns))\n",
    "        rules = [list(x) for x in set(tuple(x) for x in rules)]\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules)))\n",
    "        self.screen_rules(rules,df,N) # select the top N rules using secondary criteria, information gain\n",
    "        self.getPatternSpace()\n",
    "\n",
    "    def screen_rules(self,rules,df,N):\n",
    "        print('Screening rules using information gain')\n",
    "        start_time = time.time()\n",
    "        itemInd = {}\n",
    "        for i,name in enumerate(df.columns):\n",
    "            itemInd[name] = i\n",
    "        indices = np.array(list(itertools.chain.from_iterable([[itemInd[x] for x in rule] for rule in rules])))\n",
    "        len_rules = [len(rule) for rule in rules]\n",
    "        indptr =list(accumulate(len_rules))\n",
    "        indptr.insert(0,0)\n",
    "        indptr = np.array(indptr)\n",
    "        data = np.ones(len(indices))\n",
    "        ruleMatrix = csc_matrix((data,indices,indptr),shape = (len(df.columns),len(rules)))\n",
    "        mat = np.matrix(df) * ruleMatrix\n",
    "        lenMatrix = np.matrix([len_rules for i in range(df.shape[0])])\n",
    "        Z =  (mat ==lenMatrix).astype(int)\n",
    "        Zpos = [Z[i] for i in np.where(self.Y>0)][0]\n",
    "        TP = np.array(np.sum(Zpos,axis=0).tolist()[0])\n",
    "        supp_select = np.where(TP>=self.supp*sum(self.Y)/100)[0]\n",
    "        FP = np.array(np.sum(Z,axis = 0))[0] - TP\n",
    "        TN = len(self.Y) - np.sum(self.Y) - FP\n",
    "        FN = np.sum(self.Y) - TP\n",
    "        p1 = TP.astype(float)/(TP+FP)\n",
    "        p2 = FN.astype(float)/(FN+TN)\n",
    "        pp = (TP+FP).astype(float)/(TP+FP+TN+FN)\n",
    "        tpr = TP.astype(float)/(TP+FN)\n",
    "        fpr = FP.astype(float)/(FP+TN)\n",
    "        cond_entropy = -pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1))-(1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2))\n",
    "        cond_entropy[p1*(1-p1)==0] = -((1-pp)*(p2*np.log(p2)+(1-p2)*np.log(1-p2)))[p1*(1-p1)==0]\n",
    "        cond_entropy[p2*(1-p2)==0] = -(pp*(p1*np.log(p1)+(1-p1)*np.log(1-p1)))[p2*(1-p2)==0]\n",
    "        cond_entropy[p1*(1-p1)*p2*(1-p2)==0] = 0\n",
    "        select = np.argsort(cond_entropy[supp_select])[::-1][-N:]\n",
    "        self.rules = [rules[i] for i in supp_select[select]]\n",
    "        self.RMatrix = np.array(Z[:,supp_select[select]])\n",
    "        print('\\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(self.rules)))\n",
    "\n",
    "    def set_parameters(self, a1=100,b1=10,a2=100,b2=10,al=None,bl=None):\n",
    "        # input al and bl are lists\n",
    "        # a1/(a1 + b1) and a2/(a2 + b2) should be a  bigger than 0.5. They correspond to alpha_+, beta_+ and alpha_- and beta_- from the paper. In fact, these four values play an important role and it needs to be tuned while following the a1/(a1 + b1) >0.5 and a2/(a2 + b2) >0.5 principle\n",
    "        self.alpha_1 = a1\n",
    "        self.beta_1 = b1\n",
    "        self.alpha_2 = a2\n",
    "        self.beta_2 = b2\n",
    "        if al ==None or bl==None or len(al)!=self.maxlen or len(bl)!=self.maxlen:\n",
    "            print('No or wrong input for alpha_l and beta_l. The model will use default parameters!')\n",
    "            self.C = [1.0/self.maxlen for i in range(self.maxlen)]\n",
    "            self.C.insert(0,-1)\n",
    "            self.alpha_l = [10 for i in range(self.maxlen+1)]\n",
    "            self.beta_l= [10*self.patternSpace[i]/self.C[i] for i in range(self.maxlen+1)]\n",
    "        else:\n",
    "            self.alpha_l=[1] + list(al)\n",
    "            self.beta_l = [1] + list(bl)\n",
    "\n",
    "    def fit(self, Niteration = 300, Nchain = 1, q = 0.1, init = [], keep_most_accurate_model = True,print_message=True):\n",
    "        # print('Searching for an optimal solution...')\n",
    "        start_time = time.time()\n",
    "        nRules = len(self.rules)\n",
    "        self.rules_len = [len(rule) for rule in self.rules]\n",
    "        maps = defaultdict(list)\n",
    "        T0 = 1000\n",
    "        split = 0.7*Niteration\n",
    "        acc = {}\n",
    "        most_accurate_model = defaultdict(list)\n",
    "        for chain in range(Nchain):\n",
    "            # initialize with a random pattern set\n",
    "            if init !=[]:\n",
    "                rules_curr = init.copy()\n",
    "            else:\n",
    "                N = sample(range(1,min(8,nRules),1),1)[0]\n",
    "                rules_curr = sample(range(nRules),N)\n",
    "            rules_curr_norm = self.normalize(rules_curr)\n",
    "            pt_curr = -100000000000\n",
    "            maps[chain].append([-1,[pt_curr/3,pt_curr/3,pt_curr/3],rules_curr,[self.rules[i] for i in rules_curr]])\n",
    "            acc[chain] = 0\n",
    "            for iter in range(Niteration):\n",
    "                if iter>=split:\n",
    "                    p = np.array(range(1+len(maps[chain])))\n",
    "                    p = np.array(list(accumulate(p)))\n",
    "                    p = p/p[-1]\n",
    "                    index = find_lt(p,random.random())\n",
    "                    rules_curr = maps[chain][index][2].copy()\n",
    "                    rules_curr_norm = maps[chain][index][2].copy()\n",
    "                rules_new, rules_norm = self.propose(rules_curr.copy(), rules_curr_norm.copy(),q)\n",
    "                cfmatrix,prob =  self.compute_prob(rules_new)\n",
    "                T = T0**(1 - iter/Niteration)\n",
    "                pt_new = sum(prob)\n",
    "                alpha = np.exp(float(pt_new -pt_curr)/T)\n",
    "                \n",
    "                if (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)> acc[chain]:\n",
    "                    most_accurate_model[chain] = rules_new[:]\n",
    "                    acc[chain] = (cfmatrix[0] + cfmatrix[2])/sum(cfmatrix)\n",
    "                    if print_message:\n",
    "                        print('found a more accurate model: accuracy = {}'.format(acc[chain]))\n",
    "                        \n",
    "                if pt_new > sum(maps[chain][-1][1]):\n",
    "                    maps[chain].append([iter,prob,rules_new,[self.rules[i] for i in rules_new]])\n",
    "                    if print_message:\n",
    "                        print('\\n** chain = {}, max at iter = {} ** \\n accuracy = {}, TP = {},FP = {}, TN = {}, FN = {}\\n pt_new is {}, prior_ChsRules={}, likelihood_1 = {}, likelihood_2 = {}\\n '.format(chain, iter,(cfmatrix[0]+cfmatrix[2]+0.0)/len(self.Y),cfmatrix[0],cfmatrix[1],cfmatrix[2],cfmatrix[3],sum(prob), prob[0], prob[1], prob[2]))\n",
    "                        # print '\\n** chain = {}, max at iter = {} ** \\n obj = {}, prior = {}, llh = {} '.format(chain, iter,prior+llh,prior,llh)\n",
    "                        self.print_rules(rules_new)\n",
    "                        print(rules_new)\n",
    "                if random.random() <= alpha:\n",
    "                    rules_curr_norm,rules_curr,pt_curr = rules_norm.copy(),rules_new.copy(),pt_new\n",
    "        # print('\\tTook %0.3fs to generate an optimal rule set' % (time.time() - start_time))\n",
    "        if keep_most_accurate_model:\n",
    "            acc_list = [acc[chain] for chain in range(Nchain)]\n",
    "            index = acc_list.index(max(acc_list))\n",
    "            return [self.rules[i] for i in most_accurate_model[index]] \n",
    "        else:\n",
    "            pt_max = [sum(maps[chain][-1][1]) for chain in range(Nchain)]\n",
    "            index = pt_max.index(max(pt_max))\n",
    "            return maps[index][-1][3]\n",
    "\n",
    "    def propose(self, rules_curr,rules_norm,q):\n",
    "        nRules = len(self.rules)\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules_curr],axis = 1)>0).astype(int)\n",
    "        incorr = np.where(self.Y!=Yhat)[0]\n",
    "        N = len(rules_curr)\n",
    "        if len(incorr)==0:\n",
    "            clean = True\n",
    "            move = ['clean']\n",
    "            # it means the BOA correctly classified all points but there could be redundant patterns, so cleaning is needed\n",
    "        else:\n",
    "            clean = False\n",
    "            ex = sample(incorr.tolist(),1)[0]\n",
    "            t = random.random()\n",
    "            if self.Y[ex]==1 or N==1:\n",
    "                if t<1.0/2 or N==1:\n",
    "                    move = ['add']       # action: add\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "            else:\n",
    "                if t<1.0/2:\n",
    "                    move = ['cut']       # action: cut\n",
    "                else:\n",
    "                    move = ['cut','add'] # action: replace\n",
    "        if move[0]=='cut':\n",
    "            \"\"\" cut \"\"\"\n",
    "            if random.random()<q:\n",
    "                candidate = list(set(np.where(self.RMatrix[ex,:]==1)[0]).intersection(rules_curr))\n",
    "                if len(candidate)==0:\n",
    "                    candidate = rules_curr\n",
    "                cut_rule = sample(candidate,1)[0]\n",
    "            else:\n",
    "                p = []\n",
    "                all_sum = np.sum(self.RMatrix[:,rules_curr],axis = 1)\n",
    "                for index,rule in enumerate(rules_curr):\n",
    "                    Yhat= ((all_sum - np.array(self.RMatrix[:,rule]))>0).astype(int)\n",
    "                    TP,FP,TN,FN  = getConfusion(Yhat,self.Y)\n",
    "                    p.append(TP.astype(float)/(TP+FP+1))\n",
    "                    # p.append(log_betabin(TP,TP+FP,self.alpha_1,self.beta_1) + log_betabin(FN,FN+TN,self.alpha_2,self.beta_2))\n",
    "                p = [x - min(p) for x in p]\n",
    "                p = np.exp(p)\n",
    "                p = np.insert(p,0,0)\n",
    "                p = np.array(list(accumulate(p)))\n",
    "                if p[-1]==0:\n",
    "                    index = sample(range(len(rules_curr)),1)[0]\n",
    "                else:\n",
    "                    p = p/p[-1]\n",
    "                index = find_lt(p,random.random())\n",
    "                cut_rule = rules_curr[index]\n",
    "            rules_curr.remove(cut_rule)\n",
    "            rules_norm = self.normalize(rules_curr)\n",
    "            move.remove('cut')\n",
    "            \n",
    "        if len(move)>0 and move[0]=='add':\n",
    "            \"\"\" add \"\"\"\n",
    "            if random.random()<q:\n",
    "                add_rule = sample(range(nRules),1)[0]\n",
    "            else: \n",
    "                Yhat_neg_index = list(np.where(np.sum(self.RMatrix[:,rules_curr],axis = 1)<1)[0])\n",
    "                mat = np.multiply(self.RMatrix[Yhat_neg_index,:].transpose(),self.Y[Yhat_neg_index])\n",
    "                # TP = np.array(np.sum(mat,axis = 0).tolist()[0])\n",
    "                TP = np.sum(mat,axis = 1)\n",
    "                FP = np.array((np.sum(self.RMatrix[Yhat_neg_index,:],axis = 0) - TP))\n",
    "                TN = np.sum(self.Y[Yhat_neg_index]==0)-FP\n",
    "                FN = sum(self.Y[Yhat_neg_index]) - TP\n",
    "                p = (TP.astype(float)/(TP+FP+1))\n",
    "                p[rules_curr]=0\n",
    "                add_rule = sample(np.where(p==max(p))[0].tolist(),1)[0]\n",
    "            if add_rule not in rules_curr:\n",
    "                rules_curr.append(add_rule)\n",
    "                rules_norm = self.normalize(rules_curr)\n",
    "\n",
    "        if len(move)>0 and move[0]=='clean':\n",
    "            remove = []\n",
    "            for i,rule in enumerate(rules_norm):\n",
    "                Yhat = (np.sum(self.RMatrix[:,[rule for j,rule in enumerate(rules_norm) if (j!=i and j not in remove)]],axis = 1)>0).astype(int)\n",
    "                TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "                if TP+FP==0:\n",
    "                    remove.append(i)\n",
    "            for x in remove:\n",
    "                rules_norm.remove(x)\n",
    "            return rules_curr, rules_norm\n",
    "        return rules_curr, rules_norm\n",
    "\n",
    "    def compute_prob(self,rules):\n",
    "        Yhat = (np.sum(self.RMatrix[:,rules],axis = 1)>0).astype(int)\n",
    "        TP,FP,TN,FN = getConfusion(Yhat,self.Y)\n",
    "        Kn_count = list(np.bincount([self.rules_len[x] for x in rules], minlength = self.maxlen+1))\n",
    "        prior_ChsRules= sum([log_betabin(Kn_count[i],self.patternSpace[i],self.alpha_l[i],self.beta_l[i]) for i in range(1,len(Kn_count),1)])            \n",
    "        likelihood_1 =  log_betabin(TP,TP+FP,self.alpha_1,self.beta_1)\n",
    "        likelihood_2 = log_betabin(TN,FN+TN,self.alpha_2,self.beta_2)\n",
    "        return [TP,FP,TN,FN],[prior_ChsRules,likelihood_1,likelihood_2]\n",
    "\n",
    "    def normalize_add(self, rules_new, rule_index):\n",
    "        rules = rules_new.copy()\n",
    "        for rule in rules_new:\n",
    "            if set(self.rules[rule]).issubset(self.rules[rule_index]):\n",
    "                return rules_new.copy()\n",
    "            if set(self.rules[rule_index]).issubset(self.rules[rule]):\n",
    "                rules.remove(rule)\n",
    "        rules.append(rule_index)\n",
    "        return rules\n",
    "\n",
    "    def normalize(self, rules_new):\n",
    "        try:\n",
    "            rules_len = [len(self.rules[index]) for index in rules_new]\n",
    "            rules = [rules_new[i] for i in np.argsort(rules_len)[::-1][:len(rules_len)]]\n",
    "            p1 = 0\n",
    "            while p1<len(rules):\n",
    "                for p2 in range(p1+1,len(rules),1):\n",
    "                    if set(self.rules[rules[p2]]).issubset(set(self.rules[rules[p1]])):\n",
    "                        rules.remove(rules[p1])\n",
    "                        p1 -= 1\n",
    "                        break\n",
    "                p1 += 1\n",
    "            return rules\n",
    "        except:\n",
    "            return rules_new.copy()\n",
    "\n",
    "\n",
    "    def print_rules(self, rules_max):\n",
    "        for rule_index in rules_max:\n",
    "            print(self.rules[rule_index])\n",
    "            \n",
    "\n",
    "\"\"\" parameters \"\"\"\n",
    "# The following parameters are recommended to change depending on the size and complexity of the data\n",
    "N = 2000      # number of rules to be used in SA_patternbased and also the output of generate_rules\n",
    "Niteration = 500  # number of iterations in each chain\n",
    "Nchain = 2         # number of chains in the simulated annealing search algorithm\n",
    "\n",
    "supp = 5           # 5% is a generally good number. The higher this supp, the 'larger' a pattern is\n",
    "maxlen = 3         # maxmum length of a pattern\n",
    "\n",
    "# \\rho = alpha/(alpha+beta). Make sure \\rho is close to one when choosing alpha and beta. \n",
    "alpha_1 = 500       # alpha_+\n",
    "beta_1 = 1          # beta_+\n",
    "alpha_2 = 500         # alpha_-\n",
    "beta_2 = 1       # beta_-\n",
    "\n",
    "\"\"\" input file \"\"\"\n",
    "# notice that in the example, X is already binary coded. \n",
    "# Data has to be binary coded and the column name shd have the form: attributename_attributevalue\n",
    "filepathX = 'tictactoe_X.txt' # input file X\n",
    "filepathY = 'tictactoe_Y.txt' # input file Y\n",
    "df = pd.read_csv(filepathX,header=0,sep=\" \")\n",
    "Y = np.loadtxt(open(filepathY,\"rb\"),delimiter=\" \")\n",
    "\n",
    "import random\n",
    "lenY = len(Y)\n",
    "train_index = random.sample(range(lenY), int(0.70 * lenY))\n",
    "test_index = [i for i in range(lenY) if i not in train_index]\n",
    "\n",
    "model = BOA(df.iloc[train_index],Y[train_index])\n",
    "model.generate_rules(supp,maxlen,N)\n",
    "model.set_parameters(alpha_1,beta_1,alpha_2,beta_2,None,None)\n",
    "rules = model.fit(Niteration,Nchain,print_message=True)\n",
    "\n",
    "# test\n",
    "Yhat = predict(rules,df.iloc[test_index])\n",
    "TP,FP,TN,FN = getConfusion(Yhat,Y[test_index])\n",
    "tpr = float(TP)/(TP+FN)\n",
    "fpr = float(FP)/(FP+TN)\n",
    "print('TP = {}, FP = {}, TN = {}, FN = {} \\n accuracy = {}, tpr = {}, fpr = {}'.format(TP,FP,TN,FN, float(TP+TN)/(TP+TN+FP+FN),tpr,fpr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
